<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>k8s Service Discovery and Traffic Exposure - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="Learn the service discovery and traffic exposure mechanisms in K8S clusters, including workload types, service types, and DNS resolution principles." /><meta name="keywords" content="k8s, Service Discovery, Traffic Exposure" />






<meta name="generator" content="Hugo 0.96.0 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2022-07/k8s-service-discovery-and-traffic-exposure/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">


<meta property="og:title" content="k8s Service Discovery and Traffic Exposure" />
<meta property="og:description" content="Learn the service discovery and traffic exposure mechanisms in K8S clusters, including workload types, service types, and DNS resolution principles." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2022-07/k8s-service-discovery-and-traffic-exposure/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-07-13T12:24:59+08:00" />
<meta property="article:modified_time" content="2022-07-13T12:24:59+08:00" />

<meta itemprop="name" content="k8s Service Discovery and Traffic Exposure">
<meta itemprop="description" content="Learn the service discovery and traffic exposure mechanisms in K8S clusters, including workload types, service types, and DNS resolution principles."><meta itemprop="datePublished" content="2022-07-13T12:24:59+08:00" />
<meta itemprop="dateModified" content="2022-07-13T12:24:59+08:00" />
<meta itemprop="wordCount" content="5769">
<meta itemprop="keywords" content="k8s," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="k8s Service Discovery and Traffic Exposure"/>
<meta name="twitter:description" content="Learn the service discovery and traffic exposure mechanisms in K8S clusters, including workload types, service types, and DNS resolution principles."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SOBYTE</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SOBYTE</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">k8s Service Discovery and Traffic Exposure</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-07-13 12:24:59 </span>
        <div class="post-category">
            <a href="/categories/tutorials/"> tutorials </a>
            </div>
          <span class="more-meta"> 5769 words </span>
          <span class="more-meta"> 12 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-cloud-native-foundation-concepts">1. Cloud-native foundation concepts</a>
          <ul>
            <li><a href="#11-k8s-architecture">1.1 K8S Architecture</a></li>
            <li><a href="#12-cni-foundation">1.2 CNI Foundation</a></li>
            <li><a href="#13-overlay-networks">1.3 Overlay networks</a></li>
            <li><a href="#14-border-gateway-protocol-bgp">1.4 Border Gateway Protocol (BGP)</a></li>
            <li><a href="#15-routability-routability">1.5 Routability (routability)</a></li>
          </ul>
        </li>
        <li><a href="#2-k8s-service-exposure">2. K8S Service Exposure</a>
          <ul>
            <li><a href="#21-workload-and-svc">2.1 Workload and SVC</a></li>
            <li><a href="#22-types-of-svcs">2.2 Types of SVCs</a></li>
            <li><a href="#23-identifying-port-concepts">2.3 Identifying Port Concepts</a></li>
          </ul>
        </li>
        <li><a href="#3-dns-service-in-k8s">3, DNS service in K8S</a>
          <ul>
            <li><a href="#31-dns-creation-rules">3.1 DNS Creation Rules</a></li>
            <li><a href="#32-dns-policy-configuration">3.2 DNS Policy Configuration</a></li>
            <li><a href="#33-dns-resolution-rules">3.3 DNS resolution rules</a></li>
            <li><a href="#34-dns-resolution-process">3.4 DNS resolution process</a></li>
          </ul>
        </li>
        <li><a href="#4-four-tier-service-exposure">4, four-tier service exposure</a></li>
        <li><a href="#5-seven-tier-service-exposure">5, seven-tier service exposure</a>
          <ul>
            <li><a href="#51-ingress">5.1 Ingress</a></li>
            <li><a href="#52-ingress-controllers">5.2 Ingress Controllers</a></li>
          </ul>
        </li>
        <li><a href="#6-summary">6. Summary</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>This article introduces the service discovery and traffic exposure mechanism in K8S clusters, including the workload type, service type, DNS resolution principle, and the rules of Layer 4 service exposure and Layer 7 service exposure in K8S.</p>
<h2 id="1-cloud-native-foundation-concepts">1. Cloud-native foundation concepts</h2>
<h3 id="11-k8s-architecture">1.1 K8S Architecture</h3>
<p>The following diagram is a brief introduction to the K8S architecture design in the <a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">official K8S documentation</a>. This architecture diagram focuses on the relationship between the cloud vendor&rsquo;s API, the control plane (Control Plane) and the working nodes (Node) in the K8S cluster from the cloud vendor&rsquo;s perspective, but strips out the third-party implementations such as CRI, CNI, CSI, etc.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/67c6b375a6f64a048a10c4e228cb33c8.png" alt="K8S Architecture"></p>
<p>Based on the official architecture diagram we introduce CRI and CNI into the architecture diagram, we can get this model as follows.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/ac61ffe7a2eb4ef2b0c62420a9332317.png" alt="Architecture"></p>
<ul>
<li><code>kube-apiserver</code> exposes the Kubernetes API to the public. it is the Kubernetes front-end control layer. It is designed to scale horizontally, i.e. by deploying more instances.</li>
<li><code>etcd</code> is used for Kubernetes back-end storage. etcd is responsible for keeping configuration information of Kubernetes Cluster and status information of various resources, always providing a backup plan for etcd data of Kubernetes cluster. When data changes, etcd quickly notifies Kubernetes-related components.</li>
<li>When there is a new Pod in the cluster that has not yet been assigned to a Node, <code>kube-scheduler</code> will analyze and assign it to the most appropriate Node based on the load on each Node and the application&rsquo;s needs for high availability, performance, data affinity, and other aspects. to the most appropriate node.</li>
<li><code>kube-controller-manager</code> runs the controllers, which are background threads that handle regular tasks in the cluster. Logically, each controller is a separate process, but to reduce complexity, they are compiled into separate executables and run in a single process. These controllers include: Node Controllers (<code>Node Controllers</code>), Replication Controllers (<code>Replication Controllers</code>), Endpoint Controllers (<code>Endpoints Controllers</code>), Service Account &amp; Token Controllers`), etc.</li>
<li><code>kube-proxy</code> is a network proxy that runs on each node in the cluster. kube-proxy implements the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding. service logically represents multiple Pods on the backend, and the outside world accesses the Pods through service. The requests received by service are forwarded to Pods through kube-proxy. kube-proxy service is responsible for forwarding TCP/UDP data streams from service to back-end containers. <strong>If there are multiple replicas, kube-proxy will implement load balancing.</strong></li>
<li>The <a href="https://landscape.cncf.io/card-mode?category=runtime&amp;grouping=category&amp;zoom=150">three major plug-ins</a> of K8S control <strong>Runtime</strong>, <strong>Network</strong> and <strong>Storage</strong> respectively, namely <code>Container Runtime Interface (CRI)</code>, <code>Container Network Interface (CNI)</code> and <code>Container-Storage-Interface (CSI)</code>. Note that CRI and CNI are the base components that every K8S cluster must deploy, while CSI is not necessarily needed, and is generally only needed when we need to run stateful services.</li>
</ul>
<h3 id="12-cni-foundation">1.2 CNI Foundation</h3>
<p>K8S itself does not implement the network model within the cluster, but provides a CNI interface for third parties to implement by abstracting it out, which saves development resources to focus on K8S itself, and can leverage the power of the open source community to create a rich ecology, some implementation details and requirements of CNI we can find on <a href="https://github.com/containernetworking/cni/blob/main/SPEC.md">github</a>, so we won&rsquo;t go into the details here.</p>
<p>Focusing on the K8S definition of the network model within a cluster.</p>
<ul>
<li>Any two PODs in a K8S cluster can communicate directly and do not need to perform NAT</li>
<li>Each Pod in a K8S cluster must have its own unique, independent and accessible IP (IP-per-Pod)</li>
</ul>
<p>K8S does not care about how each CNI specifically implements the above ground rules, as long as the final network model conforms to the standard. So we can ensure that no matter what CNI is used, the Pod network within a K8S cluster is a huge flat network, and each Pod has equal status in this network. This design brings great convenience to many scenarios such as <strong>service discovery</strong>, <strong>load balancing</strong>, <strong>service migration</strong>, <strong>application configuration</strong> within the cluster.</p>
<h3 id="13-overlay-networks">1.3 Overlay networks</h3>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/82d84bbfe0254b029a6e6d0e6dfade05.png" alt="Overlay networks"></p>
<p>An overlay network can be understood as a virtual network built on top of another network, a concept that is often seen in SDNs. Similar to virtual NICs that need to rely on physical NICs to communicate, Overlay networks cannot just appear out of thin air and need to rely on an underlying network often referred to as an Underlay network. The Underlay network is an infrastructure layer dedicated to carrying user IP traffic, and the relationship between it and the Overlay network is somewhat similar to that of a physical machine and a virtual machine; both the Underlay network and the physical machine are real entities that correspond to real network devices and computing devices, respectively. Overlay networks and virtual machines are virtualized layers that depend on the lower layer entities using software.</p>
<p>In a K8S cluster that uses Overlay network, we can think of the <strong>Underlay network at the bottom as the network where the Node nodes of the K8S cluster are located, while the Overlay network at the top is generally used to handle the network communication between Pods</strong> . Under normal circumstances, the Underlay network and the Overlay network do not interfere with each other, and they do not know each other&rsquo;s network situation. However, since the Overlay network needs to rely on the Underlay network to transmit data, when the data from the Overlay network is sent to the Underlay network for transmission, it needs to encapsulate the packets and turn them into packets that the Underlay network can understand; conversely, when the data is transmitted from the Underlay network back to the When the data is transmitted from the Underlay network back to the Overlay network, it needs to be unpacked. The two common network protocols used for encapsulation in the K8S Overlay network implementation are VXLAN and IP-in-IP.</p>
<p>The main advantages of using Overlay network are as follows.</p>
<ul>
<li>Highly flexible, Overlay network is separated from the underlying hardware network facilities, so it has advantages that traditional Underlay network cannot match in scenarios such as cross-room and cross-data center</li>
</ul>
<p>The main disadvantages of using Overlay networks are as follows.</p>
<ul>
<li>Slight performance impact. The process of encapsulating packets takes up a small amount of CPU, and the extra bytes needed to encode the encapsulation (VXLAN or IP-in-IP header) in the packet reduces the maximum size of internal packets that can be sent, which in turn can mean more packets need to be sent for the same amount of total data.</li>
<li>Pod IP addresses cannot be routed outside the cluster.</li>
</ul>
<h3 id="14-border-gateway-protocol-bgp">1.4 Border Gateway Protocol (BGP)</h3>
<p>BGP (Border Gateway Protocol/Border Gateway Protocol) is a standards-based network protocol for sharing routes across a network. It is one of the fundamental components of the Internet and has excellent scaling characteristics. In K8S, BGP is a routing protocol with a high presence, and there are many related CNI or LoadBalancer that use BGP protocol to implement features such as route reachability or ECMP.</p>
<p>The best supported and most widely used CNI for BGP protocol is Calico, and Cilium also has support for BGP mode which is still in beta stage.</p>
<h3 id="15-routability-routability">1.5 Routability (routability)</h3>
<p><strong>An important difference between different K8S cluster networks is the routability of the Pod&rsquo;s IP outside the K8S cluster.</strong></p>
<blockquote>
<p>Since Pods within a K8S cluster are necessarily routable to each other, the routability between services outside the cluster to Pods within the cluster is explored here.</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/5ebf06fc2c5f49e08991c54ac6db430f.png" alt="Routability"></p>
<h4 id="routing-unreachable">Routing Unreachable</h4>
<p><strong>The so-called routing unreachable means that there is no way for machines outside the K8S cluster to establish a direct connection with the Pods inside the cluster, and the servers outside the cluster do not know how to route packets to the Pod IPs.</strong></p>
<p>In this case, when a Pod within the cluster needs to actively establish a connection with a service outside the cluster, it will perform SNAT (Source Network Address Translation) through K8S. In this case, the IP of the server outside the cluster is the Node IP of the K8S cluster node where the Pod is located instead of the Pod&rsquo;s own IP, and the destination IP of the returned data sent by the server outside the cluster is always the Node IP of the K8S cluster node, and the data is sent back to the Pod after the Node IP has been translated. In this case, servers outside the cluster cannot know the IP of the Pod, and cannot directly obtain the real request IP.</p>
<p>Conversely, it is more complicated because servers outside the cluster do not know how to route packets to Pod IPs, so there is no way to actively request these Pods, and they can only expose services outside the cluster through K8S services (NodePort, LoadBalancer, Ingress). At this point, the server outside the cluster is accessing a K8S service, not a specific Pod.</p>
<h4 id="reachable-by-route">Reachable by route</h4>
<p>If the Pod IP address is routable outside the cluster, the pod can connect directly to servers outside the cluster without SNAT, and servers outside the cluster can connect directly to the pod without going through the K8S services (NodePort, LoadBalancer, Ingress).</p>
<p>The advantages of a Pod IP address that can be routed outside the cluster are as follows.</p>
<ul>
<li>Reduced network hierarchy, reduced network-level architectural complexity, reduced user understanding costs, maintenance costs, Debug costs, etc.</li>
<li>Simpler implementation in this architecture for special application scenarios (e.g., machines outside the cluster need to connect directly to Pods)</li>
</ul>
<p>The main disadvantages of Pod IP addresses that can be routed outside the cluster are as follows.</p>
<ul>
<li>Pod IPs must also be unique in the network outside the cluster. If there are multiple K8S clusters that need to be routed outside the cluster, then a different CIDR needs to be used for each cluster Pod. this requires some planning for internal IP usage and the possibility of running out of internal IPs when the cluster is large enough.</li>
</ul>
<h4 id="determinants-of-routability">Determinants of routability</h4>
<ul>
<li>If the cluster is using an overlay network, in general Pod IPs cannot be routed outside the cluster</li>
<li>If no overlay network is used, it depends on the deployment environment (cloud vendor/local deployment), the CNI used (Calico-BGP, Cilium-BGP, etc.), and the actual network planning, etc.</li>
<li>Current implementation of out-of-cluster routability for K8S networks is generally through BGP protocol</li>
</ul>
<h2 id="2-k8s-service-exposure">2. K8S Service Exposure</h2>
<p>Under normal circumstances, the workloads we deploy in a K8S cluster are required to provide services externally. The <strong>&ldquo;external &ldquo;</strong> here refers to providing services to all external servers other than this load, and depending on whether these external servers are located in the K8S cluster, we can divide them into K8S cluster internal traffic and K8S cluster external traffic.</p>
<h3 id="21-workload-and-svc">2.1 Workload and SVC</h3>
<p>Before we start, let&rsquo;s clarify a few points.</p>
<ul>
<li>Workload (Workload) in K8S generally refers to real work tasks in a cluster. For example, <code>deployments</code> for stateless services, <code>statefulsets</code> for stateful services, <code>daemonsets</code> for some special purposes, <code>cronjobs</code> for timed services, etc., all these belong to the workload in K8S.</li>
<li>The service (SVC) in K8S is more like a collection of rules, where all the pods that meet certain conditions are grouped into a Service, and then a specific Service is formed.</li>
<li>Each SVC in K8S will have a corresponding domain name, which is composed in the format of <code>$service_name.$namespace_name.svc.$cluster_name</code>. Generally speaking, <code>$cluster_name</code> in a k8s cluster is <code>cluster.local</code>, and this field is generally This field is usually set when the cluster is created, and it can be very troublesome to change it later.</li>
</ul>
<p>In summary, we can conclude the following.</p>
<ol>
<li>the workload (workload) and service exposure (service) in K8S are isolated from each other and given to different <code>api</code>s to implement</li>
<li>each SVC will have a <code>service name+namespace+svc+cluster name</code>/<code>$service_name.$namespace_name.svc.$cluster_name</code> domain name that can be used for access (e.g. <code>app.namespace.svc.cluster.local</code>)</li>
<li>Access between services within a K8S cluster is mainly through this domain name</li>
</ol>
<h3 id="22-types-of-svcs">2.2 Types of SVCs</h3>
<p>Generally speaking svc can be divided into four categories: <code>Headless</code>, <code>ClusterIP</code>, <code>NodePort</code>, <code>LoadBalancer</code>. The relationship between the four is not completely mutually exclusive, as follows.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/2e2bcbe4dc3c4de0a8e49fc30a865805.png" alt="Types of SVCs"></p>
<h4 id="headless-services">Headless Services</h4>
<ul>
<li>The <code>Headless</code> type service is completely mutually exclusive with the other three, and the <code>Headless</code> Service can be created by specifying the Cluster IP (<code>spec.clusterIP</code>) with a value of <code>&quot;None&quot;</code>.</li>
<li>At this point the result of the domain name resolution of this service is all the Pod IPs associated with this service, and the requests will reach the pods directly when accessed using this domain name.</li>
<li>The load balancing policy at this point is equivalent to using only DNS resolution for load balancing, and not using k8s built-in kube-proxy for load balancing.</li>
<li>the <code>Headless</code> type service does not create SRV records belonging to the corresponding domain name.</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/0b3d6669bf274be6a084e23d765b1884.png" alt="Headless Services"></p>
<p><code>Headless Services</code> The advantage of this approach is that it is simple enough and the request link is short, but the disadvantage is also obvious, that is, the DNS caching problem brings uncontrollable. Many programs query DNS and do not refer to the standard TTL value, either frequent queries put enormous pressure on the DNS server, or the query is cached all the time after the service has changed and is still requesting the old IP.</p>
<h4 id="clusterip-services">ClusterIP Services</h4>
<p>In a Kubernetes cluster, each Node runs a <code>kube-proxy</code> process. The <code>kube-proxy</code> is responsible for implementing a form of VIP (Virtual IP) for the Service, commonly referred to as <code>ClusterIP Services</code>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/3e1e2df625a846cf8bf302c65da64a65.png" alt="ClusterIP Services"></p>
<ul>
<li><code>ClusterIP</code> is the most commonly used service type and the default service type, as well as the basis for the two services <code>NodePort</code>, <code>LoadBalancer</code>.</li>
<li>For services of type <code>ClusterIP</code>, K8S assigns a VIP called <code>CLUSTER-IP</code> to the service.</li>
<li>ClusterIP is a separate IP segment, distinct from the K8S host node IP segment and Pod IP segment, also defined at cluster initialization time.</li>
<li>ClusterIP can be seen inside the <code>kube-ipvs0</code> NIC on top of each k8s host node.</li>
<li>The result of domain name resolution for services of the ClusterIP type is this VIP, and requests will first go through the VIP and then be distributed by the kube-proxy to the individual pods.</li>
<li>If k8s uses ipvs, you can use the ipvsadm command on top of the K8S host node to see the forwarding rules for these load balancing.</li>
<li>The <code>ClusterIP</code> type service also creates the SRV record corresponding to the domain name it belongs to, and the port in the SRV record is the port of the ClusterIP</li>
</ul>
<p>The advantage of <code>ClusterIP Services</code> is that the VIP is located in front of the Pod, which can effectively avoid the problems caused by direct DNS resolution mentioned earlier; the disadvantage is also obvious, when the volume of requests is large, the processing performance of the kube-proxy component will first become the bottleneck of the entire request link.</p>
<h4 id="nodeport-service">NodePort Service</h4>
<ul>
<li>Starting with NodePort, the service is not limited to be exposed inside the K8S cluster, but can be provided outside the cluster</li>
<li>The NodePort type picks a port on top of the K8S host node to assign to a service (default range is 30000-32767), and users can access the service by requesting the specified port from any K8S node IP</li>
<li>NodePort Service domain name resolution results in a <code>CLUSTER-IP</code>, the load balancing logic and implementation of requests within the cluster is the same as <code>ClusterIP Service</code>.</li>
<li>The request path of the NodePort service is <strong>directly from the K8S node IP to the Pod</strong> and does not go through the ClusterIP, but the forwarding logic is still implemented by <code>kube-proxy</code>.</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/d8484a28ce6b455cbcf1c770db6cdeee.png" alt="NodePort Service"></p>
<p>The advantage of the <code>NodePort Service</code> approach is that it is very simple to expose the service to the outside of the cluster through K8S&rsquo;s own functionality; the disadvantages are obvious: the port limitations of NodePort itself (limited number and range of options) and the performance bottleneck of the kube-proxy component when the volume of requests is high.</p>
<h4 id="loadbalancer-service">LoadBalancer Service</h4>
<ul>
<li>LoadBalancer service type is the most advanced and elegant way for K8S to expose services outside the cluster, and also the highest threshold.</li>
<li>LoadBalancer service type requires the K8S cluster to support a cloud-native LoadBalancer, which is partly not implemented by K8S itself but given to the cloud vendor/third party, so for cloud environments K8S clusters can directly use the LoadBalancer provided by the cloud vendor, and of course there are some open source cloud-native LoadBalancer, such as MetalLB, OpenELB, PureLB, etc..</li>
<li>The resolution result of the LoadBalancer service domain name resolution is a <code>CLUSTER-IP</code>.</li>
<li>The LoadBalancer service also assigns an <code>EXTERNAL-IP</code>, through which machines outside the cluster can access the service.</li>
<li>LoadBalancer service will create NodePort at the same time by default, which means a LoadBalancer type service is also a NodePort service and also a clusterIP service; some cloud-native LoadBalancer can create a NodePort service by specifying <code>allocateLoadBalancerNodePorts: false</code> to deny the creation of NodePort services.</li>
</ul>
<blockquote>
<p>We still borrow the diagram from <a href="https://openelb.github.io/docs/concepts/bgp-mode/">OpenELB&rsquo;s official website</a> to explain the process, and note that this is BGP mode.</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/7017a65fc41845afb8c4a05786945976.png" alt="LoadBalancer Service"></p>
<p>The advantage of LoadBalancer Service is that it is convenient, efficient, and applicable to a wide range of scenarios, and can cover almost all external service exposures; the disadvantage is that there are few choices of mature and available cloud-native LoadBalancer, and the implementation threshold is high.</p>
<h3 id="23-identifying-port-concepts">2.3 Identifying Port Concepts</h3>
<p>When we do SVC and Workload deployment configuration, we often encounter various configuration items with <code>Port</code> in their names, which is one of the several concepts in K8S that are easy to confuse people. The relationship between the four can be more clearly distinguished by the following diagram.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/193fb0d34e244e02804c2dec1df06afd.png" alt="Identifying Port Concepts"></p>
<ul>
<li><code>nodePort</code>: works only for the Loadbalancer service and the NodePort service, which is used to specify the port of the host node of the K8S cluster, the default range is <code>30000-32767</code>, and a service can be accessed outside the K8S cluster via <code>NodeIP:nodePort</code>.</li>
<li><code>port</code>: only works for <code>CLUSTER-IP</code> and <code>EXTERNAL-IP</code>, that is, for the Loadbalancer service, the NodePort service and the ClusterIP service, which can be accessed internally by the K8S cluster via <code>CLUSTER-IP:port</code>, and externally by the K8S cluster via <code>EXTERNAL-IP:port</code> to access.</li>
<li><code>targetPort</code>: Pod&rsquo;s external access port, port and nodePort traffic will be forwarded to this port above the Pod with reference to the corresponding ipvs rule, which means the data forwarding path is <code>NodeIP:nodePort -&gt; PodIP:targetPort</code>, <code>CLUSTER-IP: port -&gt; PodIP:targetPort</code>, <code>EXTERNAL-IP:port -&gt; PodIP:targetPort</code></li>
<li><code>containerPort</code>: and the remaining three concepts do not belong to the same dimension, <code>containerPort</code> is mainly configured in <code>Workload</code>, the remaining three are configured in <code>service</code>. The <code>containerPort</code> is mainly used in the container inside the Pod to inform the K8S container about the port that provides services inside the container. CNI implementation and the difference in network policies configured by K8S, the role of <code>containerPort</code> is not obvious, and very often it is misconfigured or can work without configuration.</li>
</ul>
<p>In summary, we can learn the main difference between the four, then we should make sure that the <code>targetPort</code>, the <code>containerPort</code> and the actual listening port of the program running inside the Pod are consistent, so as to ensure that the requested data forwarding link is normal.</p>
<h2 id="3-dns-service-in-k8s">3, DNS service in K8S</h2>
<p>As we all know, in K8S, IP is subject to change at any time, and the most frequent change is <code>Pod IP</code>, <code>Cluster IP</code> is not necessarily not subject to change, <code>EXTERNAL-IP</code>, although you can manually specify a static IP to remain unchanged, but mainly for services outside the cluster; therefore, in K8S cluster, the best way to access each other&rsquo;s services in a K8S cluster is through the domain name.</p>
<h3 id="31-dns-creation-rules">3.1 DNS Creation Rules</h3>
<blockquote>
<p>In a K8S cluster, Kubernetes creates DNS records for Services and Pods.</p>
</blockquote>
<p>We introduced earlier that each SVC in K8S will have a corresponding domain name in the format of <code>$service_name.$namespace_name.svc.$cluster_name</code>, and will also create a <code>$pod_name.$service_name.svc.$cluster_name</code> for all Pods under this SVC. name.$namespace_name.svc.$cluster_name`, and the result of this domain name is the Pod IP.</p>
<p>Pod domain names have two relatively obvious characteristics.</p>
<ul>
<li>
<p>First, the composition of the domain name is special because the name of the Pod is used in the domain name, and the pod name is subject to change in K8S (for example, when the service is updated or rolled over and restarted), and because the Pod naming is not too obvious by default (most names will contain a string of random UUIDs)</p>
</li>
<li>
<p>Second, the resolution of the domain name is special, compared to other types of domain names in the cluster, the resolution of Pod domain names can be precise to a specific Pod, so some special services that require peer-to-peer communication can use such Pod domain names</p>
</li>
</ul>
<h3 id="32-dns-policy-configuration">3.2 DNS Policy Configuration</h3>
<p>DNS policies can be set on a Pod-by-Pod basis. Currently Kubernetes supports the following Pod-specific DNS policies. These policies can be set in the <code>dnsPolicy</code> field of the Pod policy.</p>
<ul>
<li><code>Default</code>: Pod inherits the domain name resolution configuration from the K8S host node where it is running.</li>
<li><code>ClusterFirst</code>: <strong>Default option if no <code>dnsPolicy</code> configuration is specified</strong>, all queried domain names will be resolved and forwarded to the DNS service inside the cluster for resolution based on the <code>/etc/resolv.conf</code> configuration generated by the generated cluster&rsquo;s K8S domain name and other information.</li>
<li><code>ClusterFirstWithHostNet</code>: this is mainly used for Pods running as <code>hostNetwork</code> and can be configured as this field if these pods want to use the DNS services within the K8S cluster.</li>
<li><code>None</code>: this setting allows the Pod to ignore the DNS settings in the Kubernetes environment and the Pod will use the DNS settings configured in its <code>dnsConfig</code> field.</li>
</ul>
<p><strong>Note:</strong> The following focuses on the <code>ClusterFirst</code> mode</p>
<h3 id="33-dns-resolution-rules">3.3 DNS resolution rules</h3>
<p>DNS queries refer to the <code>/etc/resolv.conf</code> configuration in the Pod, which is generated by the kubelet for each Pod. Therefore, each pod has a <code>/etc/resolv.conf</code> file like the following, where you can change the DNS query rules by modifying the configuration.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">nameserver 10.32.0.10
</span></span><span class="line"><span class="cl">search &lt;namespace&gt;.svc.cluster.local svc.cluster.local cluster.local
</span></span><span class="line"><span class="cl">options ndots:5
</span></span></code></pre></td></tr></table>
</div>
</div><p>There are several points to note in the configuration here.</p>
<ul>
<li><code>nameserver</code>: the IP of the DNS server in the cluster, typically the <code>ClusterIP</code> of <code>CoreDNS</code></li>
<li><code>search</code>: the domain to be searched, by default it will be added level by level starting from the namespace the pod belongs to</li>
<li><code>options ndots</code>: the number of domain points to trigger the above <code>search</code>, default is 1, the upper limit is 15, in K8S is generally 5; for example, in Linux <code>tinychen.com</code> the domain <code>ndots</code> is 1, <code>tinychen.com.</code> the domain <code>ndots</code> is only 2 (need to note that all domains actually have a root domain <code>.</code>, so the full name of <code>tinychen.com</code> should be <code>tinychen.com.</code>)</li>
</ul>
<p>This is a more general case, let&rsquo;s look at a more specific configuration</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># 首先进入一个pod查看里面的DNS解析配置</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@tiny-calico-master-88-1 tiny-calico<span class="o">]</span><span class="c1"># kubectl exec -it -n ngx-system ngx-ex-deploy-6bf6c99d95-5qh2w /bin/bash</span>
</span></span><span class="line"><span class="cl">kubectl <span class="nb">exec</span> <span class="o">[</span>POD<span class="o">]</span> <span class="o">[</span>COMMAND<span class="o">]</span> is DEPRECATED and will be removed in a future version. Use kubectl <span class="nb">exec</span> <span class="o">[</span>POD<span class="o">]</span> -- <span class="o">[</span>COMMAND<span class="o">]</span> instead.
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ngx-ex-deploy-6bf6c99d95-5qh2w /<span class="o">]</span><span class="c1"># cat /etc/resolv.conf</span>
</span></span><span class="line"><span class="cl">nameserver 10.88.0.10
</span></span><span class="line"><span class="cl">search ngx-system.svc.cali-cluster.tclocal svc.cali-cluster.tclocal cali-cluster.tclocal k8s.tcinternal
</span></span><span class="line"><span class="cl">options ndots:5
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ngx-ex-deploy-6bf6c99d95-5qh2w /<span class="o">]</span><span class="c1"># exit</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>The <code>/etc/resolv.conf</code> configuration file inside this pod has two differences from the previous one.</p>
<ul>
<li>
<p><code>cluster.local</code> becomes <code>cali-cluster.tclocal</code> Here we can see that the configuration of the coredns is the same <code>cali-cluster.tclocal</code>, that is, the configuration in <code>/etc/resolv.conf</code> is actually the same as the configuration, or more precisely, the same cluster name that was configured when the K8S cluster was initialized</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># 再查看K8S集群中的coredns的configmap</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@tiny-calico-master-88-1 tiny-calico<span class="o">]</span><span class="c1"># kubectl get configmaps -n kube-system coredns -oyaml</span>
</span></span><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">data:
</span></span><span class="line"><span class="cl">Corefile: <span class="p">|</span>
</span></span><span class="line"><span class="cl">    .:53 <span class="o">{</span>
</span></span><span class="line"><span class="cl">        errors
</span></span><span class="line"><span class="cl">        health <span class="o">{</span>
</span></span><span class="line"><span class="cl">        lameduck 5s
</span></span><span class="line"><span class="cl">        <span class="o">}</span>
</span></span><span class="line"><span class="cl">        ready
</span></span><span class="line"><span class="cl">        kubernetes cali-cluster.tclocal in-addr.arpa ip6.arpa <span class="o">{</span>
</span></span><span class="line"><span class="cl">        pods insecure
</span></span><span class="line"><span class="cl">        fallthrough in-addr.arpa ip6.arpa
</span></span><span class="line"><span class="cl">        ttl <span class="m">30</span>
</span></span><span class="line"><span class="cl">        <span class="o">}</span>
</span></span><span class="line"><span class="cl">        prometheus :9153
</span></span><span class="line"><span class="cl">        forward . 10.31.100.100 <span class="o">{</span>
</span></span><span class="line"><span class="cl">        max_concurrent <span class="m">1000</span>
</span></span><span class="line"><span class="cl">        <span class="o">}</span>
</span></span><span class="line"><span class="cl">        cache <span class="m">30</span>
</span></span><span class="line"><span class="cl">        loop
</span></span><span class="line"><span class="cl">        reload
</span></span><span class="line"><span class="cl">        loadbalance
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">kind: ConfigMap
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">creationTimestamp: <span class="s2">&#34;2022-05-06T05:19:08Z&#34;</span>
</span></span><span class="line"><span class="cl">name: coredns
</span></span><span class="line"><span class="cl">namespace: kube-system
</span></span><span class="line"><span class="cl">resourceVersion: <span class="s2">&#34;3986029&#34;</span>
</span></span><span class="line"><span class="cl">uid: 54f5f803-a5ab-4c77-b149-f02229bcad0a
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>search</code> adds a new <code>k8s.tcinternal</code></p>
<p>In fact, when we look at the DNS configuration rules of the K8S host node, we find that <code>k8s.tcinternal</code> is inherited from the host.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># 最后查看宿主机节点上面的DNS解析配置</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@tiny-calico-master-88-1 tiny-calico<span class="o">]</span><span class="c1"># cat /etc/resolv.conf</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generated by NetworkManager</span>
</span></span><span class="line"><span class="cl">search k8s.tcinternal
</span></span><span class="line"><span class="cl">nameserver 10.31.254.253
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="34-dns-resolution-process">3.4 DNS resolution process</h3>
<blockquote>
<p>Warm tip: When reading this part, pay special attention to whether there is a dot at the end of the domain name <code>.</code></p>
</blockquote>
<h4 id="when-ndots-is-smaller-than-options-ndots">When ndots is smaller than options ndots</h4>
<p>As we said before the value of options ndots is 1 by default and 5 in K8S, for obvious effect we use 5 in K8S as an example here.</p>
<p>Here again, there are two SVCs in a namespace <code>demo-ns</code>, <code>demo-svc1</code> and <code>demo-svc2</code>, so their <code>/etc/resolv.conf</code> should look like the following.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">nameserver 10.32.0.10
</span></span><span class="line"><span class="cl">search demo-ns.svc.cluster.local svc.cluster.local cluster.local
</span></span><span class="line"><span class="cl">options ndots:5
</span></span></code></pre></td></tr></table>
</div>
</div><p>We directly request the domain name <code>demo-svc2</code> in <code>demo-svc1</code>, at this time ndots is 1, less than 5 in the configuration, so it will trigger the <code>search</code> rule above, at this time the first resolved domain name is <code>demo-svc2.demo-ns.svc.cluster.local</code>, when the resolution is not continue the following <code>demo-svc2.svc.cluster.local</code>, <code>demo-svc2.cluster.local</code>, and finally only then is it directly to resolve <code>demo-svc2.</code>.</p>
<p>Note that the above rules apply to any domain name, that is, when we try to access an external domain such as <code>tinychen.com</code> in the pod, the above queries will also be performed in turn.</p>
<h4 id="when-ndots-is-greater-than-or-equal-to-options-ndots">When ndots is greater than or equal to options ndots</h4>
<p>We directly request the domain <code>demo-svc2.demo-ns.svc.cluster.local</code> in <code>demo-svc1</code>, the ndots is 4 at this time, which still triggers the search rule above.</p>
<p>And requesting the domain <code>demo-svc2.demo-ns.svc.cluster.local.</code>, the ndots is 5, which is equal to 5 in the configuration, so it will not trigger the <code>search</code> rule above, and will go directly to resolve the domain <code>demo-svc2.demo-ns.svc.cluster.local.</code> and return results.</p>
<p>If we request a longer domain name such as POD domain <code>pod-1.demo-svc2.demo-ns.svc.cluster.local.</code>, the ndots at this time is 6, which is greater than the 5 in the configuration, so it will not trigger the <code>search</code> rule above either, but will directly query the domain name and return the resolution.</p>
<h4 id="summary">Summary</h4>
<p>From the above analysis, we can easily conclude the following points.</p>
<ul>
<li>Services within the same namespace (namespace) access each other directly through <code>$service_name</code> without using a full domain name (FQDN), when DNS resolution is fastest.</li>
<li>Services across namespaces (namespace) can access each other via <code>$service_name.$namespace_name</code>, when <strong>DNS resolution fails on the first query and only matches the correct domain name on the second</strong>.</li>
<li>Access between all services via full domain name (FQDN) <code>$service_name.$namespace_name.svc.$cluster_name.</code> when <strong>DNS resolution is fastest</strong>.</li>
<li>Accessing most common external domains (ndots less than 5) within a K8S cluster triggers the <code>search</code> rule, so you can use FQDN when accessing external domains, i.e., configure a dotted number at the end of the domain name <code>.</code></li>
</ul>
<h2 id="4-four-tier-service-exposure">4, four-tier service exposure</h2>
<p>For exposing services in K8S clusters to provide services outside the cluster, the general way can be divided into <strong>Four-layer Service Exposure</strong> and <strong>Seven-layer Service Exposure</strong>, because the former is generally the basis of the latter, so here we start with an introduction to four-layer service exposure.</p>
<p>Before we start we need to clarify the concept of four layers, here four layers refers to the fourth layer in the OSI seven-layer model, that is, the transport layer in which the TCP and UDP protocols are located, that is, we often say <code>protocol + IP + port</code> level of load balancing, common four-layer load balancers are LVS, DPVS, Haproxy (four layers and seven layers are available), nginx (four layers and seven layers are available ), etc. <strong>The two most common means of four-tier service exposure in K8S are Nodeport and LoadBalancer</strong>, which we mentioned earlier.</p>
<p>Let&rsquo;s look at this architecture diagram below, and note that the entire blue dotted line range is a K8S cluster. To facilitate differentiation, we assume here that the traffic coming in from outside the cluster is all north-south traffic (client-server traffic), and the traffic inside the cluster is all east-west traffic (server-server traffic).</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/19916fa61d35458088693ab1816cd1c6.png" alt="four-tier service exposure"></p>
<p>Let&rsquo;s start from the bottom to the top</p>
<ul>
<li>The figure has multiple namespaces such as <code>frontend</code>, <code>backend</code>, <code>devops</code>, etc., which are common means to isolate different resources, and can be divided according to different businesses and people using them in actual landing scenarios, and the specific division dimensions and criteria are best depending on the actual business situation.</li>
<li>In fact, not only workload, service will also be divided according to different namespace, including most api in K8S cluster, we need to specify namespace when looking for</li>
<li>In the k8s service layer, the diagram mainly shows the Cluster-IP and Headless methods for internal cluster access</li>
<li>Note that the Headless service is exclusive to the other three service types, and it is not load balanced by kube-proxy, so the blue solid box for the Headless service is blank, while the Cluster-IP is the kube-proxy component</li>
<li>CoreDNS can be used to provide naming services starting with v1.11 of K8S and replacing <code>kube-dns</code> as the default DNS service starting with v1.13.</li>
<li>In Kubernetes version 1.21, kubeadm removed support for using <code>kube-dns</code> as a DNS application. For <code>kubeadm</code> v1.24, the only cluster DNS application supported is CoreDNS</li>
<li>CoreDNS itself is also a <code>workload</code>, which is a <code>deployments.apps</code> in the <code>kube-system</code> namespace</li>
<li>CoreDNS also gets information about services within the k8s cluster by requesting the api-server within the k8s cluster</li>
<li>Further up is located at the boundary of the entire K8S cluster, where there is inevitably an api-server that exposes the control interface to both inside and outside the cluster, and through which we can get information about the K8S cluster</li>
<li>api-server itself does not store information, most of the cluster information of the K8S cluster itself is stored in the etcd service, and api-server will go to the etcd to read the relevant data and return it.</li>
<li>The last is the service used to expose the four layers of services, usually NodePort or LoadBalancer, because of the port and IP and other reasons, most of them are actually exposed out in the way of LoadBalancer when they are used.</li>
<li>The LoadBalancer service is not exposed to the outside world as an IP, but as an <code>IP+port</code>, which means that actually multiple ports of an IP can provide services for different types of services, such as ports 80 and 443 for http/https services, 3306 for database services, etc.</li>
<li>K8S does not have a built-in LoadBalancer, so you need to implement LoadBalancer, currently there are two mainstream ways: one is to use the LoadBalancer provided by cloud vendors such as AWS, Azure, Ali Tencent, etc. These LoadBalancers are basically closed source solutions, basically only applicable to their own cloud environment The second is to use some existing open source LoadBalancer, mainly MetalLB, OpenELB and PureLB.</li>
<li>Open source LoadBalancer basically has two main modes of operation: Layer2 mode and BGP mode. Whether Layer2 mode or BGP mode, the core idea is to somehow direct traffic from a specific VIP to the k8s cluster, and then forward the traffic to a specific service behind it via kube-proxy.</li>
</ul>
<h2 id="5-seven-tier-service-exposure">5, seven-tier service exposure</h2>
<p>The advantage of the four-tier LoadBalancer service exposure approach is that it is widely applicable, because it works on four tiers and therefore can be adapted to almost all types of applications. But there are also some disadvantages.</p>
<ul>
<li>For most application scenarios are http protocol requests, it is not necessary to configure an <code>EXTERNAL-IP</code> for each service to expose the service, which is a serious waste of resources (public IP is very precious), and includes IP address has been HTTPS using certificate management, etc. are very troublesome</li>
<li>A more common scenario is to configure a domain name (virtual host) or routing rule for each configuration, then expose one or a few <code>EXTERNAL-IPs</code> to the public, and import all the request traffic to a centralized entry gateway (e.g. Nginx), which will then perform various load balancing, routing rule, certificate management (SSL termination), etc.</li>
</ul>
<p>In K8S, these things are generally done by ingress.</p>
<h3 id="51-ingress">5.1 Ingress</h3>
<p>Ingress is an API object that manages external access to services in a cluster, typically through HTTP. Ingress can provide load balancing, routing rule, certificate management (SSL termination), etc. Ingress exposes HTTP and HTTPS routing from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on Ingress resources.</p>
<p>The following diagram is an official K8S diagram of how ingress works.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/f1b12d6a2bac4cdead49b46c5dd89cfa.png" alt="official K8S diagram of how ingress works"></p>
<p>Here we need to pay attention to the distinction between <code>Ingress</code> and <code>Ingress Controllers</code>, which are two different concepts and are not equivalent</p>
<ul>
<li>From a conceptual point of view, <code>Ingress</code> is very similar to the aforementioned service, and Ingress itself is not a real workload.</li>
<li>And <code>Ingress Controllers</code> are more inclined to be deployed in K8S clusters with some special gateways (such as NGX), take the officially maintained <a href="https://kubernetes.github.io/ingress-nginx/"><code>ingress-nginx</code></a> of K8S as an example, <strong>it is essentially is actually a special deployments with Ingress resource type</strong>, and <code>Ingress Controllers</code> is a concrete implementation of <code>Ingress</code>.</li>
</ul>
<h3 id="52-ingress-controllers">5.2 Ingress Controllers</h3>
<p>We learned above that the so-called <code>Ingress Controllers</code> themselves are actually special workloads (generally deployment), so they themselves need to be exposed to the cluster in some way in order to provide services, and here they are generally chosen to be exposed through LoadBalancer for four layers of services.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/07/13/5e6f4fd4da554ae3927c125e6e72396c.png" alt="Ingress Controllers"></p>
<p>For the above four-layer service exposure architecture diagram, we add an Ingress after the LoadBalancer at the entrance to get the seven-layer Ingress service exposure architecture diagram.</p>
<ul>
<li>The processing logic in this diagram is the same as the four-tier service exposure, the only difference is that the HTTP protocol traffic is first passed through the loadbalancer at the entrance and then forwarded to the ingress, which is then forwarded according to the ingress rule inside to make a judgment.</li>
<li>The ingress-nginx of k8s will communicate with the api-server in the cluster and get the service information, because <code>Ingress Controllers</code> itself has the ability of load balancing, so when forwarding the traffic to the specific service in the backend, it will not go through the ClusterIP (even if the service type is ClusterIP). does not pass through), but is forwarded directly to the <code>Pod IP</code> to which it belongs.</li>
</ul>
<h2 id="6-summary">6. Summary</h2>
<p>Here the basic service exposure of K8S need to understand the knowledge is introduced. Since K8S itself is indeed very complex, this article can only scratch the surface when introducing it. With the continuous development of K8S, the ordinary service exposure is now not good enough to meet the high-end needs of some scenarios, which then triggered a lot of evolution such as service mesh, sidecar, sidecarless, and so on.</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/k8s/">k8s</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2022-07/rockylinux-quick-start/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Rockylinux Quick Start</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2022-07/k8s-03-deploy-k8s-with-calico/">
            <span class="next-text nav-default">Kubeadm Deployment k8s Cluster &#43; calico</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>



<script type="text/javascript" src="/js/main.min.64437849d125a2d603b3e71d6de5225d641a32d17168a58106e0b61852079683.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
