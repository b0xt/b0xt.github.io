<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Kubeadm Deployment k8s Cluster &#43; Flannel - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="Learn how to deploy kubernetes clusters &#43; Flannel plugins on CentOS using kubeadm." /><meta name="keywords" content="Kubernetes Cluster, kubeadm, flannel" />






<meta name="generator" content="Hugo 0.102.0 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2022-07/k8s-02-deploy-k8s-with-flannel/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">


<meta property="og:title" content="Kubeadm Deployment k8s Cluster &#43; Flannel" />
<meta property="og:description" content="Learn how to deploy kubernetes clusters &#43; Flannel plugins on CentOS using kubeadm." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2022-07/k8s-02-deploy-k8s-with-flannel/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-07-12T12:55:28+08:00" />
<meta property="article:modified_time" content="2022-07-12T12:55:28+08:00" />

<meta itemprop="name" content="Kubeadm Deployment k8s Cluster &#43; Flannel">
<meta itemprop="description" content="Learn how to deploy kubernetes clusters &#43; Flannel plugins on CentOS using kubeadm."><meta itemprop="datePublished" content="2022-07-12T12:55:28+08:00" />
<meta itemprop="dateModified" content="2022-07-12T12:55:28+08:00" />
<meta itemprop="wordCount" content="4311">
<meta itemprop="keywords" content="k8s," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubeadm Deployment k8s Cluster &#43; Flannel"/>
<meta name="twitter:description" content="Learn how to deploy kubernetes clusters &#43; Flannel plugins on CentOS using kubeadm."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SOBYTE</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SOBYTE</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Kubeadm Deployment k8s Cluster &#43; Flannel</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-07-12 12:55:28 </span>
        <div class="post-category">
            <a href="/categories/tutorials/"> tutorials </a>
            </div>
          <span class="more-meta"> 4311 words </span>
          <span class="more-meta"> 21 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-preparation">1. Preparation</a>
          <ul>
            <li><a href="#11-flannel-cluster-node-information">1.1 flannel-cluster node information</a></li>
            <li><a href="#12-checking-mac-and-product_uuid">1.2 Checking mac and product_uuid</a></li>
            <li><a href="#13-configure-ssh-password-free-login-optional">1.3 Configure ssh password-free login (optional)</a></li>
            <li><a href="#14-modify-the-hosts-file">1.4 Modify the hosts file</a></li>
            <li><a href="#15-turn-off-swap-memory">1.5 Turn off swap memory</a></li>
            <li><a href="#16-configure-time-synchronization">1.6 Configure time synchronization</a></li>
            <li><a href="#17-shutting-down-selinux">1.7 Shutting down selinux</a></li>
            <li><a href="#18-configuring-firewalls">1.8 Configuring Firewalls</a></li>
            <li><a href="#19-configuring-netfilter-parameters">1.9 Configuring netfilter parameters</a></li>
            <li><a href="#110-turn-off-ipv6-optional">1.10 Turn off IPV6 (optional)</a></li>
            <li><a href="#111-configuring-ipvs-optional">1.11 Configuring IPVS (optional)</a></li>
          </ul>
        </li>
        <li><a href="#2-install-container-runtime">2. Install container runtime</a>
          <ul>
            <li><a href="#21-installing-docker">2.1 Installing docker</a></li>
            <li><a href="#22-configuring-cgroup-drivers">2.2 Configuring cgroup drivers</a></li>
            <li><a href="#23-about-kubelets-cgroup-driver">2.3 About kubelet&rsquo;s cgroup driver</a></li>
          </ul>
        </li>
        <li><a href="#3-installing-the-kube-triplet">3. Installing the kube triplet</a></li>
        <li><a href="#4-initialize-the-cluster">4. Initialize the cluster</a>
          <ul>
            <li><a href="#41-writing-the-configuration-file">4.1 Writing the configuration file</a></li>
            <li><a href="#42-initialize-the-cluster">4.2 Initialize the cluster</a></li>
            <li><a href="#43-configuring-kubeconfig">4.3 Configuring kubeconfig</a></li>
            <li><a href="#44-adding-worker-nodes">4.4 Adding worker nodes</a></li>
          </ul>
        </li>
        <li><a href="#5-install-cni">5. Install CNI</a>
          <ul>
            <li><a href="#51-writing-the-manifest-file">5.1 Writing the manifest file</a></li>
            <li><a href="#52-deploying-flannel">5.2 Deploying flannel</a></li>
          </ul>
        </li>
        <li><a href="#6-deploy-test-cases">6. Deploy test cases</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>This article mainly deploys <code>v1.23.6</code> version of k8s native cluster based on <code>docker</code> and <code>flannel</code> components on centos7 system, because the cluster is mainly used for own learning and testing, plus limited resources, not involved in high availability deployment for now.</p>
<h2 id="1-preparation">1. Preparation</h2>
<h3 id="11-flannel-cluster-node-information">1.1 flannel-cluster node information</h3>
<p>The machines are all 8C8G virtual machines with 100G hard disk.</p>
<table>
<thead>
<tr>
<th>IP</th>
<th>Hostname</th>
</tr>
</thead>
<tbody>
<tr>
<td>10.31.8.1</td>
<td>tiny-flannel-master-8-1.k8s.tcinternal</td>
</tr>
<tr>
<td>10.31.8.11</td>
<td>tiny-flannel-worker-8-11.k8s.tcinternal</td>
</tr>
<tr>
<td>10.31.8.12</td>
<td>tiny-flannel-worker-8-12.k8s.tcinternal</td>
</tr>
<tr>
<td>10.8.64.0/18</td>
<td>podSubnet</td>
</tr>
<tr>
<td>10.8.0.0/18</td>
<td>serviceSubnet</td>
</tr>
</tbody>
</table>
<h3 id="12-checking-mac-and-product_uuid">1.2 Checking mac and product_uuid</h3>
<p>All nodes in the same k8s cluster need to make sure that both <code>mac</code> address and <code>product_uuid</code> are unique, so you need to check the relevant information before starting cluster initialization.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Check mac address</span>
</span></span><span class="line"><span class="cl">ip link 
</span></span><span class="line"><span class="cl">ifconfig -a
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check product_uuid</span>
</span></span><span class="line"><span class="cl">sudo cat /sys/class/dmi/id/product_uuid
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="13-configure-ssh-password-free-login-optional">1.3 Configure ssh password-free login (optional)</h3>
<p>If the nodes of the k8s cluster have multiple NICs, ensure that each node can be accessed through the correct NIC interconnect.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Generate a public key under the root user, and configure the key to be used for password-free login</span>
</span></span><span class="line"><span class="cl">su root
</span></span><span class="line"><span class="cl">ssh-keygen
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/.ssh/
</span></span><span class="line"><span class="cl">cat id_rsa.pub &gt;&gt; authorized_keys
</span></span><span class="line"><span class="cl">chmod <span class="m">600</span> authorized_keys
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">cat &gt;&gt; ~/.ssh/config <span class="s">&lt;&lt;EOF
</span></span></span><span class="line"><span class="cl"><span class="s">Host tiny-flannel-master-8-1.k8s.tcinternal
</span></span></span><span class="line"><span class="cl"><span class="s">    HostName 10.31.8.1
</span></span></span><span class="line"><span class="cl"><span class="s">    User root
</span></span></span><span class="line"><span class="cl"><span class="s">    Port 22
</span></span></span><span class="line"><span class="cl"><span class="s">    IdentityFile ~/.ssh/id_rsa
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">Host tiny-flannel-worker-8-11.k8s.tcinternal
</span></span></span><span class="line"><span class="cl"><span class="s">    HostName 10.31.8.11
</span></span></span><span class="line"><span class="cl"><span class="s">    User root
</span></span></span><span class="line"><span class="cl"><span class="s">    Port 22
</span></span></span><span class="line"><span class="cl"><span class="s">    IdentityFile ~/.ssh/id_rsa
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">Host tiny-flannel-worker-8-12.k8s.tcinternal
</span></span></span><span class="line"><span class="cl"><span class="s">    HostName 10.31.8.12
</span></span></span><span class="line"><span class="cl"><span class="s">    User root
</span></span></span><span class="line"><span class="cl"><span class="s">    Port 22
</span></span></span><span class="line"><span class="cl"><span class="s">    IdentityFile ~/.ssh/id_rsa
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="14-modify-the-hosts-file">1.4 Modify the hosts file</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">cat &gt;&gt; /etc/hosts <span class="s">&lt;&lt;EOF
</span></span></span><span class="line"><span class="cl"><span class="s">10.31.8.1  tiny-flannel-master-8-1 tiny-flannel-master-8-1.k8s.tcinternal
</span></span></span><span class="line"><span class="cl"><span class="s">10.31.8.11 tiny-flannel-worker-8-11 tiny-flannel-worker-8-11.k8s.tcinternal
</span></span></span><span class="line"><span class="cl"><span class="s">10.31.8.12 tiny-flannel-worker-8-12 tiny-flannel-worker-8-12.k8s.tcinternal
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="15-turn-off-swap-memory">1.5 Turn off swap memory</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Use the command to turn off swap memory directly</span>
</span></span><span class="line"><span class="cl">swapoff -a
</span></span><span class="line"><span class="cl"><span class="c1"># Modify fstab file to disable automatic mounting of swap partition on boot</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s1">&#39;/swap / s/^\(.*\)$/#\1/g&#39;</span> /etc/fstab
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="16-configure-time-synchronization">1.6 Configure time synchronization</h3>
<p>Here you can choose either ntp or chrony synchronization according to your custom, and the synchronized time source server can choose Aliyun&rsquo;s <code>ntp1.aliyun.com</code> or National Time Center&rsquo;s <code>ntp.ntsc.ac.cn</code>.</p>
<h4 id="use-ntp-to-synchronize">Use ntp to synchronize</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Install ntpdate tool using yum</span>
</span></span><span class="line"><span class="cl">yum install ntpdate -y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Synchronize time using the source of the national time center</span>
</span></span><span class="line"><span class="cl">ntpdate ntp.ntsc.ac.cn
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check the time at last</span>
</span></span><span class="line"><span class="cl">hwclock
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="synchronize-with-chrony">Synchronize with chrony</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Install chrony with yum</span>
</span></span><span class="line"><span class="cl">yum install chrony -y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set boot up and turn on chony and check running status</span>
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> chronyd.service
</span></span><span class="line"><span class="cl">systemctl start chronyd.service
</span></span><span class="line"><span class="cl">systemctl status chronyd.service
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Of course you can also customize the time server</span>
</span></span><span class="line"><span class="cl">vim /etc/chrony.conf
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Before modification</span>
</span></span><span class="line"><span class="cl">$ grep server /etc/chrony.conf
</span></span><span class="line"><span class="cl"><span class="c1"># Use public servers from the pool.ntp.org project.</span>
</span></span><span class="line"><span class="cl">server 0.centos.pool.ntp.org iburst
</span></span><span class="line"><span class="cl">server 1.centos.pool.ntp.org iburst
</span></span><span class="line"><span class="cl">server 2.centos.pool.ntp.org iburst
</span></span><span class="line"><span class="cl">server 3.centos.pool.ntp.org iburst
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># After modification</span>
</span></span><span class="line"><span class="cl">$ grep server /etc/chrony.conf
</span></span><span class="line"><span class="cl"><span class="c1"># Use public servers from the pool.ntp.org project.</span>
</span></span><span class="line"><span class="cl">server ntp.ntsc.ac.cn iburst
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Restart the service to make the configuration file take effect</span>
</span></span><span class="line"><span class="cl">systemctl restart chronyd.service
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># View chrony&#39;s ntp server status</span>
</span></span><span class="line"><span class="cl">chronyc sourcestats -v
</span></span><span class="line"><span class="cl">chronyc sources -v
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="17-shutting-down-selinux">1.7 Shutting down selinux</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Close directly using the command</span>
</span></span><span class="line"><span class="cl">setenforce <span class="m">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># You can also modify the /etc/selinux/config file directly</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s1">&#39;s/^SELINUX=enforcing$/SELINUX=disabled/&#39;</span> /etc/selinux/config
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="18-configuring-firewalls">1.8 Configuring Firewalls</h3>
<p>Communication and service exposure between k8s clusters requires the use of more ports, so for convenience, disable the firewall directly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># centos7 use systemctl to disable the default firewalld service</span>
</span></span><span class="line"><span class="cl">systemctl disable firewalld.service
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="19-configuring-netfilter-parameters">1.9 Configuring netfilter parameters</h3>
<p>The main thing here is to configure the kernel to load <code>br_netfilter</code> and <code>iptables</code> to release <code>ipv6</code> and <code>ipv4</code> traffic to ensure that the containers in the cluster can communicate properly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span></span></span><span class="line"><span class="cl"><span class="s">br_netfilter
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span class="line"><span class="cl"><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span class="line"><span class="cl"><span class="s">net.bridge.bridge-nf-call-iptables = 1
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span><span class="line"><span class="cl">sudo sysctl --system
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="110-turn-off-ipv6-optional">1.10 Turn off IPV6 (optional)</h3>
<p>Although newer versions of k8s already support dual-stack networks, this cluster deployment process does not involve communication over IPv6 networks, so turn off IPv6 network support.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Add the ipv6 disable parameter directly to the kernel</span>
</span></span><span class="line"><span class="cl">grubby --update-kernel<span class="o">=</span>ALL --args<span class="o">=</span>ipv6.disable<span class="o">=</span><span class="m">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="111-configuring-ipvs-optional">1.11 Configuring IPVS (optional)</h3>
<p>IPVS is a component specifically designed to cope with load balancing scenarios. <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md#run-kube-proxy-in-ipvs-mode">IPVS implementation in kube-proxy</a> increases scalability by reducing the use of iptables. Instead of using PREROUTING in the iptables input chain, a dummy interface is created called kube-ipvs0, which enables IPVS to achieve more efficient forwarding performance than iptables when the load balancing configuration in a k8s cluster becomes more numerous.</p>
<blockquote>
<p><strong>Notes</strong> : use <code>nf_conntrack</code> instead of <code>nf_conntrack_ipv4</code> for Linux kernel 4.19 and later.</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Make sure ipset and ipvsadm are installed before using ipvs mode</span>
</span></span><span class="line"><span class="cl">sudo yum install ipset ipvsadm -y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Manually load ipvs related modules</span>
</span></span><span class="line"><span class="cl">modprobe -- ip_vs
</span></span><span class="line"><span class="cl">modprobe -- ip_vs_rr
</span></span><span class="line"><span class="cl">modprobe -- ip_vs_wrr
</span></span><span class="line"><span class="cl">modprobe -- ip_vs_sh
</span></span><span class="line"><span class="cl">modprobe -- nf_conntrack_ipv4
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Configure boot up to automatically load ipvs related modules</span>
</span></span><span class="line"><span class="cl">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/ipvs.conf
</span></span></span><span class="line"><span class="cl"><span class="s">ip_vs
</span></span></span><span class="line"><span class="cl"><span class="s">ip_vs_rr
</span></span></span><span class="line"><span class="cl"><span class="s">ip_vs_wrr
</span></span></span><span class="line"><span class="cl"><span class="s">ip_vs_sh
</span></span></span><span class="line"><span class="cl"><span class="s">nf_conntrack_ipv4
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sudo sysctl --system
</span></span><span class="line"><span class="cl"><span class="c1"># It&#39;s best to reboot the system to make sure it works</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ lsmod <span class="p">|</span> grep -e ip_vs -e nf_conntrack_ipv4
</span></span><span class="line"><span class="cl">ip_vs_sh               <span class="m">12688</span>  <span class="m">0</span>
</span></span><span class="line"><span class="cl">ip_vs_wrr              <span class="m">12697</span>  <span class="m">0</span>
</span></span><span class="line"><span class="cl">ip_vs_rr               <span class="m">12600</span>  <span class="m">0</span>
</span></span><span class="line"><span class="cl">ip_vs                 <span class="m">145458</span>  <span class="m">6</span> ip_vs_rr,ip_vs_sh,ip_vs_wrr
</span></span><span class="line"><span class="cl">nf_conntrack_ipv4      <span class="m">15053</span>  <span class="m">2</span>
</span></span><span class="line"><span class="cl">nf_defrag_ipv4         <span class="m">12729</span>  <span class="m">1</span> nf_conntrack_ipv4
</span></span><span class="line"><span class="cl">nf_conntrack          <span class="m">139264</span>  <span class="m">7</span> ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4
</span></span><span class="line"><span class="cl">libcrc32c              <span class="m">12644</span>  <span class="m">4</span> xfs,ip_vs,nf_nat,nf_conntrack
</span></span><span class="line"><span class="cl">$ cut -f1 -d <span class="s2">&#34; &#34;</span>  /proc/modules <span class="p">|</span> grep -e ip_vs -e nf_conntrack_ipv4
</span></span><span class="line"><span class="cl">ip_vs_sh
</span></span><span class="line"><span class="cl">ip_vs_wrr
</span></span><span class="line"><span class="cl">ip_vs_rr
</span></span><span class="line"><span class="cl">ip_vs
</span></span><span class="line"><span class="cl">nf_conntrack_ipv4
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="2-install-container-runtime">2. Install container runtime</h2>
<h3 id="21-installing-docker">2.1 Installing docker</h3>
<p>Detailed official documentation can be found <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">here</a>, as <code>docker-shim</code> was removed in the just released version 1.24, so the installation of <code>version ≥ 1.24</code> needs to pay attention to the <code>container runtime</code> selection. Here we have installed a version lower than 1.24, so we continue to use docker.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Install the necessary dependencies and import the official docker yum source</span>
</span></span><span class="line"><span class="cl">sudo yum install -y yum-utils device-mapper-persistent-data lvm2
</span></span><span class="line"><span class="cl">sudo yum-config-manager --add-repo  https://download.docker.com/linux/centos/docker-ce.repo
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># We install the latest version of docker directly</span>
</span></span><span class="line"><span class="cl">yum install docker-ce docker-ce-cli containerd.io
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="22-configuring-cgroup-drivers">2.2 Configuring cgroup drivers</h3>
<p>CentOS 7 uses <code>systemd</code> to initialize the system and manage processes. Initializing processes generates and uses a root control group (<code>cgroup</code>), and acts as a <code>cgroup</code> manager. <code>Systemd</code> is tightly integrated with <code>cgroup</code> and will assign a <code>cgroup</code> to each <code>systemd</code> unit. We can also configure the <code>container runtime</code> and <code>kubelet</code> to use <code>cgroupfs</code>. Using <code>cgroupfs</code> with <code>systemd</code> means that there will be two different <code>cgroup managers</code>. When both cgroupfs and systemd are present on a system, it tends to become unstable, so it is best to change the settings so that the container runtime and kubelet use <code>systemd</code> as the <code>cgroup</code> driver to make the system more stable. For Docker, you need to set the <code>native.cgroupdriver=systemd</code> parameter.</p>
<ul>
<li>Refer to the official documentation: <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers">https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers</a></li>
<li>Refer to the configuration instructions document: <a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker">https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker</a></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">sudo mkdir /etc/docker
</span></span><span class="line"><span class="cl">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/docker/daemon.json
</span></span></span><span class="line"><span class="cl"><span class="s">{
</span></span></span><span class="line"><span class="cl"><span class="s">  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span></span></span><span class="line"><span class="cl"><span class="s">  &#34;log-driver&#34;: &#34;json-file&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s">  &#34;log-opts&#34;: {
</span></span></span><span class="line"><span class="cl"><span class="s">    &#34;max-size&#34;: &#34;100m&#34;
</span></span></span><span class="line"><span class="cl"><span class="s">  },
</span></span></span><span class="line"><span class="cl"><span class="s">  &#34;storage-driver&#34;: &#34;overlay2&#34;
</span></span></span><span class="line"><span class="cl"><span class="s">}
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> docker
</span></span><span class="line"><span class="cl">sudo systemctl daemon-reload
</span></span><span class="line"><span class="cl">sudo systemctl restart docker
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Lastly, check if the Cgroup Driver is systemd</span>
</span></span><span class="line"><span class="cl">$ docker info <span class="p">|</span> grep systemd
</span></span><span class="line"><span class="cl"> Cgroup Driver: systemd
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="23-about-kubelets-cgroup-driver">2.3 About kubelet&rsquo;s cgroup driver</h3>
<p>k8s has <a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/">detailed documentation</a> on how to set kubelet&rsquo;s <code>cgroup driver</code>. Note in particular that starting with version 1.22, if the kubelet cgroup driver is not set manually, it will be set to systemd by default.</p>
<blockquote>
<p><strong>Note:</strong> In v1.22, if the user is not setting the <code>cgroupDriver</code> field under <code>KubeletConfiguration</code> , <code>kubeadm</code> will default it to <code>systemd</code> .</p>
</blockquote>
<p>A simpler way to specify a <code>cgroup driver</code> for a kubelet is to add a <code>cgroupDriver</code> field to <code>kubeadm-config.yaml</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="c"># kubeadm-config.yaml</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterConfiguration</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kubeadm.k8s.io/v1beta3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kubernetesVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1.21.0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">KubeletConfiguration</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kubelet.config.k8s.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">cgroupDriver</span><span class="p">:</span><span class="w"> </span><span class="l">systemd</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>We can check the configmaps directly to see the kubeadm-config configuration of the cluster after initialization.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl describe configmaps kubeadm-config -n kube-system
</span></span><span class="line"><span class="cl">Name:         kubeadm-config
</span></span><span class="line"><span class="cl">Namespace:    kube-system
</span></span><span class="line"><span class="cl">Labels:       &lt;none&gt;
</span></span><span class="line"><span class="cl">Annotations:  &lt;none&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Data</span>
</span></span><span class="line"><span class="cl"><span class="o">====</span>
</span></span><span class="line"><span class="cl">ClusterConfiguration:
</span></span><span class="line"><span class="cl">----
</span></span><span class="line"><span class="cl">apiServer:
</span></span><span class="line"><span class="cl">  extraArgs:
</span></span><span class="line"><span class="cl">    authorization-mode: Node,RBAC
</span></span><span class="line"><span class="cl">  timeoutForControlPlane: 4m0s
</span></span><span class="line"><span class="cl">apiVersion: kubeadm.k8s.io/v1beta3
</span></span><span class="line"><span class="cl">certificatesDir: /etc/kubernetes/pki
</span></span><span class="line"><span class="cl">clusterName: kubernetes
</span></span><span class="line"><span class="cl">controllerManager: <span class="o">{}</span>
</span></span><span class="line"><span class="cl">dns: <span class="o">{}</span>
</span></span><span class="line"><span class="cl">etcd:
</span></span><span class="line"><span class="cl">  local:
</span></span><span class="line"><span class="cl">    dataDir: /var/lib/etcd
</span></span><span class="line"><span class="cl">imageRepository: registry.aliyuncs.com/google_containers
</span></span><span class="line"><span class="cl">kind: ClusterConfiguration
</span></span><span class="line"><span class="cl">kubernetesVersion: v1.23.6
</span></span><span class="line"><span class="cl">networking:
</span></span><span class="line"><span class="cl">  dnsDomain: cali-cluster.tclocal
</span></span><span class="line"><span class="cl">  serviceSubnet: 10.88.0.0/18
</span></span><span class="line"><span class="cl">scheduler: <span class="o">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">BinaryData</span>
</span></span><span class="line"><span class="cl"><span class="o">====</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Events:  &lt;none&gt;
</span></span></code></pre></td></tr></table>
</div>
</div><p>Of course, since we need to install a version higher than 1.22.0 and use systemd, we don&rsquo;t need to repeat the configuration.</p>
<h2 id="3-installing-the-kube-triplet">3. Installing the kube triplet</h2>
<blockquote>
<p>Reference: <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl</a></p>
</blockquote>
<p>The kube triplet is <code>kubeadm</code>, <code>kubelet</code> and <code>kubectl</code>, the specific functions and roles of the three are as follows.</p>
<ul>
<li><code>kubeadm</code>: the command used to initialize the cluster.</li>
<li><code>kubelet</code>: used on each node in the cluster to start Pods, containers, etc.</li>
<li><code>kubectl</code>: Command line tool used to communicate with the cluster.</li>
</ul>
<p>Some points to note are as follows.</p>
<ul>
<li><code>kubeadm</code> will not help us manage <code>kubelet</code> and <code>kubectl</code>, and the other two are the same, which means that the three are independent of each other and there is no case of who manages who.</li>
<li>The version of <code>kubelet</code> must be less than or equal to the version of <code>API-server</code>, otherwise compatibility issues are likely to arise.</li>
<li><code>kubectl</code> does not need to be installed on every node in the cluster, nor does it have to be installed on a node in the cluster, it can be installed separately on top of your own local machine environment, and then with the <code>kubeconfig</code> file you can use the <code>kubectl</code> command to remotely manage the corresponding k8s cluster.</li>
</ul>
<p>The installation of CentOS 7 is relatively simple, we can just use the official <code>yum</code> source. Note that you need to set the state of <code>selinux</code> here, but we have already turned off selinux, so we will skip this step.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Import the official Google yum sources directly</span>
</span></span><span class="line"><span class="cl">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span></span></span><span class="line"><span class="cl"><span class="s">[kubernetes]
</span></span></span><span class="line"><span class="cl"><span class="s">name=Kubernetes
</span></span></span><span class="line"><span class="cl"><span class="s">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span></span></span><span class="line"><span class="cl"><span class="s">enabled=1
</span></span></span><span class="line"><span class="cl"><span class="s">gpgcheck=1
</span></span></span><span class="line"><span class="cl"><span class="s">repo_gpgcheck=1
</span></span></span><span class="line"><span class="cl"><span class="s">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span></span></span><span class="line"><span class="cl"><span class="s">exclude=kubelet kubeadm kubectl
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Of course, if you can&#39;t connect to Google&#39;s source, you can consider using the domestic Ali mirror source</span>
</span></span><span class="line"><span class="cl">cat <span class="s">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span></span></span><span class="line"><span class="cl"><span class="s">[kubernetes]
</span></span></span><span class="line"><span class="cl"><span class="s">name=Kubernetes
</span></span></span><span class="line"><span class="cl"><span class="s">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
</span></span></span><span class="line"><span class="cl"><span class="s">enabled=1
</span></span></span><span class="line"><span class="cl"><span class="s">gpgcheck=1
</span></span></span><span class="line"><span class="cl"><span class="s">repo_gpgcheck=1
</span></span></span><span class="line"><span class="cl"><span class="s">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The next step is to install the kubectl triad directly</span>
</span></span><span class="line"><span class="cl">sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span class="o">=</span>kubernetes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># If the network environment is bad and the gpgcheck verification fails to read the yum source properly, consider disabling the repo_gpgcheck of the yum source</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s1">&#39;s/repo_gpgcheck=1/repo_gpgcheck=0/g&#39;</span> /etc/yum.repos.d/kubernetes.repo
</span></span><span class="line"><span class="cl"><span class="c1"># Or disable gpgcheck during installation</span>
</span></span><span class="line"><span class="cl">sudo yum install -y kubelet kubeadm kubectl --nogpgcheck --disableexcludes<span class="o">=</span>kubernetes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># If you want to install a specific version, you can use this command to view the relevant version information</span>
</span></span><span class="line"><span class="cl">sudo yum list --nogpgcheck kubelet kubeadm kubectl --showduplicates --disableexcludes<span class="o">=</span>kubernetes
</span></span><span class="line"><span class="cl"><span class="c1"># Here we use docker-shim in order to preserve the use of docker-shim, so we follow the previous version 1.23.6 from version 1.24.0</span>
</span></span><span class="line"><span class="cl">sudo yum install -y kubelet-1.23.6-0 kubeadm-1.23.6-0 kubectl-1.23.6-0 --nogpgcheck --disableexcludes<span class="o">=</span>kubernetes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Configure boot-up kubelet after installation</span>
</span></span><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> --now kubelet
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="4-initialize-the-cluster">4. Initialize the cluster</h2>
<h3 id="41-writing-the-configuration-file">4.1 Writing the configuration file</h3>
<p>After all the nodes in the cluster have performed the above three operations, we can start creating the k8s cluster. Since we are not involved in a high availability deployment this time, we can operate directly on top of our target master node during initialization.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># We&#39;ll first use the kubeadm command to check out the major image versions</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Since we previously specified an old 1.23.6 installation, the apiserver image version here will be rolled back as well</span>
</span></span><span class="line"><span class="cl">$ kubeadm config images list
</span></span><span class="line"><span class="cl">I0507 14:14:34.992275   <span class="m">20038</span> version.go:255<span class="o">]</span> remote version is much newer: v1.24.0<span class="p">;</span> falling back to: stable-1.23
</span></span><span class="line"><span class="cl">k8s.gcr.io/kube-apiserver:v1.23.6
</span></span><span class="line"><span class="cl">k8s.gcr.io/kube-controller-manager:v1.23.6
</span></span><span class="line"><span class="cl">k8s.gcr.io/kube-scheduler:v1.23.6
</span></span><span class="line"><span class="cl">k8s.gcr.io/kube-proxy:v1.23.6
</span></span><span class="line"><span class="cl">k8s.gcr.io/pause:3.6
</span></span><span class="line"><span class="cl">k8s.gcr.io/etcd:3.5.1-0
</span></span><span class="line"><span class="cl">k8s.gcr.io/coredns/coredns:v1.8.6
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># To facilitate editing and management, we still export the initialization parameters to a configuration file</span>
</span></span><span class="line"><span class="cl">$ kubeadm config print init-defaults &gt; kubeadm-flannel.conf
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>Considering that in most cases domestic(China) networks cannot use Google&rsquo;s k8s.gcr.io image source, we can directly modify the <code>imageRepository</code> parameter in the configuration file to be Ali&rsquo;s image source</li>
<li><code>kubernetesVersion</code> field to specify the version of k8s we want to install</li>
<li><code>localAPIEndpoint</code> parameter needs to be modified to the IP and port of our master node, which is the apiserver address of the k8s cluster after initialization</li>
<li><code>serviceSubnet</code> and <code>dnsDomain</code> parameters can be changed by default, here I changed them according to my needs</li>
<li>The <code>name</code> parameter in <code>nodeRegistration</code> is changed to <code>hostname</code> of the corresponding master node</li>
<li>The new configuration block uses ipvs, which can be found in the <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md#cluster-created-by-kubeadm">official documentation</a></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kubeadm.k8s.io/v1beta3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">bootstrapTokens</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">groups</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">system:bootstrappers:kubeadm:default-node-token</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">token</span><span class="p">:</span><span class="w"> </span><span class="l">abcdef.0123456789abcdef</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ttl</span><span class="p">:</span><span class="w"> </span><span class="l">24h0m0s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">usages</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">signing</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">authentication</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">InitConfiguration</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">localAPIEndpoint</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">advertiseAddress</span><span class="p">:</span><span class="w"> </span><span class="m">10.31.8.1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">bindPort</span><span class="p">:</span><span class="w"> </span><span class="m">6443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">nodeRegistration</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">criSocket</span><span class="p">:</span><span class="w"> </span><span class="l">/var/run/dockershim.sock</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tiny-flannel-master-8-1.k8s.tcinternal</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">taints</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiServer</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">timeoutForControlPlane</span><span class="p">:</span><span class="w"> </span><span class="l">4m0s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kubeadm.k8s.io/v1beta3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">certificatesDir</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/kubernetes/pki</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">clusterName</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">controllerManager</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">dns</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">etcd</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">local</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dataDir</span><span class="p">:</span><span class="w"> </span><span class="l">/var/lib/etcd</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">imageRepository</span><span class="p">:</span><span class="w"> </span><span class="l">registry.aliyuncs.com/google_containers</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterConfiguration</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kubernetesVersion</span><span class="p">:</span><span class="w"> </span><span class="m">1.23.6</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">networking</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">dnsDomain</span><span class="p">:</span><span class="w"> </span><span class="l">flan-cluster.tclocal</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">serviceSubnet</span><span class="p">:</span><span class="w"> </span><span class="m">10.8.0.0</span><span class="l">/18</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">podSubnet</span><span class="p">:</span><span class="w"> </span><span class="m">10.8.64.0</span><span class="l">/18</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">scheduler</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kubeproxy.config.k8s.io/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">KubeProxyConfiguration</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l">ipvs</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42-initialize-the-cluster">4.2 Initialize the cluster</h3>
<p>At this point we check the mirror version in the corresponding configuration file, we will find that it has become the version corresponding to the AliCloud mirror source.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Check the corresponding image version to make sure the configuration file is valid</span>
</span></span><span class="line"><span class="cl">$ kubeadm config images list --config kubeadm-flannel.conf
</span></span><span class="line"><span class="cl">registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.6
</span></span><span class="line"><span class="cl">registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.6
</span></span><span class="line"><span class="cl">registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.6
</span></span><span class="line"><span class="cl">registry.aliyuncs.com/google_containers/kube-proxy:v1.23.6
</span></span><span class="line"><span class="cl">registry.aliyuncs.com/google_containers/pause:3.6
</span></span><span class="line"><span class="cl">registry.aliyuncs.com/google_containers/etcd:3.5.1-0
</span></span><span class="line"><span class="cl">registry.aliyuncs.com/google_containers/coredns:v1.8.6
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># After confirming that there is no problem we pull the image directly</span>
</span></span><span class="line"><span class="cl">$ kubeadm config images pull --config kubeadm-flannel.conf
</span></span><span class="line"><span class="cl"><span class="o">[</span>config/images<span class="o">]</span> Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.6
</span></span><span class="line"><span class="cl"><span class="o">[</span>config/images<span class="o">]</span> Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.6
</span></span><span class="line"><span class="cl"><span class="o">[</span>config/images<span class="o">]</span> Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.6
</span></span><span class="line"><span class="cl"><span class="o">[</span>config/images<span class="o">]</span> Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.23.6
</span></span><span class="line"><span class="cl"><span class="o">[</span>config/images<span class="o">]</span> Pulled registry.aliyuncs.com/google_containers/pause:3.6
</span></span><span class="line"><span class="cl"><span class="o">[</span>config/images<span class="o">]</span> Pulled registry.aliyuncs.com/google_containers/etcd:3.5.1-0
</span></span><span class="line"><span class="cl"><span class="o">[</span>config/images<span class="o">]</span> Pulled registry.aliyuncs.com/google_containers/coredns:v1.8.6
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialization</span>
</span></span><span class="line"><span class="cl">$ kubeadm init --config kubeadm-flannel.conf
</span></span><span class="line"><span class="cl"><span class="o">[</span>init<span class="o">]</span> Using Kubernetes version: v1.23.6
</span></span><span class="line"><span class="cl"><span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks
</span></span><span class="line"><span class="cl"><span class="o">[</span>preflight<span class="o">]</span> Pulling images required <span class="k">for</span> setting up a Kubernetes cluster
</span></span><span class="line"><span class="cl"><span class="o">[</span>preflight<span class="o">]</span> This might take a minute or two, depending on the speed of your internet connection
</span></span><span class="line"><span class="cl"><span class="o">[</span>preflight<span class="o">]</span> You can also perform this action in beforehand using <span class="s1">&#39;kubeadm config images pull&#39;</span>
</span></span><span class="line"><span class="cl">... A bunch of output is omitted here...
</span></span></code></pre></td></tr></table>
</div>
</div><p>When we see this output below, our cluster has been initialized successfully.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">Your Kubernetes control-plane has initialized successfully!
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">To start using your cluster, you need to run the following as a regular user:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  mkdir -p <span class="nv">$HOME</span>/.kube
</span></span><span class="line"><span class="cl">  sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
</span></span><span class="line"><span class="cl">  sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Alternatively, <span class="k">if</span> you are the root user, you can run:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="nb">export</span> <span class="nv">KUBECONFIG</span><span class="o">=</span>/etc/kubernetes/admin.conf
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">You should now deploy a pod network to the cluster.
</span></span><span class="line"><span class="cl">Run <span class="s2">&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
</span></span><span class="line"><span class="cl">  https://kubernetes.io/docs/concepts/cluster-administration/addons/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Then you can join any number of worker nodes by running the following on each as root:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubeadm join 10.31.8.1:6443 --token abcdef.0123456789abcdef <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --discovery-token-ca-cert-hash sha256:d7160866920c0331731ad3c1c31a6e5b6c788b5682f86971cacaa940211db9ab
</span></span><span class="line"><span class="cl">        
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-configuring-kubeconfig">4.3 Configuring kubeconfig</h3>
<p>After successful initialization, we can&rsquo;t view the k8s cluster information right away. We need to configure kubeconfig related parameters in order to properly use kubectl to connect to apiserver to read cluster information.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># For non-root users, you can do this</span>
</span></span><span class="line"><span class="cl">mkdir -p <span class="nv">$HOME</span>/.kube
</span></span><span class="line"><span class="cl">sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
</span></span><span class="line"><span class="cl">sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># If you are root user, you can import environment variables directly</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KUBECONFIG</span><span class="o">=</span>/etc/kubernetes/admin.conf
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Add kubectl&#39;s auto-completion feature</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;source &lt;(kubectl completion bash)&#34;</span> &gt;&gt; ~/.bashrc
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>As we mentioned earlier, <code>kubectl</code> does not have to be installed in the cluster. In fact, you can install <code>kubectl</code> on any machine that can connect to the <code>apiserver</code> and configure <code>kubeconfig</code> according to the steps to use the <code>kubectl</code> command line to manage the corresponding k8s cluster.</p>
</blockquote>
<p>Once the configuration is complete, we can then execute the relevant commands to view the information about the cluster.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl cluster-info
</span></span><span class="line"><span class="cl">Kubernetes control plane is running at https://10.31.8.1:6443
</span></span><span class="line"><span class="cl">CoreDNS is running at https://10.31.8.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">To further debug and diagnose cluster problems, use <span class="s1">&#39;kubectl cluster-info dump&#39;</span>.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl get nodes -o wide
</span></span><span class="line"><span class="cl">NAME                                     STATUS     ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
</span></span><span class="line"><span class="cl">tiny-flannel-master-8-1.k8s.tcinternal   NotReady   control-plane,master   79s   v1.23.6   10.31.8.1     &lt;none&gt;        CentOS Linux <span class="m">7</span> <span class="o">(</span>Core<span class="o">)</span>   3.10.0-1160.62.1.el7.x86_64   docker://20.10.14
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl get pods -A -o wide
</span></span><span class="line"><span class="cl">NAMESPACE     NAME                                                             READY   STATUS    RESTARTS   AGE   IP          NODE                                     NOMINATED NODE   READINESS GATES
</span></span><span class="line"><span class="cl">kube-system   coredns-6d8c4cb4d-2clkj                                          0/1     Pending   <span class="m">0</span>          86s   &lt;none&gt;      &lt;none&gt;                                   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">kube-system   coredns-6d8c4cb4d-8mznz                                          0/1     Pending   <span class="m">0</span>          86s   &lt;none&gt;      &lt;none&gt;                                   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">kube-system   etcd-tiny-flannel-master-8-1.k8s.tcinternal                      1/1     Running   <span class="m">0</span>          91s   10.31.8.1   tiny-flannel-master-8-1.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">kube-system   kube-apiserver-tiny-flannel-master-8-1.k8s.tcinternal            1/1     Running   <span class="m">0</span>          92s   10.31.8.1   tiny-flannel-master-8-1.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">kube-system   kube-controller-manager-tiny-flannel-master-8-1.k8s.tcinternal   1/1     Running   <span class="m">0</span>          90s   10.31.8.1   tiny-flannel-master-8-1.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">kube-system   kube-proxy-dkvrn                                                 1/1     Running   <span class="m">0</span>          86s   10.31.8.1   tiny-flannel-master-8-1.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">kube-system   kube-scheduler-tiny-flannel-master-8-1.k8s.tcinternal            1/1     Running   <span class="m">0</span>          92s   10.31.8.1   tiny-flannel-master-8-1.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="44-adding-worker-nodes">4.4 Adding worker nodes</h3>
<p>At this point we need to go ahead and add the remaining two nodes as worker nodes to run the load. Run the command directly on top of the remaining node that was output during successful cluster initialization to successfully join the cluster.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubeadm join 10.31.8.1:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:d7160866920c0331731ad3c1c31a6e5b6c788b5682f86971cacaa940211db9ab
</span></span><span class="line"><span class="cl"><span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks
</span></span><span class="line"><span class="cl"><span class="o">[</span>preflight<span class="o">]</span> Reading configuration from the cluster...
</span></span><span class="line"><span class="cl"><span class="o">[</span>preflight<span class="o">]</span> FYI: You can look at this config file with <span class="s1">&#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>kubelet-start<span class="o">]</span> Writing kubelet configuration to file <span class="s2">&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>kubelet-start<span class="o">]</span> Writing kubelet environment file with flags to file <span class="s2">&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>kubelet-start<span class="o">]</span> Starting the kubelet
</span></span><span class="line"><span class="cl"><span class="o">[</span>kubelet-start<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to perform the TLS Bootstrap...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">This node has joined the cluster:
</span></span><span class="line"><span class="cl">* Certificate signing request was sent to apiserver and a response was received.
</span></span><span class="line"><span class="cl">* The Kubelet was informed of the new secure connection details.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Run <span class="s1">&#39;kubectl get nodes&#39;</span> on the control-plane to see this node join the cluster.
</span></span></code></pre></td></tr></table>
</div>
</div><p>It doesn&rsquo;t matter if we accidentally don&rsquo;t save the output of successful initialization, we can use the kubectl tool to view or generate the token.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># View the list of existing tokens</span>
</span></span><span class="line"><span class="cl">$ kubeadm token list
</span></span><span class="line"><span class="cl">TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
</span></span><span class="line"><span class="cl">abcdef.0123456789abcdef   23h         2022-05-08T06:27:34Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># If the token is no longer valid, then create a new token</span>
</span></span><span class="line"><span class="cl">$ kubeadm token create
</span></span><span class="line"><span class="cl">pyab3u.j1a9ld7vk03znbk8
</span></span><span class="line"><span class="cl">$ kubeadm token list
</span></span><span class="line"><span class="cl">TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
</span></span><span class="line"><span class="cl">abcdef.0123456789abcdef   23h         2022-05-08T06:27:34Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token
</span></span><span class="line"><span class="cl">pyab3u.j1a9ld7vk03znbk8   23h         2022-05-08T06:34:28Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># If you cannot find the --discovery-token-ca-cert-hash parameter, you can use the openssl tool on the master node to get it</span>
</span></span><span class="line"><span class="cl">$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt <span class="p">|</span> openssl rsa -pubin -outform der 2&gt;/dev/null <span class="p">|</span> openssl dgst -sha256 -hex <span class="p">|</span> sed <span class="s1">&#39;s/^.* //&#39;</span>
</span></span><span class="line"><span class="cl">d6cdc5a3bc40cbb0ae85776eb4fcdc1854942e2dd394470ae0f2f97714dd9fb9
</span></span></code></pre></td></tr></table>
</div>
</div><p>After adding the nodes, we can see that there are two more nodes in the cluster, but the state of the nodes is still <code>NotReady</code>, so we need to deploy CNI.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get nodes
</span></span><span class="line"><span class="cl">NAME                                      STATUS     ROLES                  AGE     VERSION
</span></span><span class="line"><span class="cl">tiny-flannel-master-8-1.k8s.tcinternal    NotReady   control-plane,master   7m49s   v1.23.6
</span></span><span class="line"><span class="cl">tiny-flannel-worker-8-11.k8s.tcinternal   NotReady   &lt;none&gt;                 2m58s   v1.23.6
</span></span><span class="line"><span class="cl">tiny-flannel-worker-8-12.k8s.tcinternal   NotReady   &lt;none&gt;                 102s    v1.23.6
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="5-install-cni">5. Install CNI</h2>
<h3 id="51-writing-the-manifest-file">5.1 Writing the manifest file</h3>
<p><a href="https://github.com/flannel-io/flannel#flannel">flannel</a> should be one of the many open source CNI plugins with the lowest entry barrier to CNI, simple to deploy, easy to understand the principles, and the related documentation is abundant on the web.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># We first download the official yaml template, and then modify the key fields one by one</span>
</span></span><span class="line"><span class="cl">$ wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
</span></span></code></pre></td></tr></table>
</div>
</div><p>For the <code>kube-flannel.yml</code> file, we need to modify some <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/configuration.md">parameters</a> to adapt to our cluster.</p>
<ul>
<li>
<p>the <code>net-conf.json</code> parameter, which configures the pod&rsquo;s network segment, here we use the previously planned <code>10.8.64.0/18</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="err">net-conf.json:</span> <span class="err">|</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;Network&#34;</span><span class="p">:</span> <span class="s2">&#34;10.8.64.0/18&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;Backend&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;Type&#34;</span><span class="p">:</span> <span class="s2">&#34;vxlan&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="52-deploying-flannel">5.2 Deploying flannel</h3>
<p>Once the modifications are done, we can deploy it directly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl apply -f kube-flannel.yml
</span></span><span class="line"><span class="cl">Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
</span></span><span class="line"><span class="cl">podsecuritypolicy.policy/psp.flannel.unprivileged created
</span></span><span class="line"><span class="cl">clusterrole.rbac.authorization.k8s.io/flannel created
</span></span><span class="line"><span class="cl">clusterrolebinding.rbac.authorization.k8s.io/flannel created
</span></span><span class="line"><span class="cl">serviceaccount/flannel created
</span></span><span class="line"><span class="cl">configmap/kube-flannel-cfg created
</span></span><span class="line"><span class="cl">daemonset.apps/kube-flannel-ds created
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check if the pod is running properly</span>
</span></span><span class="line"><span class="cl">$ kubectl get pods -A
</span></span><span class="line"><span class="cl">NAMESPACE     NAME                                                             READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">kube-system   coredns-6d8c4cb4d-np7q2                                          1/1     Running   <span class="m">0</span>          14m
</span></span><span class="line"><span class="cl">kube-system   coredns-6d8c4cb4d-z8f5b                                          1/1     Running   <span class="m">0</span>          14m
</span></span><span class="line"><span class="cl">kube-system   etcd-tiny-flannel-master-8-1.k8s.tcinternal                      1/1     Running   <span class="m">0</span>          14m
</span></span><span class="line"><span class="cl">kube-system   kube-apiserver-tiny-flannel-master-8-1.k8s.tcinternal            1/1     Running   <span class="m">0</span>          14m
</span></span><span class="line"><span class="cl">kube-system   kube-controller-manager-tiny-flannel-master-8-1.k8s.tcinternal   1/1     Running   <span class="m">0</span>          14m
</span></span><span class="line"><span class="cl">kube-system   kube-flannel-ds-9fq4z                                            1/1     Running   <span class="m">0</span>          12m
</span></span><span class="line"><span class="cl">kube-system   kube-flannel-ds-ckstx                                            1/1     Running   <span class="m">0</span>          7m18s
</span></span><span class="line"><span class="cl">kube-system   kube-flannel-ds-qj55x                                            1/1     Running   <span class="m">0</span>          8m25s
</span></span><span class="line"><span class="cl">kube-system   kube-proxy-bncfl                                                 1/1     Running   <span class="m">0</span>          14m
</span></span><span class="line"><span class="cl">kube-system   kube-proxy-lslcm                                                 1/1     Running   <span class="m">0</span>          7m18s
</span></span><span class="line"><span class="cl">kube-system   kube-proxy-pmwhf                                                 1/1     Running   <span class="m">0</span>          8m25s
</span></span><span class="line"><span class="cl">kube-system   kube-scheduler-tiny-flannel-master-8-1.k8s.tcinternal            1/1     Running   <span class="m">0</span>          14m
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check flannel&#39;s pod log for reported errors</span>
</span></span><span class="line"><span class="cl">$ kubectl logs -f -l <span class="nv">app</span><span class="o">=</span>flannel -n kube-system
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="6-deploy-test-cases">6. Deploy test cases</h2>
<p>After the cluster is deployed we deploy an nginx in the k8s cluster to test if it works. First we create a namespace named <code>nginx-quic</code>, then we create a <code>deployment</code> named <code>nginx-quic-deployment</code> in this namespace to deploy pods, and finally we create a <code>service</code> to expose the service, here we first Here we first use <code>nodeport</code> to expose the port for testing purposes.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ cat nginx-quic.yaml
</span></span><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">kind: Namespace
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: nginx-quic
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: apps/v1
</span></span><span class="line"><span class="cl">kind: Deployment
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: nginx-quic-deployment
</span></span><span class="line"><span class="cl">  namespace: nginx-quic
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">    matchLabels:
</span></span><span class="line"><span class="cl">      app: nginx-quic
</span></span><span class="line"><span class="cl">  replicas: <span class="m">2</span>
</span></span><span class="line"><span class="cl">  template:
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">      labels:
</span></span><span class="line"><span class="cl">        app: nginx-quic
</span></span><span class="line"><span class="cl">    spec:
</span></span><span class="line"><span class="cl">      containers:
</span></span><span class="line"><span class="cl">      - name: nginx-quic
</span></span><span class="line"><span class="cl">        image: tinychen777/nginx-quic:latest
</span></span><span class="line"><span class="cl">        imagePullPolicy: IfNotPresent
</span></span><span class="line"><span class="cl">        ports:
</span></span><span class="line"><span class="cl">        - containerPort: <span class="m">80</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">kind: Service
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: nginx-quic-service
</span></span><span class="line"><span class="cl">  namespace: nginx-quic
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">    app: nginx-quic
</span></span><span class="line"><span class="cl">  ports:
</span></span><span class="line"><span class="cl">  - protocol: TCP
</span></span><span class="line"><span class="cl">    port: <span class="m">8080</span> <span class="c1"># match for service access port</span>
</span></span><span class="line"><span class="cl">    targetPort: <span class="m">80</span> <span class="c1"># match for pod access port</span>
</span></span><span class="line"><span class="cl">    nodePort: <span class="m">30088</span> <span class="c1"># match for external access port</span>
</span></span><span class="line"><span class="cl">  type: NodePort
</span></span></code></pre></td></tr></table>
</div>
</div><p>We check the status directly after the deployment is complete.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># Direct Deployment</span>
</span></span><span class="line"><span class="cl">$ kubectl apply -f nginx-quic.yaml
</span></span><span class="line"><span class="cl">namespace/nginx-quic created
</span></span><span class="line"><span class="cl">deployment.apps/nginx-quic-deployment created
</span></span><span class="line"><span class="cl">service/nginx-quic-service created
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check the running status of the deployment</span>
</span></span><span class="line"><span class="cl">$ kubectl get deployment -o wide -n nginx-quic
</span></span><span class="line"><span class="cl">NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                          SELECTOR
</span></span><span class="line"><span class="cl">nginx-quic-deployment   2/2     <span class="m">2</span>            <span class="m">2</span>           48s   nginx-quic   tinychen777/nginx-quic:latest   <span class="nv">app</span><span class="o">=</span>nginx-quic
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check the running status of the service</span>
</span></span><span class="line"><span class="cl">$ kubectl get service -o wide -n nginx-quic
</span></span><span class="line"><span class="cl">NAME                 TYPE       CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE   SELECTOR
</span></span><span class="line"><span class="cl">nginx-quic-service   NodePort   10.8.4.218   &lt;none&gt;        8080:30088/TCP   62s   <span class="nv">app</span><span class="o">=</span>nginx-quic
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check the running status of the pod</span>
</span></span><span class="line"><span class="cl">$ kubectl get pods -o wide -n nginx-quic
</span></span><span class="line"><span class="cl">NAME                                     READY   STATUS    RESTARTS   AGE   IP          NODE                                      NOMINATED NODE   READINESS GATES
</span></span><span class="line"><span class="cl">nginx-quic-deployment-696d959797-jm8w5   1/1     Running   <span class="m">0</span>          73s   10.8.66.2   tiny-flannel-worker-8-12.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">nginx-quic-deployment-696d959797-lwcqz   1/1     Running   <span class="m">0</span>          73s   10.8.65.2   tiny-flannel-worker-8-11.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># View IPVS rules</span>
</span></span><span class="line"><span class="cl">$ ipvsadm -ln
</span></span><span class="line"><span class="cl">IP Virtual Server version 1.2.1 <span class="o">(</span><span class="nv">size</span><span class="o">=</span>4096<span class="o">)</span>
</span></span><span class="line"><span class="cl">Prot LocalAddress:Port Scheduler Flags
</span></span><span class="line"><span class="cl">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
</span></span><span class="line"><span class="cl">TCP  172.17.0.1:30088 rr
</span></span><span class="line"><span class="cl">  -&gt; 10.8.65.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">  -&gt; 10.8.66.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">TCP  10.8.4.218:8080 rr
</span></span><span class="line"><span class="cl">  -&gt; 10.8.65.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">  -&gt; 10.8.66.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">TCP  10.8.64.0:30088 rr
</span></span><span class="line"><span class="cl">  -&gt; 10.8.65.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">  -&gt; 10.8.66.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">TCP  10.8.64.1:30088 rr
</span></span><span class="line"><span class="cl">  -&gt; 10.8.65.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">  -&gt; 10.8.66.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">TCP  10.31.8.1:30088 rr
</span></span><span class="line"><span class="cl">  -&gt; 10.8.65.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span><span class="line"><span class="cl">  -&gt; 10.8.66.2:80                 Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Finally we test that this image of nginx-quic returns by default the IP and port of the user request obtained in the nginx container.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># First we test within the cluster</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Access the pod directly, the IP shown is the IP of the flannel.1 NIC on the master node</span>
</span></span><span class="line"><span class="cl">$ curl 10.8.66.2:80
</span></span><span class="line"><span class="cl">10.8.64.0:38958
</span></span><span class="line"><span class="cl">$ curl 10.8.65.2:80
</span></span><span class="line"><span class="cl">10.8.64.0:46484
</span></span><span class="line"><span class="cl"><span class="c1"># Direct access to the service&#39;s ClusterIP, when the request will be forwarded to the pod</span>
</span></span><span class="line"><span class="cl">$ curl 10.8.4.218:8080
</span></span><span class="line"><span class="cl">10.8.64.0:26305
</span></span><span class="line"><span class="cl"><span class="c1"># Direct access to nodeport, where the request is forwarded to the pod and does not go through the ClusterIP</span>
</span></span><span class="line"><span class="cl">$ curl 10.31.8.1:30088
</span></span><span class="line"><span class="cl">10.8.64.0:6519
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Then we test outside the cluster</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Direct access to the nodeport of the three nodes, where the request will be forwarded to the pod and will not go through the ClusterIP</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Since the externalTrafficPolicy defaults to Cluster, the IP that nginx gets is the IP of the flannel.1 NIC of the node we are accessing</span>
</span></span><span class="line"><span class="cl">$ curl 10.31.8.1:30088
</span></span><span class="line"><span class="cl">10.8.64.0:50688
</span></span><span class="line"><span class="cl">$ curl 10.31.8.11:30088
</span></span><span class="line"><span class="cl">10.8.65.1:41032
</span></span><span class="line"><span class="cl">$ curl 10.31.8.12:30088
</span></span><span class="line"><span class="cl">10.8.66.0:11422
</span></span></code></pre></td></tr></table>
</div>
</div>
    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/k8s/">k8s</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2022-07/k8s-03-deploy-k8s-with-calico/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Kubeadm Deployment k8s Cluster &#43; calico</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2022-07/k8s-01-what-is-kubernetes/">
            <span class="next-text nav-default">What is kubernetes?</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>








</body>
</html>
