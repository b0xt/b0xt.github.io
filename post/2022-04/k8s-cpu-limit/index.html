<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Will adding K8S CPU limit reduce service performance? - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="Explore whether adding the K8S CPU limit will degrade service performance." /><meta name="keywords" content="Kubernetes,  CPU limit" />






<meta name="generator" content="Hugo 0.96.0 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2022-04/k8s-cpu-limit/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Will adding K8S CPU limit reduce service performance?" />
<meta property="og:description" content="Explore whether adding the K8S CPU limit will degrade service performance." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2022-04/k8s-cpu-limit/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-04-12T21:55:08+08:00" />
<meta property="article:modified_time" content="2022-04-12T21:55:08+08:00" />

<meta itemprop="name" content="Will adding K8S CPU limit reduce service performance?">
<meta itemprop="description" content="Explore whether adding the K8S CPU limit will degrade service performance."><meta itemprop="datePublished" content="2022-04-12T21:55:08+08:00" />
<meta itemprop="dateModified" content="2022-04-12T21:55:08+08:00" />
<meta itemprop="wordCount" content="1764">
<meta itemprop="keywords" content="Kubernetes," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Will adding K8S CPU limit reduce service performance?"/>
<meta name="twitter:description" content="Explore whether adding the K8S CPU limit will degrade service performance."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SOBYTE</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/ukraine/">
        <li class="mobile-menu-item">UKRAINE</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SOBYTE</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/ukraine/">UKRAINE</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Will adding K8S CPU limit reduce service performance?</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-04-12 21:55:08 </span>
        <div class="post-category">
            <a href="/categories/tutorials/"> tutorials </a>
            </div>
          <span class="more-meta"> 1764 words </span>
          <span class="more-meta"> 4 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-why-is-this">1. Why is this?</a></li>
        <li><a href="#2-as-a-simple-example">2. As a simple example</a></li>
        <li><a href="#3-is-there-an-unnecessary-limit">3. Is there an unnecessary limit</a></li>
        <li><a href="#4-what-causes-this">4. What causes this?</a></li>
        <li><a href="#5-how-the-linux-kernel-solves-this-problem">5. How the linux kernel solves this problem</a></li>
        <li><a href="#6-summary">6. Summary</a>
          <ul>
            <li><a href="#unrestricted-personally-i-dont-think-this-is-a-good-idea">unrestricted (personally, I don&rsquo;t think this is a good idea)</a></li>
            <li><a href="#add-resources">add resources</a></li>
            <li><a href="#auto-scaling">Auto Scaling</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>As you know, Kubernetes QOS is divided into three levels</p>
<ul>
<li>Guaranteed: Each container in a Pod must have a memory/CPU limit and a request, and the values must be equal. If a container specifies only a limit but not a request, the value of the request is equal to the limit value.</li>
<li>Burstable: At least one container in the Pod has a memory or CPU request and does not satisfy the Guarantee level, i.e., the memory/CPU values are set differently.</li>
<li>BestEffort: The container must not have any memory or CPU limits or requests.</li>
</ul>
<p>Google&rsquo;s best practices tell you to always configure Guaranteed for important services so that you can keep your important services from being evicted when resources are low.</p>
<p>Best practices require you to configure it this way from an operations and post-operations perspective. Teams are just starting out and will basically work fine without setting resource requests and limits, but as your team and project grows, you will start to run into stability issues. Services interact with each other and it may be time to add limits to services and save yourself from many headaches.</p>
<p>It should already be clear here that by following best practices, the entire cluster can be made more flexible and reliable.</p>
<p>But things get interesting when it comes to CPU limits, which are compressible resources. If your application starts to reach its CPU limit, Kubernetes will start limiting your containers. This means that CPUs will be artificially limited, making your application performance potentially worse!</p>
<h2 id="1-why-is-this">1. Why is this?</h2>
<p>Because when you set hard CPU limits in the container scheduler, the kernel uses the Complete Fairness Scheduler (CFS) Cgroup to enforce those limits. the CFS Cgroup mechanism uses two settings to manage CPU allocations: quota and period. When an application uses more than its allocated CPU quota for a given time period, it is limited until the next time period.</p>
<p>All CPU metrics for cgroup are located in <code>/sys/fs/cgroup/cpu,cpuacct/&lt;container&gt;</code> . The quota and period settings are located in <code>cpu.cfs_quota_us</code> and in <code>cpu.cfs_period_us</code>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/04/12/b53b91c7ff2f4bed9a375f6d455215e5.png" alt="cpu.cfs_period_us"></p>
<p>You can also look at the limit indicator cpu.stat. Inside cpu.stat you will find.</p>
<ul>
<li>nr_periods - the number of cycles that any thread of the cgroup can run</li>
<li>nr_throttled - the number of runnable cycles for which the application uses its full quota and is limited</li>
<li>throttled_time - controls the total time of each thread in the cgroup</li>
</ul>
<h2 id="2-as-a-simple-example">2. As a simple example</h2>
<p>A single-threaded application runs on a CPU with a cgroup constraint. This application requires 200 milliseconds of processing time to complete a request. Unconstrained, its response looks like the following.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/04/12/5f2e615ac0e4404ab8ee09525f7e4cd5.png" alt="A single-threaded application"></p>
<p>Now, suppose we assign a CPU limit of 0.4 CPU to the application. This means that the application gets 40 milliseconds of runtime for every 100 millisecond cycle - even if the CPU has no other work to do during that time. 200 millisecond requests now take 440 milliseconds to complete.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/04/12/5598a0f2612a49ec9292dd8f0fe1d422.png" alt="Configuring restricted requests"></p>
<p>At this point you look at the cpu.stat throttled_time under the path of the container you are in and you will see that it is limited to 240ms (for every 100ms cycle the application can only run for 40ms and is limited to 60ms. It has been limited to 4 cycles, so 4 * 60 = 240 ms.)</p>
<p>To put it in layman&rsquo;s terms, when you ask for 1 CPU, this means that your application can use 1 CPU core per second. If it is a single thread, it will be able to use one core all the time. However, if it has 2 threads, it can use 2 cores per second for an unlimited amount of seconds. So with this limit, it can fully use 2 cores for 1/2 second, and then it will be limited. (Although this isn&rsquo;t really measured in seconds, it&rsquo;s actually us, but I find it easier to understand that way).</p>
<p>Seeing this, you might say that this is just a constraint, and that resources that are out of range just can&rsquo;t be used or they will be limited.</p>
<h2 id="3-is-there-an-unnecessary-limit">3. Is there an unnecessary limit</h2>
<p>It&rsquo;s not that simple. Many people have given feedback about unnecessary cgroup restrictions, even without CPU caps, and there are heated discussions here about</p>
<ol>
<li><a href="https://github.com/kubernetes/kubernetes/issues/67577">https://github.com/kubernetes/kubernetes/issues/67577</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/51135">https://github.com/kubernetes/kubernetes/issues/51135</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/51135">https://github.com/kubernetes/kubernetes/issues/70585</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/pull/75682">https://github.com/kubernetes/kubernetes/pull/75682</a></li>
</ol>
<p>The key metric to check when running containers is throttling. This indicates how many times your container is limited. We have found that many containers are limited regardless of whether the CPU usage is close to the limit or not. The following is an example of feedback from an enthusiastic user.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/04/12/df6cbd432ed54d80a1774a78ea413de4.png" alt="throttling"></p>
<p>In the animation you can see that the CPU limit is set to 800m (0.8 cores, 80% of the cores) and the peak usage is up to 200m (20% of the cores). After seeing that, we might think we have enough CPU to get the service running before it throttles down, right? Now look at this.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/04/12/2446832545cb438e87088f80c4fa7a08.gif" alt="animation"></p>
<p>You will notice that a CPU limit has occurred even if the CPU usage is below the CPU limit. The maximum CPU usage is not even close to the CPU limit.</p>
<p>Limiting means service performance degradation and higher latency.</p>
<h2 id="4-what-causes-this">4. What causes this?</h2>
<p>Essentially, the problem is caused by the linux kernel, see this video for details: <a href="https://www.youtube.com/watch?v=UE7QX98-kO0">https://www.youtube.com/watch?v=UE7QX98-kO0</a></p>
<p>This video probably means something like this.</p>
<p>Here is an example of a multi-threaded daemon with two worker threads, each fixed to its own core. As you can see below, the first graph shows the global quota of the cgroup over a period of time. This starts with a quota of 20ms, which is associated with 0.2 CPUs. The middle graph shows the quota allocated to each CPU queue, and the bottom graph shows how long the actual worker threads run on their CPUs.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/04/12/d947fd28002a47b595d400efc5f3419c.png" alt="multi-threaded daemon"></p>
<ul>
<li>
<p>At 30 milliseconds</p>
<ul>
<li>Worker 1 has received a request.</li>
<li>Worker 1 only needs 1 millisecond to process the request, and CPU 1 has 4 milliseconds left on each CPU storage bucket.</li>
<li>Since there is time left on each CPU run queue, but no more runnable threads on CPU 1, a timer is set to return the slack quota to the global storage bucket. This timer is set to 7ms after worker 1 stops running.</li>
</ul>
</li>
<li>
<p>at 38 ms</p>
<ul>
<li>The slack timer set on CPU 1 triggers and returns all but 1 ms of quota to the global quota pool.</li>
<li>This leaves 1 ms of quota on CPU 1.</li>
</ul>
</li>
<li>
<p>At 41 ms</p>
<ul>
<li>Worker 2 receives a long request.</li>
<li>All remaining time is transferred from the global storage bucket to CPU 2&rsquo;s per-CPU storage bucket, and Worker 2 uses all the time.</li>
</ul>
</li>
<li>
<p>At 49 milliseconds</p>
<ul>
<li>Worker 2 on CPU 2 is now limited in the event that the request is not completed.</li>
<li>This happens even though CPU 1 still has a 1ms quota.</li>
</ul>
</li>
</ul>
<p>While 1 millisecond may not have much impact on a dual-core machine, these milliseconds add up on a high core count machine. If we encounter this behavior on an 88-core (n) machine, we could be spending 87 (n-1) milliseconds per cycle. That&rsquo;s 87 milliseconds or 0.87 CPUs (per Container) that may not be available. This is how we reach low quota usage by over-throttling. In the best case, if the fix increases the available CPU per instance of the affected application by 0.87, or the required CPU quota is reduced accordingly. These benefits will increase application density and reduce application response time in our cluster.</p>
<p>This issue went largely unnoticed when it came to 8-core and 10-core machines. Now that core counts are all the rage, the problem is becoming more pronounced. This is why we have noticed an increase in limitations when running the same application on a higher core count machine.</p>
<p>To summarize, the clock deviation limit is a problem, which leads to a strict quota limit for each period. This problem has always existed, and it has worked this way at least since the 512ac999 commit and kernel v4.18.</p>
<h2 id="5-how-the-linux-kernel-solves-this-problem">5. How the linux kernel solves this problem</h2>
<p>Pre-patched code expires at runtime if and only if the per CPU expiration time matches the global expiration time <code>cfs_rq-&gt;runtime_expires ! = cfs_b-&gt;runtime_expires</code> . By testing the kernel, I proved that this almost never happens on my node. Therefore, that 1 millisecond never expires. The patch changed this logic from being based on clock time to cycle sequence counting, fixing a long-standing bug in the kernel. The code is as follows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-diff" data-lang="diff"><span class="line"><span class="cl"><span class="gd">- if (cfs_rq-&gt;runtime_expires != cfs_b-&gt;runtime_expires) { 
</span></span></span><span class="line"><span class="cl"><span class="gd"></span><span class="gi">+ if (cfs_rq-&gt;expires_seq == cfs_b-&gt;expires_seq) { 
</span></span></span><span class="line"><span class="cl"><span class="gi"></span>               /* 延长本地期限，漂移以 2 个滴答为界 */ 
</span></span><span class="line"><span class="cl">                cfs_rq-&gt;runtime_expires + = TICK_NSEC; 
</span></span><span class="line"><span class="cl">       } else { 
</span></span><span class="line"><span class="cl">                /* 全局截止日期提前，过期已过 */ 
</span></span><span class="line"><span class="cl">                cfs_rq-&gt;runtime_remaining = 0; 
</span></span><span class="line"><span class="cl">        }
</span></span></code></pre></td></tr></table>
</div>
</div><p>Modification issues are part of the 5.4+ mainline kernel. They have been backported to many of the available kernels.</p>
<ul>
<li>Linux-stable: 4.14.154+, 4.19.84+, 5.3.9+</li>
<li>Ubuntu: 4.15.0-67+, 5.3.0-24+</li>
<li>Redhat Enterprise Linux:
<ol>
<li>RHEL 7: 3.10.0-1062.8.1.el7+</li>
<li>RHEL 8: 4.18.0-147.2.1.el8_1+</li>
</ol>
</li>
<li>CoreOS: v4.19.84+</li>
</ul>
<p>The bug <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=763a9ec06c4">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=763a9ec06c4</a> has been fixed and merged into the kernel of Linux distributions running 4.19 or later.</p>
<p>However, in reading the kubernetes issue <a href="https://github.com/kubernetes/kubernetes/issues/67577">https://github.com/kubernetes/kubernetes/issues/67577</a>, we can see that various Linux projects have been referencing this issue, so I guess some Linux distributions still have this bug and are working on integrating a fix. and are working on integrating a fix.</p>
<p>If your Linux distribution has a kernel version lower than 4.19, I suggest you upgrade to the latest Linux distribution for your node, but in any case, you should try to remove the CPU limit and see if there are any limitations.</p>
<h2 id="6-summary">6. Summary</h2>
<p>Monitor your containers for poor performance due to throttle, if it does happen, it is best to solve it by upgrading the kernel version in batches, if that is not possible, you can do the following.</p>
<h3 id="unrestricted-personally-i-dont-think-this-is-a-good-idea">unrestricted (personally, I don&rsquo;t think this is a good idea)</h3>
<ul>
<li>Pods with performance requirements are scheduled to nodes with specific taint.</li>
<li>Remove CPU limits for these Pods, consider adding limits at the namespace level.</li>
</ul>
<h3 id="add-resources">add resources</h3>
<p>Also CPU throttle throttling is mainly due to lower CPU limits. Its limits affect the behavior of Cgroups. Therefore, a quick solution to this problem is to increase the limit by 10-25% based on monitoring to ensure that spikes are reduced or avoided altogether.</p>
<h3 id="auto-scaling">Auto Scaling</h3>
<p>Because setting CPU requests and limits to the same value usually gives people the behavior they expect, a simple solution to this problem is to set CPU requests and limits to the same value and add an HPA that allows Pods to automatically scale up and down based on load.</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/kubernetes/">Kubernetes</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2022-04/python-package/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">How does Python find packages?</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2022-04/web-service-process/">
            <span class="next-text nav-default">Process Management for Web Services</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.64437849d125a2d603b3e71d6de5225d641a32d17168a58106e0b61852079683.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
