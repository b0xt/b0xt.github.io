<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>OpenEBS Usage Guide - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="The most complete guide to using OpenEBS for cloud-native storage on the web." /><meta name="keywords" content="OpenEBS" />






<meta name="generator" content="Hugo 0.102.0 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2022-05/open-ebs/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">


<meta property="og:title" content="OpenEBS Usage Guide" />
<meta property="og:description" content="The most complete guide to using OpenEBS for cloud-native storage on the web." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2022-05/open-ebs/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-05-24T13:13:44+08:00" />
<meta property="article:modified_time" content="2022-05-24T13:13:44+08:00" />

<meta itemprop="name" content="OpenEBS Usage Guide">
<meta itemprop="description" content="The most complete guide to using OpenEBS for cloud-native storage on the web."><meta itemprop="datePublished" content="2022-05-24T13:13:44+08:00" />
<meta itemprop="dateModified" content="2022-05-24T13:13:44+08:00" />
<meta itemprop="wordCount" content="7734">
<meta itemprop="keywords" content="OpenEBS," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="OpenEBS Usage Guide"/>
<meta name="twitter:description" content="The most complete guide to using OpenEBS for cloud-native storage on the web."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SOBYTE</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SOBYTE</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">OpenEBS Usage Guide</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-05-24 13:13:44 </span>
        <div class="post-category">
            <a href="/categories/tutorials/"> tutorials </a>
            </div>
          <span class="more-meta"> 7734 words </span>
          <span class="more-meta"> 16 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#what-is-openebs">What is OpenEBS?</a>
          <ul>
            <li><a href="#what-can-openebs-do">What can OpenEBS do?</a></li>
            <li><a href="#compare-to-traditional-distributed-storage">Compare to traditional distributed storage</a></li>
            <li><a href="#openebs-storage-engine-recommendations">OpenEBS Storage Engine Recommendations</a></li>
            <li><a href="#openebs-features">OpenEBS Features</a></li>
            <li><a href="#introduction-to-cas">Introduction to CAS</a></li>
          </ul>
        </li>
        <li><a href="#introduction-to-the-openesb-architecture">Introduction to the OpenESB architecture</a>
          <ul>
            <li><a href="#control-plane">Control plane</a></li>
            <li><a href="#data-plane">Data plane</a></li>
          </ul>
        </li>
        <li><a href="#cas-engine">CAS Engine</a>
          <ul>
            <li><a href="#storage-engine-overview">Storage Engine Overview</a></li>
            <li><a href="#storage-engine-types">Storage engine types</a></li>
            <li><a href="#storage-engine-declaration">Storage engine declaration</a></li>
            <li><a href="#cas-engine-usage-scenarios">CAS Engine Usage Scenarios</a></li>
            <li><a href="#node-disk-manager-ndm">Node Disk Manager (NDM)</a></li>
          </ul>
        </li>
        <li><a href="#practice">Practice</a>
          <ul>
            <li><a href="#local-pv-hostpath-practices">Local PV Hostpath Practices</a></li>
            <li><a href="#local-pv-device-practices">Local PV Device Practices</a></li>
            <li><a href="#summary-1">Summary</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="what-is-openebs">What is OpenEBS?</h2>
<p><a href="https://openebs.io/">OpenEBS</a> is an open source cloud native storage solution hosted at the <code>CNCF</code> Foundation, which is currently in the sandbox phase of the project.</p>
<p>OpenEBS is a set of storage engines that allow you to choose the right storage solution for stateful workloads (StatefulSet) and Kubernetes platform types. At a high level, OpenEBS supports two main types of volumes - local and replicated volumes.</p>
<p>OpenEBS is a Kubernetes native hyperconverged storage solution that manages the local storage available to nodes and provides locally or highly available distributed persistent volumes for stateful workloads. Another advantage of being a fully Kubernetes-native solution is that administrators and developers can interact and manage OpenEBS using all the great tools available for Kubernetes such as kubectl, Helm, Prometheus, Grafana, Weave Scope, and more.</p>
<h3 id="what-can-openebs-do">What can OpenEBS do?</h3>
<p><code>OpenEBS</code> manages storage on <code>k8s</code> nodes and provides local storage volumes or distributed storage volumes for <code>k8s</code> stateful loads (<code>StatefulSet</code>).</p>
<ul>
<li>
<p>Local volumes (Local Storage)</p>
<ul>
<li>OpenEBS can use host bare block devices or partitions, or use subdirectories on Hostpaths, or use <code>LVM</code>, <code>ZFS</code> to create persistent volumes</li>
<li>Local volumes are mounted directly to the Stateful Pod without any additional overhead in the datapath from OpenEBS</li>
<li>OpenEBS provides additional tools for local volumes for monitoring, backup/recovery, disaster recovery, snapshots supported by <code>ZFS</code> or <code>LVM</code>, etc.</li>
</ul>
</li>
<li>
<p>For distributed volumes (i.e., replicated volumes)</p>
<ul>
<li>OpenEBS uses one of the engines ( <code>Mayastor</code>, <code>cStor</code> or <code>Jiva</code>) to create microservices for each distributed persistent volume.</li>
<li>Stateful Pods write data to the OpenEBS engine, which replicates the data synchronously to multiple nodes in the cluster. the OpenEBS engine itself is deployed as a Pod and orchestrated by Kubernetes. When the node running the Stateful Pod fails, the Pod is rescheduled to another node in the cluster and OpenEBS provides access to the data using a copy of the data available on the other node.</li>
<li>Stateful Pods use <code>iSCSI</code> ( <code>cStor</code> and <code>Jiva</code> ) or <code>NVMeoF</code> ( <code>Mayastor</code> ) to connect to OpenEBS distributed persistent volumes.</li>
<li><code>OpenEBS cStor</code> and <code>Jiva</code> focus on ease of use and persistence of storage. They use custom versions of <code>ZFS</code> and <code>Longhorn</code> technologies to write data to the storage, respectively. <code>OpenEBS Mayastor</code> is the latest development of an engine designed for endurance and performance, efficiently managing compute (large pages, cores) and storage (<code>NVMe Drives</code>) to provide fast distributed block storage.</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Note</strong> : <code>OpenEBS</code> distributed block volumes are called replicated volumes to avoid confusion with traditional distributed block storage, which tends to distribute data across many nodes in a cluster. Replicated volumes are designed for cloud-native stateful workloads that require a large number of volumes whose capacity can typically be provisioned from a single node, rather than using a single large volume that is sharded across multiple nodes in a cluster.</p>
</blockquote>
<h3 id="compare-to-traditional-distributed-storage">Compare to traditional distributed storage</h3>
<p>A few key ways in which <code>OpenEBS</code> differs from other traditional storage solutions:</p>
<ul>
<li>Built using a microservice architecture, just like the applications it serves. <code>OpenEBS</code> itself is deployed as a set of containers on a <code>Kubernetes</code> worker node. Use <code>Kubernetes</code> itself to orchestrate and manage <code>OpenEBS</code> components.</li>
<li>Built entirely in user space, making it highly portable to run on any OS/platform..</li>
<li>Fully intent-driven, inheriting the same principles of <code>Kubernetes</code> ease-of-use.</li>
<li><code>OpenEBS</code> supports a range of storage engines, so developers can deploy storage technologies that fit their application design goals. Distributed applications like <code>Cassandra</code> can use the <code>LocalPV</code> engine for the lowest latency write operations. Monolithic applications like <code>MySQL</code> and <code>PostgreSQL</code> can use <code>Mayastor</code> built with <code>NVMe</code> and <code>SPDK</code> or <code>cStor</code> based on <code>ZFS</code> to achieve resiliency. Streaming applications like <code>Kafka</code> can use the <code>NVMe</code> engine <code>Mayastor</code> for optimal performance in an edge environment.</li>
</ul>
<p>The main reasons that drive users to use <code>OpenEBS</code> are:</p>
<ul>
<li>Portability on all <code>Kubernetes</code> distributions.</li>
<li>Increased developer and platform <code>SRE</code> productivity.</li>
<li>Ease of use compared to other solutions.</li>
<li>Excellent community support.</li>
<li>Free and open source.</li>
</ul>
<h4 id="local-volume-type">Local Volume Type</h4>
<p>Local volumes can only be accessed from a single node in the cluster. <code>Pods</code> that use <code>Local Volume</code> must be scheduled on the node providing the volume. Local volumes are often preferred for distributed workloads such as <code>Cassandra</code>, <code>MongoDB</code>, <code>Elastic</code>, etc. that are inherently distributed and have high availability (sharding) built in.</p>
<p>Depending on the type of storage attached to the <code>Kubernetes</code> worker node, you can choose from different dynamic local <code>PV</code>s - <code>Hostpath</code>, <code>Device</code>, <code>LVM</code>, <code>ZFS</code> or <code>Rawfile</code>.</p>
<h4 id="replicable-volume-type">Replicable volume type</h4>
<p>A <code>replicated volume</code> is, as the name implies, a volume that replicates data to multiple nodes synchronously. Volumes can support node failures. Replication can also be set up across availability zones to help applications move across availability zones.</p>
<p>Replicated volumes can also provide enterprise storage features like snapshots, clones, volume expansion, etc. Replicated volumes are preferred for stateful workloads such as <code>Percona/MySQL</code>, <code>Jira</code>, <code>GitLab</code>, etc.</p>
<p>Depending on the type of storage attached to the <code>Kubernetes</code> worker node and application performance requirements, you can choose from <code>Jiva, cStor</code> or <code>Mayastor</code>.</p>
<h3 id="openebs-storage-engine-recommendations">OpenEBS Storage Engine Recommendations</h3>
<table>
<thead>
<tr>
<th>Application Requirements</th>
<th>Storage Types</th>
<th>OpenEBS Volume Types</th>
</tr>
</thead>
<tbody>
<tr>
<td>Low Latency, High Availability, Synchronous Replication, Snapshots, Clones, Thin Provisioning</td>
<td>SSD/ Cloud Storage Volumes</td>
<td>OpenEBS Mayastor</td>
</tr>
<tr>
<td>High Availability, Synchronous Replication, Snapshots, Clones, Thin Provisioning</td>
<td>Machine /SSD/ Cloud Storage Volumes</td>
<td>OpenEBS cStor</td>
</tr>
<tr>
<td>High Availability, Synchronous Replication, Thin Provisioning</td>
<td>Host Path or External Mounted Storage</td>
<td>OpenEBS Jiva</td>
</tr>
<tr>
<td>Low Latency, Local PV</td>
<td>Hostpath or External Mounted Storage</td>
<td>Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile</td>
</tr>
<tr>
<td>Low Latency, Local PV</td>
<td>Local Mechanical /SSD/ Block Devices such as Cloud Storage Volumes</td>
<td>Dynamic Local PV - Device</td>
</tr>
<tr>
<td>Low Latency, Local PV, Snapshot, Clone</td>
<td>Local Mechanical /SSD/ Cloud Storage Volumes and other Block Devices</td>
<td>OpenEBS Dynamic Local PV - ZFS , OpenEBS Dynamic Local PV - LVM</td>
</tr>
</tbody>
</table>
<p>To summarize.</p>
<ul>
<li>Multi-computer environment, if there is an additional block device (non-system disk block device) as data disk, choose <code>OpenEBS Mayastor</code>, <code>OpenEBS cStor</code>.</li>
<li>Multi-computer environment, if there is no additional block device (non-system disk block device) as data disk, only a single system disk block device, use <code>OpenEBS Jiva</code>.</li>
<li>For standalone environment, <code>Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile</code> is recommended for local paths, as standalone is mostly used for test environment with lower data reliability requirements.</li>
</ul>
<p>It seems that <code>OpenEBS</code> is commonly used in the above three scenarios.</p>
<h3 id="openebs-features">OpenEBS Features</h3>
<h4 id="container-attached-storage">container-attached storage</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/4d7d5bd48e9344898976162595e93726.png" alt="container-attached storage"></p>
<p><code>OpenEBS</code> is an example of Container Attached Storage ( <code>Container Attached Storage, CAS</code>). The volumes provided through <code>OpenEBS</code> are always containerized. Each volume has a dedicated storage controller that is used to improve the agility and granularity of persistent storage operations for stateful applications.</p>
<h4 id="synchronous-replication">Synchronous Replication</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/bcdc452908204a7e9ec6173fe4d63b98.png" alt="Synchronous Replication"></p>
<p>Synchronous replication is an optional and popular feature of <code>OpenEBS</code>. When used with the <code>Jiva</code>, <code>cStor</code> and <code>Mayastor</code> storage engines, <code>OpenEBS</code> can replicate data volumes synchronously for high availability. Replication is performed across <code>Kubernetes</code> regions to provide high availability across <code>AZ</code> settings. This feature is particularly useful for building highly available stateful applications using local disks on cloud provider services such as <code>GKE, EKS</code> and <code>AKS</code>.</p>
<h4 id="snapshots-and-clones">Snapshots and Clones</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/19b9380033be40b990607cc9eb827833.png" alt="Snapshots and Clones"></p>
<p>Copy-on-write snapshots are another optional and popular feature of <code>OpenEBS</code>. When using the <code>cStor</code> engine, snapshots are created instantaneously and are not limited in the number of snapshots. Incremental snapshotting enhances data migration and portability across <code>Kubernetes</code> clusters and across different cloud providers or data centers. Snapshot and clone operations are performed entirely in a <code>Kubernetes</code>-native way, using the standard <code>kubectl</code> command. Common use cases include efficient replication for backups and cloning for troubleshooting or developing read-only copies of data.</p>
<h4 id="backup-and-recovery">Backup and Recovery</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/475eef9acbff408a9bc1f930650cb829.png" alt="Backup and Recovery"></p>
<p>Backup and recovery of <code>OpenEBS</code> volumes can work with <code>Kubernetes</code> backup and recovery solutions such as <code>Velero</code> (formerly <code>Heptio Ark</code>) via the open source <code>OpenEBS Velero</code> plugin. The <code>OpenEBS</code> incremental snapshot feature is often used to back up data to object storage targets such as <code>AWS S3, GCP object storage, MinIO</code>. Such storage-level snapshots and backups use only incremental data for backup, saving a lot of bandwidth and storage space.</p>
<h4 id="true-kubernetes-cloud-native-storage">True Kubernetes cloud-native storage</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/dce983c46bda4810989709a2ce2b4819.png" alt="True Kubernetes cloud-native storage"></p>
<p><code>OpenEBS</code> is a cloud-native storage for stateful applications on <code>Kubernetes</code>, and <code>cloud-native</code> means following a loosely coupled architecture. Therefore, the general benefits of a cloud-native, loosely coupled architecture are applicable. For example, developers and <code>DevOps</code> architects can use standard <code>Kubernetes</code> skills and utilities to configure, use, and manage persistent storage requirements</p>
<h4 id="reduces-storage-tco-by-up-to-50">reduces storage TCO by up to 50%</h4>
<p>On most clouds, block storage is charged based on how much is purchased, not how much is used; capacity is often over-provisioned in order to achieve higher performance and eliminate the risk of outages while fully utilizing it. The thin provisioning capabilities of <code>OpenEBS</code> can share local storage or cloud storage and then increase the amount of data for stateful applications as needed. Storage can be added dynamically without disrupting the volumes exposed to the workload or application. Some users have reported savings of over <code>60%</code> of resources as a result of using <code>OpenEBS</code> thin provisioning.</p>
<h4 id="high-availability">High Availability</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/db3ed9aa053d44e39cc0a76aee1cf76b.png" alt="High Availability"></p>
<p>Because <code>OpenEBS</code> follows the <code>CAS</code> architecture, in the event of a node failure, <code>Kubernetes</code> will redispatch the <code>OpenEBS</code> controller while the underlying data is protected by using one or more copies. More importantly - because each workload can leverage its own <code>OpenEBS</code> - there is no risk of widespread system downtime due to storage loss. For example, the volume&rsquo;s metadata is not centralized, and it could be subject to catastrophic general-purpose outages, as many shared storage systems are. Instead, metadata remains local to the volume. The loss of any node results in the loss of a copy of the volume that exists only on that node. Since volume data is synchronously replicated on at least two other nodes, it will continue to be available at the same performance level when one node fails.</p>
<h3 id="introduction-to-cas">Introduction to CAS</h3>
<p>In a <code>CAS</code> or Container Attached Storage architecture, storage runs in a container and is closely related to the application to which the storage is bound. Storage runs as a microservice with no kernel module dependencies. An orchestration system like <code>Kubernetes</code> orchestrates storage volumes, just like any other microservice or container.<code>CAS</code> has the benefits of <code>DAS</code> and <code>NAS</code>.</p>
<h4 id="pv-on-non-cas-systems">PV on non-CAS systems</h4>
<p>In the non-<code>CAS</code> model, <code>Kubernetes</code> persistent volumes remain tightly coupled to the kernel module, making the storage software on the <code>Kubernetes</code> node inherently monolithic.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/eb66c8531e1942498e7361062c5e0d09.png" alt="PV on non-CAS systems"></p>
<h4 id="pv-on-cas-based-systems">PV on CAS-based systems</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/b2f8f56e1e3e419d86361d05b1286ebe.png" alt="PV on CAS-based systems"></p>
<p>Instead, <code>CAS</code> enables you to leverage the flexibility and scalability of cloud-native applications. The storage software that defines <code>Kubernetes PV (Persistent Volume)</code> is based on a microservices architecture. The control plane (storage controller) and data plane (storage replica) of the storage software run as <code>Kubernetes Pods</code>, thus enabling you to apply all the benefits of cloud-native to <code>CAS</code>.</p>
<h4 id="cas-benefits">CAS Benefits</h4>
<ul>
<li>
<p>Agile</p>
<p>Each storage volume in <code>CAS</code> has a containerized storage controller and a corresponding containerized copy. As a result, maintenance and tuning of resources around these components is truly agile. The <code>Kubernetes</code> rolling upgrade feature allows for seamless upgrades of storage controllers and storage replicas. Resource quotas such as <code>CPU</code> and memory can be tuned using container <code>cGroups</code>.</p>
</li>
<li>
<p>Storage Policy Granularity</p>
<p>Containerizing the storage software and dedicating the storage controller to each volume allows for maximum storage policy granularity. In the <code>CAS</code> architecture, all storage policies can be configured on a per-volume basis. In addition, you can monitor the storage parameters for each volume and dynamically update the storage policy to achieve the desired results for each workload. As this additional level of granularity is added to the volume storage policy, the control of storage throughput, <code>IOPS</code>, and latency also increases.</p>
</li>
<li>
<p>Cloud Native</p>
<p><code>CAS</code> loads storage software into containers and uses <code>Kubernetes</code> Custom Resource Definitions ( <code>CRDs</code>) to declare low-level storage resources, such as disks and storage pools. This model allows storage to be seamlessly integrated into other cloud-native tools. Cloud native tools such as <code>Prometheus, Grafana, Fluentd, Weavescope, Jaeger</code> can be used to provision, monitor, and manage storage resources.</p>
</li>
<li>
<p><code>PV</code> is a microservice in <code>CAS</code></p>
<p>As shown above, in the <code>CAS</code> architecture, the software for storage controllers and replicas is entirely microservices-based, so there are no kernel components involved. Typically, the storage controller <code>POD</code> is scheduled on the same nodes as the persistent volumes for efficiency, and the replica <code>POD</code> can be scheduled anywhere on the cluster nodes. Each replica is provisioned completely independently of the other replicas using any combination of local disks, <code>SAN</code> disks, and cloud disks. This provides tremendous flexibility in the allocation of storage for large-scale workloads.</p>
</li>
<li>
<p>Hyperconverged Non-Distributed</p>
<p>The <code>CAS</code> architecture does not follow typical distributed storage architectures. Storage becomes highly available through synchronous replication from storage controllers to storage replicas. Metadata for volume replicas is not shared between nodes, but managed independently on each local node. If one node fails, the storage controller (in this case a stateless container) rotates on a node that has a second or third copy running and the data is still available.</p>
<p>Similar to hyperconverged systems, the storage and performance of the volumes in <code>CAS</code> is scalable. Since each volume has its own storage controller, storage can scale as far as a node&rsquo;s storage capacity allows. In a given <code>Kubernetes</code> cluster, as the number of container applications increases, more nodes are added, thereby increasing the overall availability of storage capacity and performance, and thus making storage available to new application containers. This process is very similar to successful hyperconverged systems such as <code>Nutanix</code>.</p>
</li>
</ul>
<h2 id="introduction-to-the-openesb-architecture">Introduction to the OpenESB architecture</h2>
<p><code>OpenESB</code> follows the Container Attached Storage (<code>CAS</code>) model, where each volume has a dedicated controller <code>POD</code> and a set of replica <code>PODs</code>. The benefits of the <code>CAS</code> architecture are discussed on the <code>CNCF</code> <a href="https://www.cncf.io/blog/2018/04/19/container-attached-storage-a-primer/">blog</a>. <code>OpenEBS</code> is simple to operate and use, as it looks and feels like other cloud-native and <code>Kubernetes</code>-friendly projects.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/6da9e9a1cabc4857adf82bfc78022632.png" alt="OpenESB"></p>
<p><code>OpenEBS</code> has many components that can be divided into the following categories :</p>
<ul>
<li>Control plane components - <code>Provisioner</code> , <code>API Server</code> , <code>volume exports</code> , <code>volume sidecars</code>.</li>
<li>Data plane components - <code>Jiva</code> , <code>cStor</code>.</li>
<li>Node disk manager - <code>Discover</code> , <code>monitor</code> , manages the media for connecting to <code>k8s</code>.</li>
<li>Integration with cloud native tools - already integrated with <code>Prometheus</code> , <code>Grafana</code> , <code>Fluentd</code> , <code>Jaeger</code>.</li>
</ul>
<h3 id="control-plane">Control plane</h3>
<p>The control plane of an <code>OpenEBS</code> cluster is often referred to as <code>Maya</code>.</p>
<p>The <code>OpenEBS</code> control plane is responsible for providing volumes, associated volume operations such as snapshots, cloning, creating storage policies, enforcing storage policies, exporting volume metrics used by <code>Prometheus/grafana</code>, and so on.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/56163e93983d40aea2b133c36e8d6d5d.png" alt="OpenEBS Control plane"></p>
<p><code>OpenEBS</code> provides a dynamic provisioner, which is the standard external storage plugin for <code>Kubernetes</code>. The main task of the <code>OpenEBS PV</code> provider is to initiate volume provisioning to application <code>PODS</code> and to implement the <code>Kubernetes</code> specification for <code>PV</code>.</p>
<p>The <code>m-apiserver</code> opens up the <code>REST API</code> for storage and takes care of a lot of volume policy processing and management.</p>
<p>Connectivity between the control plane and the data plane uses the <code>Kubernetes sidecar</code> pattern. The scenarios where the control plane needs to communicate with the data plane are shown below.</p>
<ul>
<li>For volume statistics such as <code>IOPS</code>, throughput, latency, etc. - implemented via <code>sidecar</code> for volume bursts.</li>
<li>Enforcement of volume policies using volume controller <code>pod</code>, disk/pool management using volume replica <code>pod</code> - implemented via volume management <code>sidecar</code>.</li>
</ul>
<h4 id="openebs-pv-provisioner">OpenEBS PV Provisioner</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/9d8bca30d3d44b5694d228f6b6b638eb.png" alt="OpenEBS PV Provisioner"></p>
<p>This component runs as a <code>POD</code> and makes configuration decisions , it is used in the following way:</p>
<p>The developer constructs a declaration with the required volume parameters, selects the appropriate storage class, and invokes the <code>kubelet</code> on the <code>YAML</code> specification. The <code>OpenEBS PV</code> dynamic provider interacts with the <code>maya-apiserver</code> to create deployment specifications for the volume controller <code>pod</code> and the volume copy <code>pod</code> on the appropriate node. Scheduling of volume <code>Pod</code> (controllers/copies) can be controlled using annotations in the <code>PVC</code> specification.</p>
<p>Currently, <code>OpenEBS Provisioner</code> supports only one binding type, <code>iSCSI</code>.</p>
<h4 id="maya-apiserver">Maya-ApiServer</h4>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/06171cc707b14a59970d9d65816cacac.png" alt="Maya-ApiServer"></p>
<p>m-apiserver runs as a POD. As the name implies, m-apiserver exposes the OpenEBS REST api.</p>
<p>The m-apiserver is also responsible for creating the deployment specification files needed to create the volume pod. After generating these spec files, it calls kube-apiserver to schedule these pods accordingly. openEBS PV provider creates a PV object and mounts it to the application pod at the end of volume distribution. the PV is hosted by a controller pod, which is supported by a set of replica pods in different nodes. The controller pod and replica pods are part of the data plane and are described in more detail in the Storage Engine section.</p>
<p>Another important task of m-apiserver is volume policy management. OpenEBS provides very fine-grained specifications for representing policies. m-apiserver interprets these YAML specifications, translates them into executable components, and executes them via the capacity management sidecar.</p>
<h4 id="maya-volume-exporter">Maya Volume Exporter</h4>
<p>The Maya volume exporter is a sidecar for each storage controller pod.</p>
<p>These <code>sidecar</code>s connect the control plane to the data plane for statistical information. The granularity of statistical information is at the volume level. Some example statistics are as follows.</p>
<ul>
<li>Volume read latency.</li>
<li>Volume write latency.</li>
<li>Volume read speed per second.</li>
<li>Volume write speed per second.</li>
<li>Read block size.</li>
<li>Write block size.</li>
<li>Capacity statistics.</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/91f5f2183e994107b8c5e0ef99ba5ca8.png" alt="Volme export data flow"></p>
<p>These statistics are typically pulled by the <code>Prometheus</code> client, which is installed and configured during the <code>OpenBS</code> installation.</p>
<h4 id="volume-management-sidecar">volume management <code>sidecar</code></h4>
<p><code>Sidecars</code> are also used to pass controller configuration parameters and volume policies to volume controller <code>pod</code> (volume controller <code>pod</code> is a data plane), and to pass replica configuration parameters and replica data protection parameters to volume replica <code>pod</code>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/16eff330886b4b64b08f1d01de292cb0.png" alt="Sidecars"></p>
<h3 id="data-plane">Data plane</h3>
<p>The <code>OpenEBS</code> data plane is responsible for the actual volume <code>IO</code> path. Storage engines implement the actual <code>IO</code> path in the data plane. Currently, <code>OpenEBS</code> provides two storage engines that can be easily plugged in. They are called <code>Jiva</code> and <code>cStor</code>. Both storage engines run entirely in the <code>Linux</code> user space and are based on a microservices architecture.</p>
<h4 id="jiva">Jiva</h4>
<p>The <code>Jiva</code> storage engine is based on <code>Rancher's LongHorn</code> and <code>gotgt</code> development implementations, developed in the <code>go</code> language, and runs under the user namespace. The <code>LongHorn</code> controller synchronizes the input <code>IO</code> copy to the <code>LongHorn</code> replica. This copy treats <code>Linux</code> sparse files as the basis for building storage features such as thin provisioning, snapshots, rebuilds, etc.</p>
<h4 id="cstor">cStor</h4>
<p>The <code>cStor</code> data engine is written in the <code>C</code> language and features a high-performance <code>iSCSI target</code> and <code>Copy-On-Write</code> block system that provides data integrity, data resiliency, and point-in-time snapshots and clones. <code>cStor</code> has a pooling feature that aggregates disks on a node in striped, mirrored, or <code>RAIDZ</code> mode to provide larger capacity and performance units. <code>cStor</code> can also synchronize data replication to multiple nodes across regions, thus avoiding data unavailability due to node loss or node reboot.</p>
<h4 id="localpv">LocalPV</h4>
<p>For applications that do not require storage-level replication, <code>LocalPV</code> may be a good choice because it provides higher performance. <code>OpenEBS LocalPV</code> is similar to <code>Kubernetes LocalPV</code>, except that it is dynamically provisioned by the <code>OpenEBS</code> control plane, just like any other regular <code>PV</code>. There are two types of <code>OpenEBS LocalPV</code>: <code>hostpath LocalPV</code> and <code>device LocalPV</code>. <code>hostpath LocalPV</code> refers to a subdirectory on the host, and <code>LocalPV</code> refers to a disk found on a node (either directly connected or network connected). <code>OpenEBS</code> introduces a <code>LocalPV</code> provider for selecting matching disk or host paths based on <code>PVC</code> and some criteria in the storage class specification.</p>
<h4 id="node-disk-manager">Node Disk Manager</h4>
<p>The Node Disk Manager ( <code>NDM</code>) fills a gap in the toolchain needed to manage persistent storage for stateful applications using <code>Kubernetes</code>. <code>DevOps</code> architects in the container era must meet the infrastructure needs of applications and application developers in an automated way that provides resiliency and consistency across environments. These requirements mean that the storage stack itself must be flexible enough that it can be easily used by <code>Kubernetes</code> and other software in the cloud-native ecosystem. <code>NDM</code> plays a foundational role in <code>Kubernetes</code> storage stack by unifying the different disks and providing the ability to aggregate them by identifying them as <code>Kubernetes</code> objects. In addition, <code>NDM</code> discovers, provision, monitors, and manages the underlying disks in a way that allows <code>Kubernetes PV</code> providers (such as <code>OpenEBS</code> and other storage systems) and <code>Prometheus</code> to manage disk subsystems.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/96c72c306c2047d5b1712511f755380c.png" alt="Node Disk Manager"></p>
<h2 id="cas-engine">CAS Engine</h2>
<h3 id="storage-engine-overview">Storage Engine Overview</h3>
<p>The storage engine is the data plane component of the persistent volume <code>IO</code> path. In the <code>CAS</code> architecture, users can select different data planes for different application workloads based on different configuration policies. The storage engine can be optimized for a given workload by feature set or performance.</p>
<p>Operators or administrators typically select a storage engine with a specific software version and build optimized volume templates that are fine-tuned based on the type of underlying disks, resiliency, number of replicas, and set of nodes participating in the <code>Kubernetes</code> cluster. Users can select the optimal volume template when issuing volumes, providing maximum flexibility to run the optimal combination of software and storage for all storage volumes on a given <code>Kubernetes</code> cluster.</p>
<h3 id="storage-engine-types">Storage engine types</h3>
<p><code>OpenEBS</code> provides three types of storage engines.</p>
<ul>
<li>
<p>Jiva</p>
<p><code>Jiva</code> is the first storage engine released in <code>OpenEBS version 0.1</code> and is the easiest to use. It is based on <code>GoLang</code> and uses the <code>LongHorn</code> and <code>gotgt</code> stacks internally. <code>Jiva</code> runs entirely in user space and provides standard block storage features such as synchronous replication. <code>Jiva</code> is typically suitable for smaller capacity workloads and is not suitable for situations where large numbers of snapshots and cloning features are the primary requirement.</p>
</li>
<li>
<p>cStor</p>
<p><code>cStor</code> is the latest storage engine released in <code>OpenEBS version 0.7</code>. <code>cStor</code> is very robust, provides data consistency, and supports enterprise storage features such as snapshots and clones very well. It also provides a robust storage pooling feature for comprehensive storage management in terms of capacity and performance. Together with <code>NDM</code> ( <code>Node Disk Manager</code>), <code>cStor</code> provides complete persistent storage features for stateful applications on <code>Kubernetes</code>.</p>
</li>
<li>
<p>OpenEBS Local PV</p>
<p><code>OpenEBS Local PV</code> is a new storage engine that can create persistent volumes or <code>PV</code>s from local disks or host paths on working nodes. The <code>CAS</code> engine is available from version <code>1.0.0</code> of <code>OpenEBS</code>. With <code>OpenEBS Local PV</code>, the performance will be equivalent to the local disk or file system (host path) where the volume is created. Many cloud-native applications may not require advanced storage features such as replication, snapshots, or cloning, as they provide these features themselves. Such applications require access to managed disks in the form of persistent volumes.</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/0abea2b2b33a47328d303d2dd41f25b7.png" alt="Jiva &amp; cStor"></p>
<ul>
<li><code>SP</code> storage pools, representing <code>Jiva</code> custom storage resources.</li>
<li><code>CV</code> <code>cStor</code> volume, representing a <code>cStor</code> volume custom resource.</li>
<li><code>CVR</code> <code>cStor</code> volume copy.</li>
<li><code>SPC</code> Storage pool declaration, representing <code>cStor</code> pool aggregated custom resources.</li>
<li><code>CSP</code> <code>cStor</code> storage pool, representing the custom resources on each node of the <code>cStor Pool</code>.</li>
</ul>
<p>One <code>SPC</code> corresponds to multiple <code>CSP</code>s and correspondingly one <code>CV</code> corresponds to multiple <code>CVR</code>s.</p>
<h3 id="storage-engine-declaration">Storage engine declaration</h3>
<p>Select a storage engine by specifying the annotation <code>openebs</code>. <code>io/cas-type</code> in the <code>StorageClass</code> specification. The <code>StorageClass</code> defines the details of the provisioner. A separate provisioner is specified for each <code>CAS</code> engine.</p>
<h4 id="cstor-storage-class-specification-file-contents">cStor Storage Class Specification File Contents</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">storage.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StorageClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cStor-storageclass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">openebs.io/cas-type</span><span class="p">:</span><span class="w"> </span><span class="l">cstor</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cas.openebs.io/config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: StoragePoolClaim
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: &#34;cStorPool-SSD&#34;</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l">openebs.io/provisioner-iscsi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="jiva-storage-class-specification-file-contents">Jiva Storage Class Specification File Contents</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">storage.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StorageClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">jiva-storageclass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">openebs.io/cas-type</span><span class="p">:</span><span class="w"> </span><span class="l">jiva</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cas.openebs.io/config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: StoragePool
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: default</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l">openebs.io/provisioner-iscsi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>When <code>cas</code> is of type <code>Jiva</code>, the <code>default</code> value of <code>StoragePool</code> has a special meaning. When <code>pool</code> is the default value, the <code>Jiva</code> engine will open up data storage for the replica <code>pod</code> from the container&rsquo;s (replica <code>pod</code>) own storage space. When the required volume size is small (e.g. <code>5G</code> to <code>10G</code>), <code>StoragePool default</code> works well because it can fit inside the container itself.</p>
<h4 id="local-pv-storage-class-specification-file-contents---host-path">Local PV Storage Class Specification File Contents - Host Path</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">storage.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StorageClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">localpv-hostpath-sc</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">openebs.io/cas-type</span><span class="p">:</span><span class="w"> </span><span class="l">local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cas.openebs.io/config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: BasePath
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: &#34;/var/openebs/local&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: StorageType
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: &#34;hostpath&#34;</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l">openebs.io/local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="local-pv-storage-class-specification-document-content---host-device">Local PV Storage Class Specification Document Content - Host Device</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">storage.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StorageClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">localpv-device-sc</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">openebs.io/cas-type</span><span class="p">:</span><span class="w"> </span><span class="l">local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cas.openebs.io/config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: StorageType
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: &#34;device&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: FSType
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: ext4</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l">openebs.io/local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Comparison of <code>cStor</code>, <code>Jiva</code>, <code>LocalPV</code> characteristics.</p>
<table>
<thead>
<tr>
<th>Features</th>
<th>Jiva</th>
<th>cStor</th>
<th>Local PV</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lightweight Runs in User Space</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Synchronous Replication</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Synchronous Replication</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Supports snapshots, clones</td>
<td>Basic</td>
<td>Advanced</td>
<td>No</td>
</tr>
<tr>
<td>Data consistency</td>
<td>Yes</td>
<td>Yes</td>
<td>NA</td>
</tr>
<tr>
<td>Restore backups with Velero</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Suitable for high volume workloads</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Automatic thin provisioning</td>
<td></td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Disk pooling or aggregation support</td>
<td></td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Dynamic Scaling</td>
<td></td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Data Resiliency (RAID support)</td>
<td></td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Near-native disk performance</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p><code>cStor</code> is recommended for most scenarios as it offers powerful features including snapshots/clones, storage pooling capabilities such as streamlined resource provisioning, on-demand scaling, etc.</p>
<p><code>Jiva</code> is suitable for workload scenarios with low capacity requirements, such as <code>5</code> to <code>50G</code>. Although there is no space limit for using <code>Jiva</code>, it is recommended for low-volume workloads. <code>Jiva</code> is very easy to use and provides enterprise-class container local storage without the need for dedicated hard drives. In scenarios where snapshot and cloning capabilities are required, <code>cStor</code> is preferred over <code>Jiva</code>.</p>
<h3 id="cas-engine-usage-scenarios">CAS Engine Usage Scenarios</h3>
<p>As shown in the table above, each storage engine has its own advantages. The choice of engine depends entirely on the workload of the application and its current and future capacity and/or performance growth. The following guidelines provide some assistance in choosing a specific engine when defining storage classes.</p>
<h4 id="ideal-conditions-for-selecting-cstor">Ideal conditions for selecting cStor</h4>
<ul>
<li>When you need to synchronize replicated data and have multiple disks on a node.</li>
<li>When you manage storage for multiple applications from local disks on each node or from a network disk pool. Storage tier management through features such as thin provisioning, on-demand scaling of storage pools and volumes, and on-demand scaling of storage pool performance. <code>cStor</code> for running locally . <code>Kubernetes</code> local storage service built on a <code>Kubernetes</code> cluster, similar to <code>AWS EBS</code> or Google <code>PD</code>.</li>
<li>When you need storage-level snapshot and cloning capabilities</li>
<li>When you need enterprise-class storage protection features such as data consistency, resiliency ( <code>RAID</code> protection).</li>
<li>If your application does not require storage-level replication, then using <code>OpenEBS</code> host path <code>LocalPV</code> or <code>OpenEBS</code> device <code>LocalPV</code> may be a better choice.</li>
</ul>
<h4 id="ideal-conditions-for-choosing-jiva">Ideal conditions for choosing Jiva</h4>
<ul>
<li>
<p>When you want synchronous replication of data and have a single local disk or a single managed disk (such as a cloud disk ( <code>EBS</code>, <code>GPD</code>)) and do not need snapshot or cloning features.</p>
</li>
<li>
<p><code>Jiva</code> is the easiest to manage because disk management or pool management is out of the scope of this engine. <code>Jiva</code> pools are mount paths for local disks, network disks, virtual disks, or cloud disks.</p>
</li>
<li>
<p>Jiva is preferable to cStor for the following scenarios :</p>
<ul>
<li>When the program does not need the storage level snapshot, clone feature.</li>
<li>When there are no free disks on the node. <code>Jiva</code> can be used on the host directory and still enable replication.</li>
<li>When there is no need to dynamically scale storage on local disks. Adding more disks to the <code>Jiva</code> pool is not possible, so the size of the <code>Jiva</code> pool is fixed if it is on physical disks. However, if the underlying disk is a virtual disk, a network disk, or a cloud disk, the size of the <code>Jiva</code> pool can be changed dynamically.</li>
<li>Smaller capacity requirements. Large capacity applications often require dynamic capacity increases, and <code>cStor</code> is better suited to this need.</li>
</ul>
</li>
</ul>
<h4 id="ideal-conditions-for-selecting-openebs-host-path-localpv">Ideal conditions for selecting OpenEBS host path LocalPV</h4>
<ul>
<li>When the application itself has the ability to manage replication (e.g., <code>es</code>), replication at the storage level is not required. In most such cases, the application is deployed as a <code>statefulset</code>.</li>
<li>Higher than the read/write performance requirements of <code>Jiva</code> and <code>cStor</code>.</li>
<li><code>Hostpath</code> is recommended when a specific application does not have a dedicated local disk or when a specific application does not need dedicated storage. If you want to share a local disk across multiple applications, hostpath <code>LocalPV</code> is the right way to go.</li>
</ul>
<h4 id="ideal-conditions-for-selecting-an-openebs-host-device-localpv">Ideal conditions for selecting an OpenEBS host device LocalPV</h4>
<ul>
<li>When the application manages the replication itself and does not require replication at the storage layer. In most of these cases, the application is deployed as a stateful set.</li>
<li>Higher than the read and write performance requirements of <code>Jiva</code> and <code>cStor</code>.</li>
<li>Higher than the read and write performance requirements of the <code>OpenEBS</code> host path <code>LocalPV</code>.</li>
<li>When close disk performance is required. This volume is dedicated to writing to a single <code>SSD</code> or <code>NVMe</code> interface for maximum performance.</li>
</ul>
<h4 id="summary">Summary</h4>
<ul>
<li><code>LocalPV</code> is preferred if the application is in production and does not require storage-level replication.</li>
<li>Prefer <code>cStor</code> if your application is in production and requires storage-level replication.</li>
<li>Prefer <code>Jiva</code> if your application is small and needs storage-level replication but does not require snapshots or clones.</li>
</ul>
<h3 id="node-disk-manager-ndm">Node Disk Manager (NDM)</h3>
<p>The Node Disk Manager ( <code>NDM</code>) is an important component of the <code>OpenEBS</code> architecture. The <code>NDM</code> treats block devices as resources to be monitored and managed, just like other resources such as <code>CPU</code>, memory and network. It is a daemon that runs on each node and detects additional block devices based on filters and loads them into <code>Kubernetes</code> as block device custom resources. These custom resources are intended to provide easy access to <code>Kubernetes</code> by providing something like the following:</p>
<ul>
<li>Easy access to the list of block devices available in the <code>Kubernetes</code> cluster.</li>
<li>Predicting disk failures to help take preventative action.</li>
<li>Allow dynamic mounting/unmounting of disks to storage <code>Pods</code> without restarting the corresponding <code>NDM Pod</code> running on the node where the disk is mounted/unmounted.</li>
</ul>
<p>Despite all of the above, <code>NDM</code> helps provide overall simplification of persistent volumes.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images1/2022/05/24/8ceb65645b0b455a89efadd7baf6a91f.png" alt="Node Disk Manager"></p>
<p><code>NDM</code> is deployed as a daemon during the <code>OpenEBS</code> installation. <code>NDM daemonset</code> discovers the disks on each node and creates a custom resource called <code>Block Device</code> or <code>BD</code>.</p>
<h4 id="access-rights-description">Access Rights Description</h4>
<p>The <code>NDM</code> daemon runs in a container and must access the underlying storage device and run in privileged mode. <code>NDM</code> requires privileged mode because it needs access to the <code>/dev, /proc</code> and <code>/sys</code> directories to monitor the attached devices, and also needs to use various probes to get detailed information about the attached devices. <code>NDM</code> is responsible for discovering block devices and filtering out devices that should not be used by <code>OpenEBS</code>; for example, detecting disks with <code>OS</code> file systems. The <code>NDM pod</code> by default mounts the host&rsquo;s <code>/proc</code> directory in the container and then loads <code>/proc/1/mounts</code> to find the disks used by the OS.</p>
<h4 id="ndm-daemon-features">NDM daemon features</h4>
<ul>
<li>Discover block devices on <code>Kubernetes</code> nodes.
<ul>
<li>Discover block devices at boot time - create and/or update status.</li>
<li>Maintain cluster-wide unique <code>id</code> for disks : <code>Hash</code> calculation for <code>WWN / PartitionUUUID / FileSystemUUID / DeviceMapperUUUID</code>.</li>
</ul>
</li>
<li>Detects the addition/removal of a block device in a node and updates the block device status.</li>
<li>Add a block device as a <code>Kubernetes</code> custom resource with the following properties.
<ul>
<li><code>spec</code> : if available, will update the following
<ul>
<li>device path</li>
<li>device link</li>
<li>Vendor and model information</li>
<li><code>WWN</code> and serial number</li>
<li>Capacity</li>
<li>Sector and block sizes</li>
</ul>
</li>
<li><code>labels</code> :
<ul>
<li>Host name ( <code>kubernetes.io/hostname</code> )</li>
<li>blockdevice type ( <code>ndm.io/blockdevice-type</code> )</li>
<li>Managed ( <code>ndm.io/managed</code> )</li>
</ul>
</li>
<li><code>status</code> : The status can have the following values
<ul>
<li><code>Active</code> : block device present on the node</li>
<li><code>Inactive</code> : Block device does not exist on the given node</li>
<li><code>Unknown</code> : <code>NDM</code> stopped/unable to determine status on the node where the block device was last detected</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="filters">filters</h4>
<ul>
<li>
<p>Configure filters for the block device type for which the block device <code>CR</code> is to be created. Filters can be configured by vendor type, device path mode, or mount point.</p>
</li>
<li>
<p>Filters can be either inclusion filters or exclusion filters. They are configured as <code>configmap</code> . Admin users can configure these filters at <code>OpenEBS</code> installation time by changing the <code>NDM configmap</code> in the <code>OpenEBS</code> operator <code>yaml</code> file or <code>helm</code> values. <code>yaml</code> file. If these filters need to be updated after installation, then one of the following methods can be followed :</p>
<ul>
<li>Install <code>OpenEBS</code> using the <code>operator</code> method. In the <code>yaml</code> file, update the filters in the <code>configmap</code> and apply <code>operator.yaml</code>.</li>
<li>If <code>OpenEBS</code> was installed using <code>helm</code>, update <code>configmap</code> in <code>values.yaml</code> and upgrade using <code>helm</code>.</li>
<li>Or, use <code>kubectl</code> to edit <code>NDM configmap</code> and update the filters.</li>
</ul>
</li>
</ul>
<h2 id="practice">Practice</h2>
<p>The <code>cStor</code> and <code>Jiva</code> engines of <code>OpenEBS</code> are not recommended for production environments due to the number of components and the cumbersome configuration compared to other storage solutions, so we will only discuss the <code>Local PV</code> engine here.</p>
<p>The <code>Local PV</code> engine does not have storage-level replication capabilities and is suitable for back-end storage of <code>k8s</code> stateful services (e.g., <code>es</code>, <code>redis</code>, etc.).</p>
<h3 id="local-pv-hostpath-practices">Local PV Hostpath Practices</h3>
<p>The <code>OpenEBS local PV Hostpath</code> volume has the following advantages over the <code>Kubernetes Hostpath</code> volume :</p>
<ul>
<li>The <code>OpenEBS</code> local <code>PV Hostpath</code> allows your application to access the <code>Hostpath</code> via <code>StorageClass</code>, <code>PVC</code> and <code>PV</code>. This gives you the flexibility to change the <code>PV</code> provider without having to redesign the application <code>YAML</code>.</li>
<li>Data protection with <code>Velero</code> backup and recovery.</li>
<li>Protect against hostpath security vulnerabilities by completely masking the hostpath to application <code>YAML</code> and <code>pod</code>.</li>
</ul>
<p><strong>Environmental Dependencies :</strong></p>
<ul>
<li><code>k8s 1.12</code> or higher</li>
<li><code>OpenEBS 1.0</code> or higher</li>
</ul>
<p><strong>Practical environment :</strong></p>
<ul>
<li><code>docker 19.03.8</code></li>
<li><code>k8s 1.18.6</code></li>
<li><code>CentOS7</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get node
</span></span><span class="line"><span class="cl">NAME    STATUS   ROLES           AGE     VERSION
</span></span><span class="line"><span class="cl">node1   Ready    master,worker   8m8s    v1.18.6
</span></span><span class="line"><span class="cl">node2   Ready    master,worker   7m15s   v1.18.6
</span></span><span class="line"><span class="cl">node3   Ready    master,worker   7m15s   v1.18.6
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="create-a-data-directory">Create a data directory</h4>
<p>Set up a directory on the node where <code>Local PV Hostpaths</code> will be created. This directory will be called <code>BasePath</code>. The default location is <code>/var/openebs/local</code>.</p>
<p>Nodes <code>node1</code>, <code>node2</code>, <code>node3</code> create the <code>/data/openebs/local</code> directory (/data can be pre-mounted with a data disk, if no additional data disk is mounted, the OS &lsquo;/&rsquo; mount point storage is used).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ mkdir -p /data/openebs/local
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="download-the-application-description-file">Download the application description file</h4>
<p><a href="https://openebs.github.io/charts/openebs-operator-lite.yaml">yaml file</a></p>
<h4 id="release-the-openebs-application">Release the <code>openebs</code> application</h4>
<p>Based on the above configuration file, ensure that the following mirrors are accessible to the <code>k8s</code> cluster (it is recommended to import a local private mirror repository, e.g. : <code>harbor</code>).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">openebs/node-disk-manager:1.5.0
</span></span><span class="line"><span class="cl">openebs/node-disk-operator:1.5.0
</span></span><span class="line"><span class="cl">openebs/provisioner-localpv:2.10.0
</span></span></code></pre></td></tr></table>
</div>
</div><p>Update the mirror <code>tag</code> in <code>openebs-operator.yaml</code> to the actual <code>tag</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">image: openebs/node-disk-manager:1.5.0
</span></span><span class="line"><span class="cl">image: openebs/node-disk-operator:1.5.0
</span></span><span class="line"><span class="cl">image: openebs/provisioner-localpv:2.10.0
</span></span></code></pre></td></tr></table>
</div>
</div><p>apply</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl apply -f openebs-operator.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><p>View applying Status.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get pod -n openebs -w
</span></span><span class="line"><span class="cl">NAME                                           READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">openebs-localpv-provisioner-6d6d9cfc99-4sltp   1/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-85rng                              1/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-operator-7df6668998-ptnlq          0/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-qgqm9                              1/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-zz7ps                              1/1     Running   <span class="m">0</span>          10s
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="creating-storage-classes">Creating Storage Classes</h4>
<p>Change the contents of the configuration file.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">value: <span class="s2">&#34;/var/openebs/local/&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Publish Create Storage Classes.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="l">$ cat &gt; openebs-hostpath-sc.yaml &lt;&lt;EOF</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">storage.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StorageClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">openebs-hostpath</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">openebs.io/cas-type</span><span class="p">:</span><span class="w"> </span><span class="l">local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cas.openebs.io/config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      #hostpath type will create a PV by
</span></span></span><span class="line"><span class="cl"><span class="sd">      # creating a sub-directory under the
</span></span></span><span class="line"><span class="cl"><span class="sd">      # BASEPATH provided below.
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: StorageType
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: &#34;hostpath&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      #Specify the location (directory) where
</span></span></span><span class="line"><span class="cl"><span class="sd">      # where PV(volume) data will be saved.
</span></span></span><span class="line"><span class="cl"><span class="sd">      # A sub-directory with pv-name will be
</span></span></span><span class="line"><span class="cl"><span class="sd">      # created. When the volume is deleted,
</span></span></span><span class="line"><span class="cl"><span class="sd">      # the PV sub-directory will be deleted.
</span></span></span><span class="line"><span class="cl"><span class="sd">      #Default value is /var/openebs/local
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: BasePath
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: &#34;/data/openebs/local/&#34;</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l">openebs.io/local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">volumeBindingMode</span><span class="p">:</span><span class="w"> </span><span class="l">WaitForFirstConsumer</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">reclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Delete</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">EOF</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">$ kubectl apply -f openebs-hostpath-sc.yaml</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="create-pvc-verify-availability">Create PVC Verify Availability</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ cat &gt; local-hostpath-pvc.yaml <span class="s">&lt;&lt;EOF
</span></span></span><span class="line"><span class="cl"><span class="s">kind: PersistentVolumeClaim
</span></span></span><span class="line"><span class="cl"><span class="s">apiVersion: v1
</span></span></span><span class="line"><span class="cl"><span class="s">metadata:
</span></span></span><span class="line"><span class="cl"><span class="s">  name: local-hostpath-pvc
</span></span></span><span class="line"><span class="cl"><span class="s">spec:
</span></span></span><span class="line"><span class="cl"><span class="s">  storageClassName: openebs-hostpath
</span></span></span><span class="line"><span class="cl"><span class="s">  accessModes:
</span></span></span><span class="line"><span class="cl"><span class="s">    - ReadWriteOnce
</span></span></span><span class="line"><span class="cl"><span class="s">  resources:
</span></span></span><span class="line"><span class="cl"><span class="s">    requests:
</span></span></span><span class="line"><span class="cl"><span class="s">      storage: 5G
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl apply -f local-hostpath-pvc.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><p>Check the <code>pvc</code> status.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get pvc
</span></span><span class="line"><span class="cl">NAME                 STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS       AGE
</span></span><span class="line"><span class="cl">local-hostpath-pvc   Pending                                      openebs-hostpath   2m15s
</span></span></code></pre></td></tr></table>
</div>
</div><p>The output shows <code>STATUS</code> as <code>Pending</code>. This means that <code>PVC</code> is not yet used by the application.</p>
<h4 id="create-pd">Create Pd</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ cat &gt; local-hostpath-pod.yaml <span class="s">&lt;&lt;EOF
</span></span></span><span class="line"><span class="cl"><span class="s">apiVersion: v1
</span></span></span><span class="line"><span class="cl"><span class="s">kind: Pod
</span></span></span><span class="line"><span class="cl"><span class="s">metadata:
</span></span></span><span class="line"><span class="cl"><span class="s">  name: hello-local-hostpath-pod
</span></span></span><span class="line"><span class="cl"><span class="s">spec:
</span></span></span><span class="line"><span class="cl"><span class="s">  volumes:
</span></span></span><span class="line"><span class="cl"><span class="s">  - name: local-storage
</span></span></span><span class="line"><span class="cl"><span class="s">    persistentVolumeClaim:
</span></span></span><span class="line"><span class="cl"><span class="s">      claimName: local-hostpath-pvc
</span></span></span><span class="line"><span class="cl"><span class="s">  containers:
</span></span></span><span class="line"><span class="cl"><span class="s">  - name: hello-container
</span></span></span><span class="line"><span class="cl"><span class="s">    image: busybox
</span></span></span><span class="line"><span class="cl"><span class="s">    command:
</span></span></span><span class="line"><span class="cl"><span class="s">       - sh
</span></span></span><span class="line"><span class="cl"><span class="s">       - -c
</span></span></span><span class="line"><span class="cl"><span class="s">       - &#39;while true; do echo &#34;`date` [`hostname`] Hello from OpenEBS Local PV.&#34; &gt;&gt; /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done&#39;
</span></span></span><span class="line"><span class="cl"><span class="s">    volumeMounts:
</span></span></span><span class="line"><span class="cl"><span class="s">    - mountPath: /mnt/store
</span></span></span><span class="line"><span class="cl"><span class="s">      name: local-storage
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Publish Create.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl apply -f local-hostpath-pod.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="verify-that-data-is-written-to-the-volume">Verify that data is written to the volume</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl <span class="nb">exec</span> hello-local-hostpath-pod -- cat /mnt/store/greet.txt
</span></span><span class="line"><span class="cl">Thu Jun <span class="m">24</span> 15:10:45 CST <span class="m">2021</span> <span class="o">[</span>node1<span class="o">]</span> Hello from OpenEBS Local PV.
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="verify-that-the-container-is-using-the-local-pv-hostpath-volume">Verify that the container is using the Local PV Hostpath volume</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl describe pod hello-local-hostpath-pod
</span></span><span class="line"><span class="cl">Name:         hello-local-hostpath-pod
</span></span><span class="line"><span class="cl">Namespace:    default
</span></span><span class="line"><span class="cl">Priority:     <span class="m">0</span>
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Volumes:
</span></span><span class="line"><span class="cl">  local-storage:
</span></span><span class="line"><span class="cl">    Type:       PersistentVolumeClaim <span class="o">(</span>a reference to a PersistentVolumeClaim in the same namespace<span class="o">)</span>
</span></span><span class="line"><span class="cl">    ClaimName:  local-hostpath-pvc
</span></span><span class="line"><span class="cl">    ReadOnly:   <span class="nb">false</span>
</span></span><span class="line"><span class="cl">  default-token-98scc:
</span></span><span class="line"><span class="cl">    Type:        Secret <span class="o">(</span>a volume populated by a Secret<span class="o">)</span>
</span></span><span class="line"><span class="cl">    SecretName:  default-token-98scc
</span></span><span class="line"><span class="cl">    Optional:    <span class="nb">false</span>
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="view-pvc-status">View PVC Status</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get pvc local-hostpath-pvc
</span></span><span class="line"><span class="cl">NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
</span></span><span class="line"><span class="cl">local-hostpath-pvc   Bound    pvc-6eac3773-49ef-47af-a475-acb57ed15cf6   5G         RWO            openebs-hostpath   10m
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="view-the-data-storage-directory-for-this-pv-volume">View the data storage directory for this PV volume</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get -o yaml pv pvc-6eac3773-49ef-47af-a475-acb57ed15cf6<span class="p">|</span>grep <span class="s1">&#39;path:&#39;</span>
</span></span><span class="line"><span class="cl">          f:path: <span class="o">{}</span>
</span></span><span class="line"><span class="cl">    path: /data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6
</span></span></code></pre></td></tr></table>
</div>
</div><p>And <code>pv</code> is configured with an affinity that sets the scheduling node to <code>node2</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">ReadWriteOnce</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">5G</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">claimRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolumeClaim</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">local-hostpath-pvc</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">resourceVersion</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;9034&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">uid</span><span class="p">:</span><span class="w"> </span><span class="l">6eac3773-49ef-47af-a475-acb57ed15cf6</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">local</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">fsType</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">required</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="l">node2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Delete</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">openebs-hostpath</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumeMode</span><span class="p">:</span><span class="w"> </span><span class="l">Filesystem</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>The result proves that the data exists only under <code>node2</code>.</p>
<h4 id="clean-up-pod">Clean up Pod</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl delete pod hello-local-hostpath-pod
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="benchmarking">Benchmarking</h4>
<p><a href="https://github.com/openebs/performance-benchmark/blob/master/fio-benchmarks/fio-deploy.yaml">Download the benchmarking Job declaration file</a></p>
<p>Adjust the following.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">openebs/perf-test:latest</span><span class="w"> </span><span class="c"># tag</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l">dbench</span><span class="w"> </span><span class="c"># local-hostpath-pvc</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Release Run.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl create -f fio-deploy.yaml 
</span></span></code></pre></td></tr></table>
</div>
</div><p>Check the running status.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get pod
</span></span><span class="line"><span class="cl">NAME                 READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">dbench-729cw-nqfpt   1/1     Running   <span class="m">0</span>          24s
</span></span></code></pre></td></tr></table>
</div>
</div><p>View Benchmarking Results.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl logs -f dbench-729cw-nqfpt
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">All tests complete.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">==================</span>
</span></span><span class="line"><span class="cl"><span class="o">=</span> Dbench <span class="nv">Summary</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl"><span class="o">==================</span>
</span></span><span class="line"><span class="cl">Random Read/Write IOPS: 2144/6654. BW: 131MiB/s / 403MiB/s
</span></span><span class="line"><span class="cl">Average Latency <span class="o">(</span>usec<span class="o">)</span> Read/Write: 4254.08/3661.59
</span></span><span class="line"><span class="cl">Sequential Read/Write: 1294MiB/s / 157MiB/s
</span></span><span class="line"><span class="cl">Mixed Random Read/Write IOPS: 1350/443
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="cleanup">cleanup</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl delete pvc local-hostpath-pvc
</span></span><span class="line"><span class="cl">$ kubectl delete sc openebs-hostpath
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="local-pv-device-practices">Local PV Device Practices</h3>
<p>Compared to <code>Kubernetes</code> local persistent volumes, <code>OpenEBS</code> local <code>PV</code> device volumes have the following advantages :</p>
<ul>
<li><code>OpenEBS</code> local <code>PV</code> device volume <code>provider</code> is dynamic, <code>Kubernetes</code> device volume <code>provider</code> is static.</li>
<li><code>OpenEBS NDM</code> better manages the block devices used to create local <code>pv</code>s. <code>NDM</code> provides the ability to discover block device properties, set device filters, metric collections, and detect if a block device has moved across nodes.</li>
</ul>
<p><strong>Environmental Dependencies :</strong></p>
<ul>
<li><code>k8s 1.12</code> or higher</li>
<li><code>OpenEBS 1.0</code> or higher</li>
</ul>
<p><strong>Practical environment :</strong></p>
<ul>
<li><code>docker 19.03.8</code></li>
<li><code>k8s 1.18.6</code></li>
<li><code>CentOS7</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get node
</span></span><span class="line"><span class="cl">NAME    STATUS   ROLES           AGE     VERSION
</span></span><span class="line"><span class="cl">node1   Ready    master,worker   8m8s    v1.18.6
</span></span><span class="line"><span class="cl">node2   Ready    master,worker   7m15s   v1.18.6
</span></span><span class="line"><span class="cl">node3   Ready    master,worker   7m15s   v1.18.6
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>/dev/sdb</code> on three nodes are stored as block devices.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="o">[</span>root@node1 ~<span class="o">]</span><span class="c1"># lsblk</span>
</span></span><span class="line"><span class="cl">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
</span></span><span class="line"><span class="cl">sda               8:0    <span class="m">0</span>  400G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">sda1            8:1    <span class="m">0</span>    1G  <span class="m">0</span> part /boot
</span></span><span class="line"><span class="cl">sda2            8:2    <span class="m">0</span>  399G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  centos-root 253:0    <span class="m">0</span>  399G  <span class="m">0</span> lvm  /
</span></span><span class="line"><span class="cl">sdb               8:16   <span class="m">0</span>   20G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">sr0              11:0    <span class="m">1</span>  4.4G  <span class="m">0</span> rom
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@node2 ~<span class="o">]</span><span class="c1"># lsblk</span>
</span></span><span class="line"><span class="cl">    NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
</span></span><span class="line"><span class="cl">    sda               8:0    <span class="m">0</span>  400G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">    sda1            8:1    <span class="m">0</span>    1G  <span class="m">0</span> part /boot
</span></span><span class="line"><span class="cl">    sda2            8:2    <span class="m">0</span>  399G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">      centos-root 253:0    <span class="m">0</span>  399G  <span class="m">0</span> lvm  /
</span></span><span class="line"><span class="cl">    sdb               8:16   <span class="m">0</span>   20G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">    sr0              11:0    <span class="m">1</span>  4.4G  <span class="m">0</span> rom
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@node3 ~<span class="o">]</span><span class="c1"># lsblk</span>
</span></span><span class="line"><span class="cl">    NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
</span></span><span class="line"><span class="cl">    sda               8:0    <span class="m">0</span>  400G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">    sda1            8:1    <span class="m">0</span>    1G  <span class="m">0</span> part /boot
</span></span><span class="line"><span class="cl">    sda2            8:2    <span class="m">0</span>  399G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">      centos-root 253:0    <span class="m">0</span>  399G  <span class="m">0</span> lvm  /
</span></span><span class="line"><span class="cl">    sdb               8:16   <span class="m">0</span>   20G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">    sr0              11:0    <span class="m">1</span>  4.4G  <span class="m">0</span> rom
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="create-a-data-directory-1">Create a data directory</h4>
<p>Set up a directory on the node where <code>Local PV Hostpaths</code> will be created. This directory will be called <code>BasePath</code>. The default location is <code>/var/openebs/local</code>.</p>
<p>Nodes <code>node1</code>, <code>node2</code>, <code>node3</code> create the <code>/data/openebs/local</code> directory (/data can be pre-mounted with a data disk, if no additional data disk is mounted, the OS &lsquo;/&rsquo; mount point storage is used).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ mkdir -p /data/openebs/local
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="download-the-application-description-file-1">Download the application description file</h4>
<p><a href="https://openebs.github.io/charts/openebs-operator.yaml">yaml file</a>.</p>
<h4 id="release-the-openebs-application-1">Release the openebs application</h4>
<p>Based on the above configuration file, ensure that the following mirrors are accessible to the <code>k8s</code> cluster (it is recommended to import a local private mirror repository, e.g. : <code>harbor</code>).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">openebs/node-disk-manager:1.5.0
</span></span><span class="line"><span class="cl">openebs/node-disk-operator:1.5.0
</span></span><span class="line"><span class="cl">openebs/provisioner-localpv:2.10.0
</span></span></code></pre></td></tr></table>
</div>
</div><p>Update the mirror <code>tag</code> in <code>openebs-operator.yaml</code> to the actual <code>tag</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">image: openebs/node-disk-manager:1.5.0
</span></span><span class="line"><span class="cl">image: openebs/node-disk-operator:1.5.0
</span></span><span class="line"><span class="cl">image: openebs/provisioner-localpv:2.10.0
</span></span></code></pre></td></tr></table>
</div>
</div><p>Release.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl apply -f openebs-operator.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><p>View Release Status.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get pod -n openebs -w
</span></span><span class="line"><span class="cl">NAME                                           READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">openebs-localpv-provisioner-6d6d9cfc99-4sltp   1/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-85rng                              1/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-operator-7df6668998-ptnlq          0/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-qgqm9                              1/1     Running   <span class="m">0</span>          10s
</span></span><span class="line"><span class="cl">openebs-ndm-zz7ps                              1/1     Running   <span class="m">0</span>          10s
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="create-storage-classes">Create Storage Classes</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="l">$ cat &gt; local-device-sc.yaml &lt;&lt;EOF</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">storage.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StorageClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">local-device</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">openebs.io/cas-type</span><span class="p">:</span><span class="w"> </span><span class="l">local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cas.openebs.io/config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      - name: StorageType
</span></span></span><span class="line"><span class="cl"><span class="sd">        value: device</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l">openebs.io/local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">reclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Delete</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">volumeBindingMode</span><span class="p">:</span><span class="w"> </span><span class="l">WaitForFirstConsumer</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">EOF</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">$ kubectl apply -f local-device-sc.yaml</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="creating-pods-and-pvcs">Creating Pods and PVCs</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="l">$ cat &gt; local-device-pod.yaml &lt;&lt;EOF</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolumeClaim</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">local-device-pvc</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">local-device</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="l">ReadWriteOnce</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">5G</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hello-local-device-pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">local-storage</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l">local-device-pvc</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hello-container</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">       </span>- <span class="l">sh</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">       </span>- -<span class="l">c</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">       </span>- <span class="s1">&#39;while true; do echo &#34;`date` [`hostname`] Hello from OpenEBS Local PV.&#34; &gt;&gt; /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/mnt/store</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">local-storage</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">EOF</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Release.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl apply -f local-device-pod.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><p>View <code>pod</code> status.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl get pod hello-local-device-pod -w
</span></span><span class="line"><span class="cl">NAME                     READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">hello-local-device-pod   1/1     Running   <span class="m">0</span>          9s
</span></span></code></pre></td></tr></table>
</div>
</div><p>Verify that <code>pod</code> is associated with <code>pvc</code> as <code>local-device-pvc</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl describe pod hello-local-device-pod
</span></span><span class="line"><span class="cl">Name:         hello-local-device-pod
</span></span><span class="line"><span class="cl">Namespace:    default
</span></span><span class="line"><span class="cl">Node:         node2/192.168.1.112
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Volumes:
</span></span><span class="line"><span class="cl">  local-storage:
</span></span><span class="line"><span class="cl">    Type:       PersistentVolumeClaim <span class="o">(</span>a reference to a PersistentVolumeClaim in the same namespace<span class="o">)</span>
</span></span><span class="line"><span class="cl">    ClaimName:  local-device-pvc
</span></span><span class="line"><span class="cl">    ReadOnly:   <span class="nb">false</span>
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><p>Observe that the scheduled node is <code>node2</code> and confirm that the <code>node2</code> node <code>/dev/sdb</code> is in use.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="o">[</span>root@node2 ~<span class="o">]</span><span class="c1"># lsblk</span>
</span></span><span class="line"><span class="cl">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
</span></span><span class="line"><span class="cl">sda               8:0    <span class="m">0</span>  400G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">sda1            8:1    <span class="m">0</span>    1G  <span class="m">0</span> part /boot
</span></span><span class="line"><span class="cl">sda2            8:2    <span class="m">0</span>  399G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  centos-root 253:0    <span class="m">0</span>  399G  <span class="m">0</span> lvm  /
</span></span><span class="line"><span class="cl">sdb               8:16   <span class="m">0</span>   20G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">sr0              11:0    <span class="m">1</span>  4.4G  <span class="m">0</span> rom
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@node2 ~<span class="o">]</span><span class="c1"># lsblk</span>
</span></span><span class="line"><span class="cl">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
</span></span><span class="line"><span class="cl">sda      8:0    <span class="m">0</span>  400G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">sda1   8:1    <span class="m">0</span>    1G  <span class="m">0</span> part /boot
</span></span><span class="line"><span class="cl">sda2   8:2    <span class="m">0</span>  399G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  centos-root
</span></span><span class="line"><span class="cl">       253:0    <span class="m">0</span>  399G  <span class="m">0</span> lvm  /
</span></span><span class="line"><span class="cl">sdb      8:16   <span class="m">0</span>   20G  <span class="m">0</span> disk /var/lib/kubelet/pods/266b7b14-5eb7-40ec-bccb-3ac189acf939/volumes/kubernetes.io~local-volume/pvc-9bd89019-13dc-4
</span></span><span class="line"><span class="cl">sr0     11:0    <span class="m">1</span>  4.4G  <span class="m">0</span> rom
</span></span></code></pre></td></tr></table>
</div>
</div><p>It does get used, and therein lies the power of <code>OpenEBS</code>, in its ultimate simplicity. As we discussed above, <code>NDM</code> is responsible for discovering block devices and filtering out devices that should not be used by <code>OpenEBS</code>, for example, detecting disks with <code>OS</code> file systems.</p>
<h4 id="benchmarking-1">Benchmarking</h4>
<p>Create a benchmark test <code>pvc</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ cat &gt; dbench-pvc.yaml <span class="s">&lt;&lt;EOF
</span></span></span><span class="line"><span class="cl"><span class="s">---
</span></span></span><span class="line"><span class="cl"><span class="s">kind: PersistentVolumeClaim
</span></span></span><span class="line"><span class="cl"><span class="s">apiVersion: v1
</span></span></span><span class="line"><span class="cl"><span class="s">metadata:
</span></span></span><span class="line"><span class="cl"><span class="s">  name: dbench
</span></span></span><span class="line"><span class="cl"><span class="s">spec:
</span></span></span><span class="line"><span class="cl"><span class="s">  storageClassName: local-device
</span></span></span><span class="line"><span class="cl"><span class="s">  accessModes:
</span></span></span><span class="line"><span class="cl"><span class="s">    - ReadWriteOnce
</span></span></span><span class="line"><span class="cl"><span class="s">  resources:
</span></span></span><span class="line"><span class="cl"><span class="s">    requests:
</span></span></span><span class="line"><span class="cl"><span class="s">      storage: 5G
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://github.com/openebs/performance-benchmark/blob/master/fio-benchmarks/fio-deploy.yaml">Download the Benchmark Job declaration file</a>.</p>
<p>Adjust the following content.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">openebs/perf-test:latest</span><span class="w"> </span><span class="c"># tag    </span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Release Run.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="l">$ kubectl create -f dbench-pvc.yaml</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">$ kubectl create -f fio-deploy.yaml </span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Check the running status.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">Check the running status
</span></span></code></pre></td></tr></table>
</div>
</div><p>View Benchmarking Results.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ kubectl logs -f dbench-vqk68-f9877
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">All tests complete.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">==================</span>
</span></span><span class="line"><span class="cl"><span class="o">=</span> Dbench <span class="nv">Summary</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl"><span class="o">==================</span>
</span></span><span class="line"><span class="cl">Random Read/Write IOPS: 3482/6450. BW: 336MiB/s / 1017MiB/s
</span></span><span class="line"><span class="cl">Average Latency <span class="o">(</span>usec<span class="o">)</span> Read/Write: 2305.77/1508.63
</span></span><span class="line"><span class="cl">Sequential Read/Write: 6683MiB/s / 2312MiB/s
</span></span><span class="line"><span class="cl">Mixed Random Read/Write IOPS: 3496/1171
</span></span></code></pre></td></tr></table>
</div>
</div><p>From the results, the performance is doubled compared to <code>Local PV HostPath</code> mode.</p>
<h3 id="summary-1">Summary</h3>
<p>Throughout the testing and validation process, I got the impression that <code>OpenEBS</code> is very simple to use, especially with the deployment of the <code>Local PV</code> engine.</p>
<p>However, there are some shortcomings in <code>OpenEBS</code> at this stage.</p>
<ul>
<li><code>cStor</code> and <code>Jiva</code> have more components on the data surface, and the configuration is more cumbersome (the first feeling is that there are too many conceptual components).</li>
<li>Some components of <code>cStor</code> and <code>Jiva</code> rely on internally defined image <code>tag</code> for creation, which cannot be adjusted to private library <code>tag</code> in offline environment, resulting in components not running successfully.</li>
<li>Single storage type, multiple engines only support block storage type, does not support native multi-node read and write (need to combine with <code>NFS</code> implementation), compared with <code>ceph</code> and other slightly inferior.</li>
</ul>
<p>We recommend using <code>OpenEBS</code> as backend storage for the following scenarios.</p>
<ul>
<li>Single-computer test environments.</li>
<li>Multi-computer experimental/demo environments.</li>
</ul>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/openebs/">OpenEBS</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2022-05/packer-vsphere-example/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Building Virtual Machine Images with Packer</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2022-05/benchmark-comparative-analysis-tool/">
            <span class="next-text nav-default">Benchmark Comparative Analysis Tool</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>








</body>
</html>
