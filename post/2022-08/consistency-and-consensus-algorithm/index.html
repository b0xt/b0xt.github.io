<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Consistency Problems and Consensus Algorithms for Distributed Databases - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="Learn about consistency problems and consensus algorithms for distributed databases." /><meta name="keywords" content="Consistency, Consensus" />






<meta name="generator" content="Hugo 0.101.0 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2022-08/consistency-and-consensus-algorithm/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">


<meta property="og:title" content="Consistency Problems and Consensus Algorithms for Distributed Databases" />
<meta property="og:description" content="Learn about consistency problems and consensus algorithms for distributed databases." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2022-08/consistency-and-consensus-algorithm/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-08-08T10:25:57+08:00" />
<meta property="article:modified_time" content="2022-08-08T10:25:57+08:00" />

<meta itemprop="name" content="Consistency Problems and Consensus Algorithms for Distributed Databases">
<meta itemprop="description" content="Learn about consistency problems and consensus algorithms for distributed databases."><meta itemprop="datePublished" content="2022-08-08T10:25:57+08:00" />
<meta itemprop="dateModified" content="2022-08-08T10:25:57+08:00" />
<meta itemprop="wordCount" content="6651">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Consistency Problems and Consensus Algorithms for Distributed Databases"/>
<meta name="twitter:description" content="Learn about consistency problems and consensus algorithms for distributed databases."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SOBYTE</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SOBYTE</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Consistency Problems and Consensus Algorithms for Distributed Databases</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-08-08 10:25:57 </span>
        <div class="post-category">
            <a href="/categories/tutorials/"> tutorials </a>
            </div>
          <span class="more-meta"> 6651 words </span>
          <span class="more-meta"> 32 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#i-consistency">I. Consistency</a>
          <ul>
            <li><a href="#1-transactions-consistency">1. Transactions Consistency</a></li>
            <li><a href="#2-data-consistency">2. Data Consistency</a></li>
          </ul>
        </li>
        <li><a href="#ii-base-and-final-consistency-of-distributed-systems">II. BASE and Final Consistency of Distributed Systems</a></li>
        <li><a href="#iii-consensus-algorithm">III. Consensus Algorithm</a>
          <ul>
            <li><a href="#the-byzantine-general-problem-and-byzantine-fault-tolerance">The Byzantine General Problem and Byzantine Fault Tolerance</a></li>
            <li><a href="#commonly-used-consensus-algorithms">Commonly used consensus algorithms</a></li>
            <li><a href="#application-scenarios-for-different-consensus-algorithms">Application Scenarios for Different Consensus Algorithms</a></li>
            <li><a href="#non-byzantine-error-consensus-algorithms-paxos-and-raft">Non-Byzantine error consensus algorithms Paxos and Raft</a></li>
            <li><a href="#pow-probabilistic-consensus-algorithm-that-tolerates-byzantine-errors">PoW probabilistic consensus algorithm that tolerates Byzantine errors</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>When it comes to distributed databases, whether they are centralized databases like Etcd/Zookeeper or decentralized databases like Ethereum blockchain, there are two keywords that cannot be avoided: &ldquo;<strong>consistency</strong>&rdquo; and &ldquo;<strong>consensus</strong>&rdquo;.</p>
<p>This article is the notes recorded by me when learning &ldquo;<strong>consistency</strong>&rdquo; and &ldquo;<strong>consensus</strong>&rdquo; and related theoretical knowledge, which can help us understand the difference between Etcd/Zookeeper/Consul/MySQL/PostgreSQL/DynamoDB/Cassandra/MongoDB/CockroachDB/ TiDB, etc., understand the advantages and limitations of each database, understand the meaning of database isolation level and how it should be set, and enable us to choose the applicable database in various application scenarios.</p>
<p>If you are interested in blockchain, this article can also help you understand how decentralized databases like blockchain differ from the popular distributed databases in the industry in terms of technology, what they have in common, and how they are implemented.</p>
<h2 id="i-consistency">I. Consistency</h2>
<p>&ldquo;Consistency&rdquo; itself is a rather vague definition, with many different meanings depending on the usage scenario. Since databases are still an emerging field, there are many different models of consistency, and some of these terms may describe overlapping relationships between consistency, relationships that may even bother professional database developers.</p>
<p>But at the root, when we talk about consistency, we are actually talking about <strong>transactional consistency</strong> and <strong>data consistency</strong>, and we describe each of these two types of consistency below.</p>
<h3 id="1-transactions-consistency">1. Transactions Consistency</h3>
<p>&ldquo;Transactions Consistency&rdquo; refers to the consistency of transactions in a database, which is the least important feature of ACID theory and is not the focus of this article. But it is not enough to write such a sentence here, so here is a closer look at transactions and ACID theory.</p>
<h4 id="transactions-and-acid-theory">Transactions and ACID Theory</h4>
<p>A transaction is an &ldquo;All or Nothing&rdquo; mechanism for running instructions.</p>
<p><strong>ACID Theory</strong> defines a &ldquo;sequence of database instructions&rdquo; as a transaction if it has the following four characteristics.</p>
<ul>
<li><strong>Atomicity</strong>: A transaction is an indivisible unit of work in which all operations are either completed or not completed, and cannot be stalled in some intermediate state.
<ul>
<li>For example, if A transfers $100 to B, either the transfer fails or the transfer succeeds; it cannot be stuck in an intermediate state where A has been deducted $100 and B has not received $100.</li>
<li>Atomicity has been well addressed on standalone databases, but it becomes a new challenge on distributed databases. It is not easy to support atomicity in a distributed architecture, so many NoSQL products have chosen to bypass this problem and focus on niche scenarios that are not sensitive to atomicity.
<strong>Consistency</strong>: also known as data &ldquo;<strong>correctness</strong>&rdquo; or integrity, means that changes to database state by a transaction must satisfy all predefined rules, including &ldquo;constraints&rdquo;, &ldquo;cascades&rdquo;, &ldquo;triggers&rdquo;, and any combination of these rules. For example</li>
<li>For example, if a user sets a constraint <code>unique</code> on a field, then all changes to that table by the transaction must ensure that this constraint holds, or it will fail.</li>
<li>It is the characteristic with the lowest presence</li>
</ul>
</li>
<li><strong>Isolation</strong>: <strong>Multiple transactions executing concurrently are completely isolated from each other</strong>, and they execute exactly as if they were executed serially in the order in which the transactions started.
<ul>
<li>The most complex features of a transaction</li>
</ul>
</li>
<li><strong>Durability</strong>: After a transaction is executed, the result is preserved. This is best understood.</li>
</ul>
<p>ACID is a core feature of traditional standalone databases, such as MySQL/PostgreSQL.</p>
<h4 id="the-most-complex-feature-in-acid---isolation">The most complex feature in ACID - Isolation</h4>
<p>A full implementation of ACID yields a database with very poor performance. Therefore, in relational databases, designers usually choose to sacrifice the relatively unimportant &ldquo;isolation&rdquo; to get better performance.</p>
<p>And once the isolation is not thorough enough, you may encounter some exceptions where transactions affect each other, which are classified as follows.</p>
<ul>
<li><strong>Dirty writes</strong>: i.e., transaction T1 and transaction T2 update the same data on top of the original data at the same time, resulting in a result that does not meet expectations.
<ul>
<li>Example: Two transactions try to debit $1000 from the account at the same time, but the initial state they read is $5000, so they both try to modify the account to $4000, and the result is $1000 less.</li>
<li>The simplest solution: For <code>UPDATE table SET field = field - 1000 WHERE id = 1</code>, you need to add a &ldquo;row write lock&rdquo; to the row being updated, so that other transactions that need to write this data wait.</li>
</ul>
</li>
<li><strong>Dirty reads</strong>: Transaction T1 reads data that was not committed by transaction T2. This data is not necessarily accurate and is called dirty data because if transaction T2 rolls back, T1 will get an incorrect data.
<ul>
<li>Case: Suppose Xiao Ming Xiao Hong has deposited 5000 Yuan in a bank account, and Xiao Ming Xiao Hong is using the same account to spend 1000 Yuan, in the middle of which Xiao Ming&rsquo;s payment transaction reads that the account has been modified to 4000 Yuan by Xiao Hong&rsquo;s transaction, so it modifies the balance to 3000 Yuan, and then the payment succeeds. However, after Xiao Ming&rsquo;s payment transaction succeeds, Xiao Hong&rsquo;s payment failure is rolled back and the balance is modified from 3000 to 5000. Xiao Ming then accomplishes the feat of $0 purchase.</li>
<li>The simplest solution: add a &ldquo;row write lock&rdquo; to the modified row when transaction T2 writes data, and then release the lock after T2 finishes, so that the read of transaction T1 will be blocked until the lock is released.</li>
</ul>
</li>
<li><strong>Non-repeatable reads</strong>: After transaction T1 reads the data, transaction T2 updates and commits the data immediately afterwards.
<ul>
<li>Case.
<ul>
<li>Xiao Ming buys a product on Jingdong, and when the transaction starts, it reads that there are 36 products left, so it continues to execute the logic of buying.</li>
<li>If you first read the balance by <code>SELECT field INTO myvar FROM mytable WHERE uid = 1</code> in the transaction, and then update the balance by <code>UPDATE</code> on this basis, it is likely that the data will become A mess!
<ul>
<li>The correct way is to use <code>UPDATE mytable SET field = field - 1000 WHERE id = 1</code>, because each SQL command itself is atomic and this SQL will not be a problem.</li>
</ul>
</li>
</ul>
</li>
<li>The simplest solution: when transaction T1 reads the data, it also puts a &ldquo;row&rdquo; lock on it until it no longer needs to read the data, and then releases the lock.</li>
</ul>
</li>
<li><strong>Phantom reads</strong>: When transaction T1 reads data in bulk several times, transaction T2 performs insert/delete operations into it, resulting in T1 reading a remnant of the old data instead of the current real data state.
<ul>
<li>The simplest solution: Transaction T1 puts a range lock on the bulk read, and then releases the lock after Transaction T1 has finished reading. This solves both the &ldquo;phantom read&rdquo; and &ldquo;non-repeatable read&rdquo; problems.</li>
</ul>
</li>
</ul>
<p>According to the degree of isolation, the ANSI SQL-92 standard subdivides &ldquo;isolation&rdquo; into four levels (avoiding &ldquo;dirty writes&rdquo; is a mandatory requirement for databases, so it is not recorded in the following four levels)</p>
<ul>
<li><strong>Serializable Serializable</strong>: that is, complete isolation, forcing transactions to execute serially (through a locking mechanism) whenever there is a possibility of them affecting each other.</li>
<li><strong>Repeatable read Repeatable read</strong>: avoids dirty reads and non-repeatable reads, but does not solve the problem of phantom reads.</li>
<li><strong>Read committed</strong>: only avoids dirty reads</li>
<li><strong>Read uncommitted</strong>: the lowest level, completely abandoning isolation</li>
</ul>
<p>The default isolation level of MySQL is &ldquo;Repeatable Read&rdquo;, and the default isolation level of PostgreSQL and Oracle is &ldquo;Read committed&rdquo;.</p>
<p>Why is the default isolation level of MySQL/PostgreSQL/Oracle set in this way? How to choose the correct isolation level? Let&rsquo;s do a simple analysis for a common high concurrency business scenario.</p>
<ul>
<li>First of all, &ldquo;dirty read&rdquo; must be avoided, it will make the transaction read the wrong data! The lowest &ldquo;read uncommitted&rdquo; level is directly excluded.</li>
<li>The lowest level of &ldquo;read uncommitted&rdquo; is directly excluded.</li>
<li>As long as SQL is used correctly, the &ldquo;non-repeatable read&rdquo; problem usually has no impact on the correctness of the business logic, so it can be tolerated.</li>
<li><strong>So &ldquo;read-committed&rdquo; is generally the best isolation level</strong>, which is why PostgreSQL/Oracle set it as the default isolation level.</li>
<li>So why is MySQL such a maverick, raising the default isolation level to &ldquo;repeatable reads&rdquo;? Why would a big Internet company like Ali change MySQL&rsquo;s default isolation level to &ldquo;Read Committed&rdquo;?
<ul>
<li>According to the information I found on the Internet, this is the result of MySQL&rsquo;s history. MySQL before 5.0 only supports statement binlog format, which has many problems under the &ldquo;read committed&rdquo; isolation level, the most obvious one is that it may lead to inconsistent data between master and slave databases.</li>
<li>In addition to setting the default isolation level, MySQL also prohibits the use of READ COMMITTED as the transaction isolation level when using binlogs in statement format, and attempts to modify the isolation level will result in the error <code>Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT'</code></li>
<li>The reason why Internet companies change the isolation level to &ldquo;READ COMMITTED&rdquo; is also very understandable, of course, to improve performance, the lower the isolation level, the higher the concurrency performance.</li>
</ul>
</li>
</ul>
<p>The essence of &ldquo;isolation&rdquo; is actually <strong>concurrency control</strong> of transactions, different isolation levels represent the degree of isolation of concurrent transactions, the main means of implementation is &ldquo;<strong>multiple version concurrency control MVCC</strong>&rdquo; and &ldquo;locking&rdquo;. The locking mechanism has been briefly introduced earlier, and MVCC actually creates a snapshot for each transaction with a specific isolation level, so that reads and writes will not block each other and performance is improved.</p>
<p>The analysis of anomalies in ANSI SQL-92 is still too simple, and the newly released 1995 paper <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">A Critique of ANSI SQL Isolation Levels</a> enriches and refines SQL-92, defining six isolation levels and eight anomalies, and it is highly recommended to read through this paper. Of these, we are most concerned with the Snapshot Isolation (SI) level.</p>
<h3 id="2-data-consistency">2. Data Consistency</h3>
<p>&ldquo;Data Consistency&rdquo; means that every read operation to the database should either read the latest data written, or simply report an error.</p>
<p>Data Consistency&quot; is often not a problem for standalone databases, because there is usually only one copy of the data stored on disk or in memory. However, in distributed systems, each copy of data is often stored on multiple nodes for data security, which raises the issue of consistency among data copies. Therefore, when we talk about &ldquo;data consistency&rdquo;, we usually mean &ldquo;data consistency&rdquo; of distributed systems.</p>
<p>The <strong>CAP Principle</strong> is a well-known theory in the field of distributed systems, which tells us that it is impossible to achieve all three properties in distributed systems, hence the name &ldquo;<strong>CAP Impossibility Triangle</strong>&rdquo;.</p>
<ul>
<li><strong>Data Consistency</strong>: Every read operation by a client, no matter which node of the system it accesses, either reads the same copy of the latest data written or fails
<ul>
<li>Emphasizes that the data is exactly correct</li>
</ul>
</li>
<li><strong>Availability</strong>: Any request from the client, regardless of which non-faulty node is accessed, will get the response data, but it is not guaranteed to be the same latest data
<ul>
<li>Emphasis on service availability, but no guarantee of correct data</li>
</ul>
</li>
<li><strong>Partition Tolerance</strong>: The system can operate normally even if there is an arbitrary number of lost messages or high latency between nodes
<ul>
<li>That is, network packet loss or latency can cause the system to be divided into multiple Partitions, and the system can tolerate this situation</li>
</ul>
</li>
</ul>
<p>To ensure partition fault tolerance P, consider that when a distributed system is fragmented into multiple partitions due to network problems, each partition has only two choices, A and C, and one of them must be sacrificed.</p>
<ul>
<li>cancel the operation and deny the service, which reduces availability but ensures data consistency</li>
<li>Continue processing requests, which ensures availability, but data consistency is not guaranteed</li>
</ul>
<p>If multiple partitions of a system are being served at the same time, resulting in inconsistent and conflicting data that cannot be merged, this is known as a &ldquo;<strong>brain fracture</strong>&rdquo; in a distributed system, and obviously no distributed system would want a &ldquo;brain fracture&rdquo; to occur.</p>
<p>Because a distributed system is different from a stand-alone system, it involves network communication and interaction between multiple nodes, but as long as there is network interaction, there will definitely be delays and data loss, and partitioning failures between nodes are very likely to occur. Therefore, for proper operation, P is a feature that must be guaranteed for distributed systems<strong>In case of partition failure, only A or C</strong> can be sacrificed for P.</p>
<p>Whether to engineer AP or CP depends on the situation:</p>
<ul>
<li>Etcd/Zookeeper/Consul: They are typically used to store critical meta-information about the operation of the system, and each time they are read, they have to be able to read the latest data. Therefore they implement CP at the expense of A</li>
<li>DynamoDB/Cassandra/MongoDB: data consistency is not required and it is not a big problem to use the old cache for some time, but availability is required, so they should implement AP and sacrifice C</li>
</ul>
<h4 id="data-consistency-model">Data Consistency Model</h4>
<p>A set of read and write policies on multi-copy data in distributed systems is called &ldquo;Consistency Model (of data)&rdquo;. There are so many consistency models that it is hard to distinguish them. To facilitate understanding, we first distinguish the concept of strong consistency and weak consistency from a state perspective, and then understand these many consistency models from an operational perspective.</p>
<h5 id="1-state-perspective---strong-consistency-and-weak-consistency">1. State Perspective - Strong Consistency and Weak Consistency</h5>
<p>We first consider the whole distributed system as a <strong>white box</strong>. From the state perspective, after any change operation, there are only three states of multiple data replicas of the distributed system as follows.</p>
<ul>
<li>Under certain conditions, the inconsistent states across replicas are temporary and will also transition to a consistent state, which is referred to as &ldquo;<strong>weakly consistent</strong>&rdquo;.
<ul>
<li>This is usually done using <strong>asynchronous replication</strong> to synchronize the states of the replicas.</li>
</ul>
</li>
<li>In contrast, if there is no such state as &ldquo;inconsistent&rdquo; across replicas of the system, and the data must be identical as long as the change operation succeeds, then it is called &ldquo;<strong>Strongly Consistent</strong>&rdquo;.
<ul>
<li>This requires that data updates between all replicas must be fully synchronized, and <strong>fully synchronized replication</strong> must be used.</li>
</ul>
</li>
<li><strong>Never Consistent</strong>: This is a bug in distributed systems and is also known as &ldquo;brain cracking&rdquo;.</li>
</ul>
<p>The above describes the objective, actual state of the entire system, but for the vast majority of users distributed systems are more of a <strong>black box</strong>, so the more popular classification is based on a &lsquo;black box&rsquo; approach, which classifies systems into two types based on their external state.</p>
<ul>
<li><strong>Strong Consistency</strong>: means that for any node/process of the system, after the write operation is completed, any subsequent access to any node by any user will read the new value. It is as if only one copy of the system exists.
<ul>
<li>The most commonly used algorithms are Raft/Paxos, whose write operations only require more than half of the nodes to be written successfully, so the internal state is actually inconsistent when the write completes, but the effect of reading and writing to it is no different from &ldquo;fully synchronous replication&rdquo;.</li>
</ul>
</li>
<li><strong>Weakly Consistent</strong>: means that for any node/process of the system, after the write operation completes, the value that any subsequent access may get is uncertain, but after some time, any subsequent access reads the new value.
<ul>
<li>Weak Consistency is very vaguely defined. If we refer to the fact that eventually all users can access the new value as &ldquo;<strong>system convergence</strong>&rdquo;, the time used for system convergence can be well-bounded or not. The access behavior before system convergence can have explicit specification or no specification. It all depends on the implementation of the specific system.</li>
<li>If the system can converge in finite time, then it is &ldquo;<strong>finally consistent</strong>&rdquo;, otherwise it can be considered as &ldquo;<strong>inconsistent</strong>&rdquo;.</li>
</ul>
</li>
</ul>
<p>For practical purposes, database experts have obtained many consistency models by imposing various restrictions on the effect of reading and writing before the system converges and various restrictions on the convergence time of the system.</p>
<h5 id="2-operational-perspective---multiple-consistency-models">2. Operational Perspective - Multiple Consistency Models</h5>
<p>From the operational perspective of each client, there are four consistency models.</p>
<ul>
<li><strong>Read after Write Consistency</strong>: Also known as &ldquo;Read after Write Consistency&rdquo;, i.e., after you write version N of the data, the version you subsequently read must not be smaller than version N.
<ul>
<li>The problem it solves: A posts a shaky video, but it somehow disappears after refreshing the page (the old version), only to be refreshed a few minutes later.</li>
<li>One way to implement this: add a separate read rule for the writer, and all his reads are handled by the copy that has updated its write data.</li>
</ul>
</li>
<li><strong>Monotonic Read Consistency</strong>: guarantees the order of multiple read operations, i.e. once a client reads a certain version N of data, it will not subsequently read a version lower than N.
<ul>
<li>It solves the problem that A deletes a Jitterbug video, which can be refreshed multiple times, occasionally failing to refresh the video, and occasionally refreshing the deleted video (the old version), only to be completely deleted a few minutes later.</li>
<li>One way to implement this: Create a replica mapping for each user&rsquo;s read, and subsequent reads are handled by a fixed replica to avoid randomly switching replicas and reading older values.</li>
</ul>
</li>
<li><strong>Monotonic Write Consistency</strong>: guarantees the order of multiple write operations, i.e., two write operations to the same data by the client must be executed in the order they were committed.</li>
<li><strong>Read After Write Consistency Write after Read Consistency</strong>: Read after write consistency guarantees that after a client reads version N of data (which may have been written by another client), subsequent write operations to the same data must be executed on the copy with version number greater than or equal to N.</li>
</ul>
<p>The above four consistency models only define rules from the perspective of each client, which is rather one-sided, so they are all &ldquo;weak consistency models&rdquo;.</p>
<p>Without considering the clients, and directly from the perspective of all database users&rsquo; operations, there are several consistency models as follows.</p>
<ul>
<li>
<p><strong>Linearizability</strong>: Linearizability makes use of the commit order of events, it guarantees that the order of data obtained by any read operation is the same as the commit order of read/write events.</p>
<ul>
<li>Simply put it requires that <strong>the entire system behave as if only one copy exists</strong> and that all operations are executed as if those events were executed exactly serially in the order they were committed. This is effectively saying that all concurrent events are atomic and must be executed sequentially once they conflict with each other, hence why some call it &ldquo;atomic consistency&rdquo;.</li>
<li>Linear consistency, which is exactly equivalent to the &ldquo;strong consistency&rdquo; of the external state of the system</li>
<li>A linearly consistent system is fully deterministic</li>
<li>Implementation: requires a &ldquo;<strong>global clock</strong>&rdquo; that is consistent across all nodes so that all events can be globally ordered.
<ul>
<li>Most distributed databases like TiDB/Etcd have a global clock implemented through single point timing and synchronization via protocols like NTP.</li>
<li>Google Spanner, which has global deployment needs, is a global clock TrueTime using GPS + atomic clocks, and the global error can be controlled within 7ms.</li>
</ul>
</li>
<li>Limitations: According to Einstein&rsquo;s theory of relativity, &ldquo;time is relative&rdquo;, there is no absolute time, so linear consistency is only applicable within the scope of classical physics.</li>
</ul>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/Consistency_model">Sequentially Consistent</a></strong>: Sequential consistency was first used by Leslie Lamport to describe the behavior of multicore CPUs, and is less used in the distributed systems space. less used in the distributed systems domain.</p>
<ul>
<li>The requirements for sequential consistency are twofold.
<ul>
<li>From the perspective of a single process (replica), the order of execution of all instructions is identical to the order of the code logic.</li>
<li>From the perspective of all processors (the entire distributed system), write operations need not be immediately visible to all users, but all replicas must receive these write operations in the same order.</li>
</ul>
</li>
<li>Both sequential consistency and linear consistency are about finding a set of operation histories that satisfy &ldquo;write followed by read,&rdquo; the difference being that <strong>linear consistency requires a strict temporal order, while sequential consistency only requires that the logical order of the code be satisfied</strong> and that the order of events not defined by other code logic (such as the order between events on multiple replicas), it doesn&rsquo;t matter what the exact order is, as long as all replicas see It doesn&rsquo;t matter what the exact order is, as long as all copies see the same order of events.</li>
<li>Order consistency does not provide &ldquo;certainty&rdquo;, as the same two operations may still result in different event orders.</li>
<li>Implementation: Since strict global time order is not required, it does not require a global clock, but in practice some complex operations are still needed to satisfy global determinism.</li>
</ul>
</li>
<li>
<p><strong>Causal Consistency</strong>: While the linearly consistent global clock has its limitations, Causal Consistency proposes the concept of &ldquo;<strong>logical clock</strong>&rdquo; based on the &ldquo;partial order relationship&rdquo; of write events, and guarantees that the read order is consistent with the order of write events on the logical clock.</p>
<ul>
<li>The &ldquo;off-order relationship&rdquo; relationship of write events means that at least some of the events (e.g., events within a node) are directly orderable using the local clock, and that when communication occurs between nodes, the events of the receiver must be later than the events of the caller. Based on this a &ldquo;<strong>logical clock</strong>&rdquo; can be implemented, but the disadvantage of a logical clock is that if a certain two events are not correlated, then the order given by the logical clock has no meaning.</li>
<li>Most argue that causal consistency is weaker than linear consistency, but <strong>has advantages in concurrency performance and is also sufficient to handle most anomalies</strong>, so causal consistency is also used in industry.</li>
<li>Both CockroachDB and YugabyteDB use Hybrid Logical Clocks in their designs, a scheme derived from Lamport&rsquo;s logical clocks, which has also achieved good results</li>
</ul>
</li>
<li>
<p><strong>Consistent Prefix</strong>: During synchronization between replicas, there will be some replicas that do not receive data in the same order. &ldquo;Consistent Prefix&rdquo; means that the prefix of the data order read by all users is always the same.</p>
<ul>
<li>&ldquo;Prefix&rdquo; means that the program needs to explicitly declare its &ldquo;prefix&rdquo; events when performing write operations, so that each event has a prefix that is arranged by other write events. For example, if there is currently a write event arrangement &ldquo;A B C D&rdquo;, then all data read by the user will have the same write event prefix, such as &ldquo;A&rdquo;, &ldquo;A B&rdquo;, &ldquo;A B C&rdquo;, &ldquo;A B C D&rdquo;, but no result such as &ldquo;A C&rdquo; or &ldquo;C A&rdquo; is possible.</li>
<li>It solves the consistency problem of <strong>partitioned distributed database</strong>: A B C reads and writes different copies because of the geographical difference, B asks a question in the comment section of Jitterbug, and then A makes an answer. However, if the question and answer data are in different slices, the order of the two data cannot be guaranteed when the copy is synchronized, and C may read the answer information first before refreshing B&rsquo;s question, and the order of historical events is messed up.</li>
<li>Implementation: The program needs to actively add explicit dependencies between messages, and then control their reading order accordingly, which is more complicated to implement.</li>
<li>Problem: The order between events can only be guaranteed if they are explicitly defined with cause-and-effect relationships.</li>
</ul>
</li>
</ul>
<p>Among them <strong>Linear Consistency</strong> is <strong>Strong Consistency</strong> and all other models are <strong>Weak Consistency Models</strong> or <strong>Final Consistency Models</strong>. All these models are listed in descending order of strength as follows.</p>
<ul>
<li>Linear consistency/strong consistency: the system behaves externally as if the whole system is perfectly consistent and there are no inconsistencies.</li>
<li>Sequential Consistency: only the order of events on each node is guaranteed to be consistent, with only very loose requirements on the order of events between nodes.</li>
<li>Causal Consistency: Again, only the order of events on each node is guaranteed to be consistent, but the requirements for the order of events between nodes are more lenient than sequential consistency.</li>
<li>Bounded Staleness: Ensures that the read data is no more than K versions away from the latest version.</li>
<li>Session Consistency: Monotonic reads, monotonic writes, and reads written by itself are guaranteed within a session, not between sessions.</li>
<li>Prefix Consistency: monotonic reads are guaranteed within each session, but not between sessions</li>
<li>Four consistency models from the client&rsquo;s perspective: read after write, monotonic read, monotonic write, and read after write. All four models have a very one-sided perspective and are usually included in the aforementioned consistency models.</li>
</ul>
<p>A more complete relational tree diagram: <a href="https://jepsen.io/consistency">Consistency Models</a></p>
<h2 id="ii-base-and-final-consistency-of-distributed-systems">II. BASE and Final Consistency of Distributed Systems</h2>
<p>BASE theory.</p>
<ul>
<li><strong>Basically Available</strong>: When a distributed system has unpredictable failures, the availability of some functions is allowed to be lost to guarantee the availability of core functions
<ul>
<li>Four means of achieving basic availability: traffic clipping, delayed response, experience degradation, and overload protection</li>
</ul>
</li>
<li><strong>Soft state</strong>: In flexible transactions, allow the system to have an intermediate state, and this intermediate state will not affect the overall availability of the system. For example, if the database reads and writes are separated, there is a delay from the write library to the read library (master to slave), which is actually a flexible state.</li>
<li><strong>Eventually consistent</strong>: It has been described in detail earlier, it means that for any node/process of the system, after the write operation is completed, the value that any subsequent access may get is uncertain, but after a limited period of time, any subsequent access will be able to read the new value.</li>
</ul>
<p>ACID and BASE are essentially two extremes in the implementation of distributed systems.</p>
<ul>
<li>ACID theory is, as it means &ldquo;<strong>acid</strong>&rdquo;, the consistency boundary of the CAP principle - <strong>the strongest consistency</strong>, the extreme of achieving CP at the expense of A.</li>
<li>BASE translates to &ldquo;<strong>base</strong>&rdquo;, which is the boundary of availability in the CAP principle - <strong>the highest availability, the weakest consistency</strong>, and the extreme of AP by sacrificing C.</li>
</ul>
<p>According to CAP theory, if consistency is achieved in a distributed system, availability is bound to be affected. For example, if a node fails, the execution of the entire distributed transaction fails. In fact, most scenarios do not require that much consistency, and transient inconsistencies are acceptable. In addition, also based on availability and concurrency performance considerations, it is recommended that in developing and implementing distributed systems,<strong>if not necessary, try not to implement transactions, and consider using final consistency</strong>.</p>
<p>Means of implementation of ultimate consistency.</p>
<ul>
<li><strong>Read-time repair</strong>: detecting data inconsistencies when reading data and repairing them</li>
<li><strong>Write-time repair</strong>: detects data inconsistencies and repairs them when writing data</li>
<li><strong>Asynchronous repair</strong>: this is the most common way to detect the consistency of the replica data and repair it by timing the reconciliation</li>
</ul>
<p>When implementing final consistency, it is also recommended to also implement custom write consistency levels (e.g. All, Quorum, One, Any), which are adjustable for many distributed databases.</p>
<p>But with the rise of distributed relational databases such as TiDB, the BASE theory in the distributed space is actually being overtaken by ACID, which is taking on a new lease of life.</p>
<h2 id="iii-consensus-algorithm">III. Consensus Algorithm</h2>
<p>Consensus algorithms, also known as consistency protocols, are a set of processes for reaching agreement among multiple nodes in a distributed system on a proposal Proposal (e.g., multiple transaction requests, who to execute first?). A set of processes to reach a consensus view.</p>
<p>The meaning of proposal is very broad in distributed systems, such as the order in which multiple events occur, the value corresponding to a certain key, who is the master node &hellip;&hellip; and so on. Any information that can be agreed upon can be considered as a proposal.</p>
<p>For distributed systems, each node is usually the same deterministic state machine model (also known as State-Machine Replication problem), and receiving the same sequence of instructions starting from the same initial state guarantees the same resultant state. Therefore, the most critical thing for multiple nodes in the system is the consensus on the order of multiple events, i.e., ordering.</p>
<p><strong>Consensus algorithms are a means to reach data consistency and are a necessary non-sufficient condition for strong data consistency</strong>. For example, using the Raft algorithm directly, but allowing to read any node of the cluster, only yields the final consistency of the data, and other means are needed to ensure strong consistency.</p>
<h3 id="the-byzantine-general-problem-and-byzantine-fault-tolerance">The Byzantine General Problem and Byzantine Fault Tolerance</h3>
<p>Byzantine error is an error model proposed by Lambert in 1982 in &ldquo;The Byzantine General Problem&rdquo;, describing the problem of whether consensus can be reached in a scenario where a few nodes are not only faulty, but also malicious, as described in the paper as follows.</p>
<blockquote>
<p>Nine Byzantine generals lead an army to besiege a city together, because the city is very powerful, if they do not coordinate the strategy of the generals, part of the army attack and part of the army retreat will cause the siege to fail, so the generals must vote to agree on a strategy, either attack together or retreat together.</p>
<p>Since each general occupies a corner of the city, they can only communicate with each other through messengers. During the coordination process, each general will inform all the other generals of his vote to &ldquo;attack&rdquo; or &ldquo;retreat&rdquo; by messenger, so that each general will know the result of the vote based on his own vote and the votes sent by the other generals. The decision to attack or &gt;retreat is made by each general based on his vote and the votes sent by the other generals.</p>
<p>The problem is complicated by the fact that there can be traitors among the generals who not only vote for the wrong decision, but also send their votes selectively. Suppose there is a traitor among 9 generals, and 4 of the 8 loyal generals vote &ldquo;attack&rdquo; and 4 vote &ldquo;retreat&rdquo;, then the traitor may deliberately vote &ldquo;attack&rdquo; for the 4 generals who voted &ldquo;attack&rdquo;. In this case, the traitor may deliberately vote &ldquo;attack&rdquo; for the 4 generals who voted &ldquo;attack&rdquo; and &ldquo;retreat&rdquo; for the other 4 generals who voted &ldquo;retreat&rdquo;. Thus, it appears to the 4 generals who voted &ldquo;attack&rdquo; that the vote was for 5 to attack, while the other 4 generals appear to have voted &ldquo;retreat&rdquo; for 5 to retreat. Thus, consistency is broken.</p>
<p>In another case, since the generals need to communicate with each other by messenger, even if all generals are loyal, the messengers sent may be intercepted by the enemy or even replaced by spies, which means that the message channel for communication between generals is not reliable. So when no message is received from the corresponding general, the generals will default to a vote, such as &ldquo;attack&rdquo;.</p>
</blockquote>
<p>More generally, in the case of a known rebellion by N generals, can the remaining M loyal generals reach a consensus without the influence of a traitor? What are the preconditions and how should consensus be reached? This is the Byzantine general problem.</p>
<p>If a consensus algorithm can solve the Byzantine general problem under certain conditions, then we call this algorithm a &ldquo;<strong>Byzantine Fault Tolerance (BFT)</strong>&rdquo; algorithm. Conversely if a consensus algorithm cannot accept any node as evil, then it is called &ldquo;<strong>Non-Byzantine Fault Tolerance Crash Fault Tolerance (CFT)</strong>&rdquo; algorithm.</p>
<p>It can be found by simple exhaustive enumeration that two loyalties and one traitor cannot reach consensus. This conclusion combined with the converse method can prove that the <strong>Byzantine Fault Tolerance algorithm requires the proportion of traitors to be less than 1/3</strong>.</p>
<h3 id="commonly-used-consensus-algorithms">Commonly used consensus algorithms</h3>
<p>For the case of &ldquo;<strong>Non-Byzantine Fault Tolerance Crash Fault Tolerance (CFT)</strong>&rdquo;, a number of classical algorithms already exist, including Paxos (1990), Raft (2014) and its variants. Such fault-tolerant algorithms tend to perform relatively well, process faster, and tolerate no more than half of the failed nodes.</p>
<p>For the case of &ldquo;<strong>Byzantine Fault Tolerance Byzantine Fault Tolerance (BFT)</strong>&rdquo;, there are currently algorithms such as PBFT (Practical Byzantine Fault Tolerance, 1999) for the deterministic family of algorithms, PoW (1999) for the probabilistic algorithms, and other algorithms to choose from. Deterministic algorithms are irreversible once consensus is reached, i.e., consensus is the final result; whereas the consensus result of probabilistic class algorithms is temporary, and with time or some kind of reinforcement, the consensus result is less and less likely to be overturned and eventually becomes the de facto result. Byzantine-type fault-tolerant algorithms tend to perform poorly, tolerating no more than 1/3 of the failed nodes.</p>
<p>In addition, recently proposed improved algorithms such as XFT (Cross Fault Tolerance, 2015) can provide CFT-like processing response speed and can provide BFT guarantees when most nodes are working properly. The Algorand algorithm (2017) is improved based on PBFT and solves the proposal selection problem by introducing verifiable random functions, which can theoretically achieve better performance (1000+ TPS) while tolerating Byzantine errors.</p>
<blockquote>
<p>Note: In practice, for the client to get the consensus result needs to be verified by itself, typically, enough service nodes can be accessed to compare the results and ensure the accuracy of the obtained results.</p>
</blockquote>
<p>Common consensus algorithms are listed as follows.</p>
<table>
<thead>
<tr>
<th></th>
<th>Byzantine fault tolerance</th>
<th>consistency</th>
<th>performance</th>
<th>availability (what percentage of nodes can be tolerated to fail)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Two-phase Commit 2PC</td>
<td>No</td>
<td>Strong Consistency</td>
<td>Low</td>
<td>Low</td>
</tr>
<tr>
<td>TCC(try-confirm-cancel)</td>
<td>No</td>
<td>Final Consistency</td>
<td>Low</td>
<td>Low</td>
</tr>
<tr>
<td>Paxos</td>
<td>No</td>
<td>Strong Consistency</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>ZAB</td>
<td>No</td>
<td>Final Consistency</td>
<td>Medium</td>
<td>MMediumiddle</td>
</tr>
<tr>
<td>Raft</td>
<td>No</td>
<td>Strong Consistency</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>Gossip</td>
<td>No</td>
<td>Final Consistency</td>
<td>High</td>
<td>High</td>
</tr>
<tr>
<td>Quorum NWR</td>
<td>No</td>
<td>Strong Consistency</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>PBFT</td>
<td>Yes</td>
<td>N/A</td>
<td>Low</td>
<td>Medium</td>
</tr>
<tr>
<td>PoW</td>
<td>Yes</td>
<td>N/A</td>
<td>Low</td>
<td>Medium</td>
</tr>
<tr>
<td>PoS</td>
<td>Yes</td>
<td>N/A</td>
<td>Low</td>
<td>Medium</td>
</tr>
<tr>
<td><a href="https://medium.com/solana-labs/proof-of-history-explained-by-a-water-clock-e682183417b8">PoH</a></td>
<td>Yes</td>
<td>N/A</td>
<td>Middle</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Note: Although consistency algorithms such as PoW/PoS/PoH are listed here for application in blockchain, they are very different from other Byzantine fault-tolerant algorithms such as PBFT, which will be given later.</p>
</blockquote>
<h3 id="application-scenarios-for-different-consensus-algorithms">Application Scenarios for Different Consensus Algorithms</h3>
<p>In untrustworthy environments, where malicious behavior may exist, consensus algorithms that support Byzantine fault tolerance such as PoW/PoS are needed to enable the system to reach consensus despite the presence of some nodes acting in a malicious manner. This is the reason why blockchains use PoW/PoS algorithms instead of Paxos/Raft algorithms.</p>
<p>In scenarios such as enterprise intranets, which can be considered as trusted environments, there are basically no malicious nodes or node identity authentication can be performed by means of mTLS, and it is enough for the system to have fault tolerance in such scenarios.</p>
<h3 id="non-byzantine-error-consensus-algorithms-paxos-and-raft">Non-Byzantine error consensus algorithms Paxos and Raft</h3>
<p>Due to space and effort, I will skip this part for now&hellip; I may write a new article dedicated to Paxos/Raft algorithms later.</p>
<h3 id="pow-probabilistic-consensus-algorithm-that-tolerates-byzantine-errors">PoW probabilistic consensus algorithm that tolerates Byzantine errors</h3>
<p>PoW is Proof of Work, a metric set by a system to achieve a certain goal. It is simply understood as a proof that you have done a certain amount of work. The whole process of monitoring work is usually extremely inefficient, and the certification process is very simple and efficient by certifying the results of the work to prove that the appropriate amount of work was done, which is where PoW comes in.</p>
<p>In 1993, <a href="http://www.wisdom.weizmann.ac.il/~naor/PAPERS/pvp.ps">Cynthia Dwork and Moni Naor designed a system for anti-spam and avoiding misuse of resources</a>, which is the prototype of the PoW algorithm. The core idea is as follows.</p>
<blockquote>
<p>The main idea is to require a user to compute a moderately hard but not intractable function in order to gain access to the resource, thus preventing frivolous use.</p>
</blockquote>
<p>In 1999, <a href="https://link.springer.com/chapter/10.1007/978-0-387-35568-9_18">Markus Jakobsson and Ari Juels first distilled the concept of Proofs of Work from various protocols</a>.</p>
<p>There must be two roles in a POW system, worker and verifier, and they need to have the following characteristics.</p>
<ul>
<li>There must be a certain amount of work to be done by the worker, and this amount is given by the work verifier.</li>
<li>The validator can quickly check whether the workload is up to standard.</li>
<li>The worker cannot &ldquo;create the work&rdquo; himself, but must be issued by the validator.</li>
<li>The worker cannot find a way to get the work done quickly.</li>
</ul>
<p>At this point, we should have enough understanding of PoW, which is to let workers consume a certain amount of resources as the cost of using the system. For a normal user this consumed resource is perfectly acceptable, but for a malicious attacker who wants to abuse the system&rsquo;s resources or send massive amounts of spam, it needs to consume massive computing resources as a cost, which greatly increases the cost of the attack.</p>
<p>To recap, the core of the PoW algorithm is that <strong>it adds cost to message delivery and reduces the rate of message delivery</strong>.</p>
<p>Looking at the Bitcoin blockchain converted into a Byzantine general problem, it works along the lines of</p>
<ul>
<li>Limit the number of proposals over a period of time, and only the nodes with the corresponding privileges (generals) can initiate proposals.
<ul>
<li>This is achieved through PoW workload proofs, where the Bitcoin blockchain requires nodes to perform massive hash computations as the cost of <strong>acquiring proposal privileges</strong>, and the algorithm difficulty is adjusted every two weeks to ensure that the average time to find the correct hash value for the entire system is about 10 minutes.</li>
</ul>
</li>
<li>Relaxed from strong consistency to final consistency.
<ul>
<li>The result of a proposal does not need to be followed by all nodes immediately, but only the longest chain among all the chains in the whole network that the node can search for is selected for subsequent expansion.</li>
</ul>
</li>
<li>Using asymmetric encryption algorithm to provide signature technical support for message delivery between nodes, each node (general) has its own secret key (public-private key) that uniquely identifies the node identity.
<ul>
<li>Using asymmetric encryption algorithm to deliver messages can guarantee the privacy of message delivery. Moreover, the message signature cannot be tampered with, which prevents the message from being forged by malicious nodes.</li>
</ul>
</li>
</ul>
<p>We have given a conclusion earlier: <strong>Byzantine fault-tolerant algorithm requires that the percentage of traitors must be less than 1/3</strong>.</p>
<p>But the difference between blockchain and Byzantine General problem is significant, for example.</p>
<ul>
<li>Blockchain allows any node to join or leave the blockchain at any time, while the Byzantine General problem has a predetermined number of nodes and does not take into account the addition or removal of nodes.</li>
<li>The PoW algorithm of the Bitcoin blockchain can only guarantee that the <strong>average time</strong> for the entire system to find the correct Hash value is about 10 minutes, so it is entirely possible that nodes with better performance will take less time, nodes with worse performance will take longer, and even some nodes will be lucky enough to get the result in a few seconds. The earlier the node calculates the Hash value, the higher the probability that its proposal (block) will be the longest chain.</li>
<li>If a chain is not long at the beginning, but it expands fast enough, it can become the longest chain. And the Byzantine General problem does not allow any branching, only one result exists!
<ul>
<li>Just limited by the arithmetic power, the probability of a short chain catching up with the longest chain will become smaller and smaller as time goes by.</li>
</ul>
</li>
</ul>
<p>Anyway, because of such characteristics of the blockchain, it produces some results that are different from the Byzantine fault-tolerant algorithm.</p>
<ul>
<li>The percentage of the number of nodes owned by the attacker is meaningless; the core is the arithmetic power, which corresponds to the proposal power in the blockchain.
<ul>
<li>Even if the attacker owns 99% of the nodes, but its overall arithmetic power is weak, the probability of its proposal (block) becoming the longest chain will be low.</li>
</ul>
</li>
<li>Because of the principle that &ldquo;the system always picks the longest chain for subsequent expansion&rdquo;, only if an attacker has more than 50% of the computing power, it has an absolute advantage that its block will definitely become the longest chain after a certain period of time, and it will always maintain such an advantage to achieve the purpose of the attack.</li>
</ul>
<p>As for the specific implementation of PoW algorithm and the principle and implementation of its alternative algorithms such as PoS/PoH and other emerging algorithms, we will introduce them in detail in the following blockchain series, so please look forward to them&hellip;</p>

    </div>

    
<footer class="post-footer">
      
      <nav class="post-nav">
        <a class="prev" href="/post/2022-08/k8s-schedule-ext-prometheus/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Using Prometheus to extend the kubernetes scheduler</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2022-08/dockerfile-lf-characters/">
            <span class="next-text nav-default">When using Dockerfile to build container images, you should pay more attention to the impact of &#34;line breaks&#34;.</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>








</body>
</html>
