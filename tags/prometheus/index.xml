<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>prometheus on SoByte</title>
    <link>https://www.sobyte.net/tags/prometheus/</link>
    <description>Recent content in prometheus on SoByte</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 08 Jul 2022 12:51:39 +0800</lastBuildDate><atom:link href="https://www.sobyte.net/tags/prometheus/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using Prometheus Pushgateway to push monitoring metrics</title>
      <link>https://www.sobyte.net/post/2022-07/pushgateway/</link>
      <pubDate>Fri, 08 Jul 2022 12:51:39 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-07/pushgateway/</guid>
      <description>We know that Prometheus uses the pull mode, but in some network scenarios (such as not on a subnet or firewall), Prometheus cannot directly pull the monitoring metrics data, so we may need a mode that can actively push. Pushgateway is one of the tools in the Prometheus ecosystem to solve this problem. However, Pushgateway is not a panacea and has some drawbacks. Aggregating data from multiple nodes to pushgateway,</description>
    </item>
    
    <item>
      <title>The disappearing Prometheus indicator</title>
      <link>https://www.sobyte.net/post/2022-06/prometheus-disappearing/</link>
      <pubDate>Thu, 30 Jun 2022 12:38:05 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-06/prometheus-disappearing/</guid>
      <description>Problem phenomenon We have some GPU machines and need to count GPU related information, the data is taken from Prometheus, but one day we suddenly found that some labels of some GPU node metrics are empty.
Here are the metrics of normal nodes.
1 container_accelerator_duty_cycle{acc_id=&amp;#34;GPU-d8384090-81d2-eda5-ed02-8137eb037460&amp;#34;,container_name=&amp;#34;nvidia-device-plugin-ctr&amp;#34;,endpoint=&amp;#34;https-metrics&amp;#34;,id=&amp;#34;/kubepods/besteffort/podd917e00f-a779-11e9-b971-6805ca7f5b2a/38a5decf96e9f007fcb0059d79017ea3b3c29ff4c9a81b7fec86cf63c06baf53&amp;#34;,image=&amp;#34;sha256:7354b8a316796fba0463a68da79d79eb654d741241ca3c4f62a1ef24d8e11718&amp;#34;,instance=&amp;#34;10.0.0.1:10250&amp;#34;,job=&amp;#34;kubelet&amp;#34;,make=&amp;#34;nvidia&amp;#34;,model=&amp;#34;Tesla P40&amp;#34;,name=&amp;#34;k8s_nvidia-device-plugin-ctr_nvidia-device-plugin-daemonset-sn2cg_lambda_d917e00f-a779-11e9-b971-6805ca7f5b2a_1&amp;#34;,namespace=&amp;#34;ieevee&amp;#34;,node=&amp;#34;x1.ieevee.com&amp;#34;,pod_name=&amp;#34;nvidia-device-plugin-daemonset-sn2cg&amp;#34;,service=&amp;#34;kubelet&amp;#34;} 0 Here are the metrics of the exception node.
1 container_accelerator_duty_cycle{acc_id=&amp;#34;GPU-a7b535d0-d6ca-022c-5b23-1bff863646a4&amp;#34;,container_name=&amp;#34;&amp;#34;,endpoint=&amp;#34;https-metrics&amp;#34;,id=&amp;#34;/kubepods/besteffort/pod8bb25662-de9a-11e9-84e7-f8f21e04010c/cde3858becb05366e71f230e876204be586662f274dcb4a6e2b75ea404f2d5a9&amp;#34;,instance=&amp;#34;10.0.0.2:10250&amp;#34;,job=&amp;#34;kubelet&amp;#34;,make=&amp;#34;nvidia&amp;#34;,model=&amp;#34;Tesla V100-PCIE-16GB&amp;#34;,name=&amp;#34;&amp;#34;,namespace=&amp;#34;&amp;#34;,pod_name=&amp;#34;&amp;#34;} You can see that the data taken out by Prometheus, the container_name, name, namespace, and pod_name in the label are all empty.</description>
    </item>
    
    <item>
      <title>Solve the problem that Prometheus can&#39;t collect data</title>
      <link>https://www.sobyte.net/post/2022-06/prometheus-client/</link>
      <pubDate>Thu, 16 Jun 2022 09:17:21 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-06/prometheus-client/</guid>
      <description>Before the eBPF-based next-generation observation facility was mature, we used the industry-established Prometheus+Grafana scheme to collect node and application metrics. Such a scheme is known to be invasive to the application, i.e., it requires a client package embedded inside the application to collect metrics and communicate with Prometheus.
Prometheus officially provides and maintains client packages for major languages, including Go, Java, Python, Ruby, Rust, etc., as follows.
The go client side of Prometheus is not too complicated to use and consists of two steps.</description>
    </item>
    
    <item>
      <title>Stress testing of Prometheus-compatible timing databases</title>
      <link>https://www.sobyte.net/post/2022-05/prometheus-test/</link>
      <pubDate>Sun, 22 May 2022 10:35:54 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-05/prometheus-test/</guid>
      <description>For comparison between different VictoriaMetrics versions or between VictoriaMetrics and other solutions that support the Prometheus remote_write protocol, VictoriaMetrics provides a dedicated Prometheus-benchmark project. Implementation principle The implementation of this project is actually very simple. Use node_exporter as a source for production-like metrics. use nginx as a caching proxy in front of node_exporter to reduce the load on node_exporter when it crawls too many metrics. Use vmagent to grab node_exporter</description>
    </item>
    
    <item>
      <title>Use vmalert instead of Prometheus to monitor alarms</title>
      <link>https://www.sobyte.net/post/2022-05/vmalert/</link>
      <pubDate>Thu, 19 May 2022 19:31:01 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-05/vmalert/</guid>
      <description>We have already introduced the possibility of using vmagent instead of prometheus to capture monitoring metrics data, to completely replace prometheus there is a very important part is the alarm module, before we are defined in prometheus alarm rules evaluation and sent to alertmanager, the same corresponds to the vm also has a special module to handle alarms: vmalert . vmalert will execute the configured alarm or logging rule for</description>
    </item>
    
    <item>
      <title>Use vmagent instead of Prometheus to collect monitoring metrics</title>
      <link>https://www.sobyte.net/post/2022-05/vmagent/</link>
      <pubDate>Wed, 11 May 2022 13:27:27 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-05/vmagent/</guid>
      <description>vmagent can help us collect metrics from various sources and store them in VMs or any other Prometheus-compatible storage system that supports the remote write protocol. Features vmagent has more flexibility than Prometheus for scraping metrics, such as the ability to push metrics in addition to pull metrics, and many other features. Can replace the scraping target of prometheus Support for reading and writing data from Kafka Supports adding, removing,</description>
    </item>
    
    <item>
      <title>Prometheus Storage Engine Analysis</title>
      <link>https://www.sobyte.net/post/2022-05/prometheus-storage-engine/</link>
      <pubDate>Sun, 08 May 2022 10:43:43 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-05/prometheus-storage-engine/</guid>
      <description>Prometheus is one of the most popular monitoring platforms in the cloud-native era as a temporal database. Although its overall architecture has not changed much, the underlying storage engine has evolved over several versions. This article focuses on the details of the storage format of Prometheus V2 version (which is mainly used now) and how queries locate the eligible data, aiming to have a deeper understanding of the storage engine</description>
    </item>
    
    <item>
      <title>Prometheus Long Term Remote Storage Solution VictoriaMetrics Getting Started</title>
      <link>https://www.sobyte.net/post/2022-04/victoriametrics-getting-started/</link>
      <pubDate>Sat, 23 Apr 2022 10:44:46 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-04/victoriametrics-getting-started/</guid>
      <description>VictoriaMetrics (VM) is a highly available, cost-effective and scalable monitoring solution and time series database for long-term remote storage of Prometheus monitoring data. Compared to Thanos, VictoriaMetrics is primarily a horizontally scalable local full-volume persistent storage solution, VictoriaMetrics is more than just a temporal database, its advantages are mainly reflected in the following points. External support for Prometheus-related APIs, which can be used directly in Grafana as a Prometheus data</description>
    </item>
    
    <item>
      <title>Centralized data management of multiple Prometheus instances with Thanos</title>
      <link>https://www.sobyte.net/post/2022-03/manage-multiple-prometheus-using-thanos/</link>
      <pubDate>Tue, 22 Mar 2022 14:51:27 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/manage-multiple-prometheus-using-thanos/</guid>
      <description>1. layering of monitoring As shown above, two strategies are used when building a monitoring system:
The advantage of separating IaaS, MySQL middleware, and App tier monitoring is that the systems are highly available and fault-tolerant to each other. When the App tier monitoring does not work, the IaaS tier monitoring will immediately show up. Separation of long and short term metrics. Short-term metrics are used to provide alerting systems with high frequency queries for recent data, and long-term metrics are used to provide people with queries for data sets that span a larger period of time.</description>
    </item>
    
    <item>
      <title>Monitor of Kubernetes Using Prometheus Grafana</title>
      <link>https://www.sobyte.net/post/2022-03/monitor-of-kubernetes-using-prometheus-grafana/</link>
      <pubDate>Sun, 20 Mar 2022 11:28:34 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/monitor-of-kubernetes-using-prometheus-grafana/</guid>
      <description>The Prometheus community is updating so fast that some of the documentation written before is a bit out of date. Recently, I started to focus on observability again, and make up for some knowledge points in operation and maintenance.
1. Explanation of terms Grafana A visualization tool that provides various visualization panels and supports various data sources, including Prometheus, OpenTSDB, MySQL, etc.
Prometheus A time series database, mainly used to collect, store, and provide query data to the public.</description>
    </item>
    
    <item>
      <title>Prometheus Routing and Routing Configuration with Nginx Reverse Proxy</title>
      <link>https://www.sobyte.net/post/2022-03/prometheus-nginx-proxy/</link>
      <pubDate>Sat, 19 Mar 2022 18:03:07 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/prometheus-nginx-proxy/</guid>
      <description>Background The company group an end-to-end containerized test environment, located under different sub-paths, proxied by Nginx: * For example, user A is under /a/prometheus/: /a/prometheus/. For example, user A is under /a/: /a/prometheus/ For example, user B under /b/: /b/prometheus/ The effect you want to achieve, in addition to the above distinction between different subpaths, requires that there is no such distinction inside the container: i.e., the container can be</description>
    </item>
    
    <item>
      <title>Prometheus Monitoring Kubernetes Job Resource False Alarm Issue</title>
      <link>https://www.sobyte.net/post/2022-03/prometheus-monitor-k8s-job-trap/</link>
      <pubDate>Sun, 06 Mar 2022 17:12:01 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/prometheus-monitor-k8s-job-trap/</guid>
      <description>Someone mentioned a problem before, it is about Prometheus monitoring Job task false alarm problem, the general meaning of the CronJob control Job, the front of the execution failed, monitoring will trigger the alarm, after the solution to generate a new Job can be executed normally, but will still receive the alarm in front.
This is because we generally keep some history when executing Job tasks to facilitate troubleshooting, so if there is a failed Job before, even if it will become successful later, the previous Job will continue to exist, and the default alarm rule used by most direct kube-prometheus installation and deployment is kube_job_ status_failed &amp;gt; 0, which is obviously inaccurate.</description>
    </item>
    
    <item>
      <title>Kubernetes HPA Controlled Elastic Scaling based on Prometheus Custom Metrics</title>
      <link>https://www.sobyte.net/post/2022-01/k8s-hpa-prometheus/</link>
      <pubDate>Wed, 19 Jan 2022 15:37:00 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-01/k8s-hpa-prometheus/</guid>
      <description>There are three main types of elastic scaling in Kubernetes: HPA, VPA, and CA. Here we will focus on Pod Horizontal Scaling HPA.
With the release of Kubernetes v1.23, the HPA API came to a stable version autoscaling/v2:
Scaling based on custom metrics Scaling based on multiple metrics Configurable scaling behaviour From the initial v1 version of HPA, which only supported CPU and memory utilisation scaling, to the later support for custom metrics and aggregation layer APIs, to v1.</description>
    </item>
    
    <item>
      <title>AlertManager When to Alert</title>
      <link>https://www.sobyte.net/post/2021-11/alertmanager-when-alert/</link>
      <pubDate>Mon, 22 Nov 2021 19:42:12 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2021-11/alertmanager-when-alert/</guid>
      <description>When using Prometheus for monitoring, alerting is done through AlertManager, but there are many people who are confused about the configuration related to alerting and are not quite sure when exactly it will be done. Here we will briefly introduce a few confusing parameters in AlertManager. First of all, there are two global parameters scrape_interval and evaluation_interval in Prometheus. The scrape_interval parameter represents the time interval at which Prometheus grabs</description>
    </item>
    
  </channel>
</rss>
